{
 "cells": [
  {
   "cell_type": "raw",
   "id": "97c1cb08",
   "metadata": {},
   "source": [
    "---\n",
    "title: A Biased Employment Status Algorithm\n",
    "author: Jay-U Chung\n",
    "date: '2023-04-04'\n",
    "#image: \"image.jpg\"\n",
    "description: \"Here we will examine some employment status data from Missouri and demonstrate that it exhibits bias by ethnicity.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eaa707",
   "metadata": {},
   "source": [
    "## Bias Audit\n",
    "\n",
    "In this blog post, we will be auditing the allocative bias on our algorithm. We will examine data on the employment status in Missouri, focusing on the various self-reported ethnicities. We will then train an algorithm to predict employment status and examine any biases our algorithm might exhibit with respect to these groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76024eb4",
   "metadata": {},
   "source": [
    "## Examining the Data\n",
    "\n",
    "Our data can be obtained with the folktables package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94573345-ec31-41e4-a0e0-8a1ca3628d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RT</th>\n",
       "      <th>SERIALNO</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>SPORDER</th>\n",
       "      <th>PUMA</th>\n",
       "      <th>REGION</th>\n",
       "      <th>ST</th>\n",
       "      <th>ADJINC</th>\n",
       "      <th>PWGTP</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>...</th>\n",
       "      <th>PWGTP71</th>\n",
       "      <th>PWGTP72</th>\n",
       "      <th>PWGTP73</th>\n",
       "      <th>PWGTP74</th>\n",
       "      <th>PWGTP75</th>\n",
       "      <th>PWGTP76</th>\n",
       "      <th>PWGTP77</th>\n",
       "      <th>PWGTP78</th>\n",
       "      <th>PWGTP79</th>\n",
       "      <th>PWGTP80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000034</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1013097</td>\n",
       "      <td>77</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>123</td>\n",
       "      <td>128</td>\n",
       "      <td>134</td>\n",
       "      <td>70</td>\n",
       "      <td>77</td>\n",
       "      <td>74</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000067</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1001</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1013097</td>\n",
       "      <td>73</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>70</td>\n",
       "      <td>130</td>\n",
       "      <td>72</td>\n",
       "      <td>125</td>\n",
       "      <td>73</td>\n",
       "      <td>77</td>\n",
       "      <td>70</td>\n",
       "      <td>69</td>\n",
       "      <td>127</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000097</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1013097</td>\n",
       "      <td>85</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>12</td>\n",
       "      <td>88</td>\n",
       "      <td>13</td>\n",
       "      <td>90</td>\n",
       "      <td>83</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000113</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1002</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1013097</td>\n",
       "      <td>78</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>78</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P</td>\n",
       "      <td>2018GQ0000159</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1013097</td>\n",
       "      <td>55</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>57</td>\n",
       "      <td>101</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>108</td>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  RT       SERIALNO  DIVISION  SPORDER  PUMA  REGION  ST   ADJINC  PWGTP  \\\n",
       "0  P  2018GQ0000034         4        1   400       2  29  1013097     77   \n",
       "1  P  2018GQ0000067         4        1  1001       2  29  1013097     73   \n",
       "2  P  2018GQ0000097         4        1   600       2  29  1013097     85   \n",
       "3  P  2018GQ0000113         4        1  1002       2  29  1013097     78   \n",
       "4  P  2018GQ0000159         4        1   200       2  29  1013097     55   \n",
       "\n",
       "   AGEP  ...  PWGTP71  PWGTP72  PWGTP73  PWGTP74  PWGTP75  PWGTP76  PWGTP77  \\\n",
       "0    27  ...      123      128      134       70       77       74       70   \n",
       "1    42  ...       70      130       72      125       73       77       70   \n",
       "2    20  ...       86       84       85       12       88       13       90   \n",
       "3    26  ...       80       79        6       75       78        6       77   \n",
       "4    37  ...       11       57      101       56       57      108       54   \n",
       "\n",
       "   PWGTP78  PWGTP79  PWGTP80  \n",
       "0       70       16      139  \n",
       "1       69      127      128  \n",
       "2       83       13       14  \n",
       "3       76       78       78  \n",
       "4       13       13       13  \n",
       "\n",
       "[5 rows x 286 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\n",
    "import numpy as np\n",
    "\n",
    "STATE = \"MO\"\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', \n",
    "                            horizon='1-Year', \n",
    "                            survey='person')\n",
    "\n",
    "acs_data = data_source.get_data(states=[STATE], download=True)\n",
    "\n",
    "acs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da13995",
   "metadata": {},
   "source": [
    "Here we select the possible features, most notably the race (RAC1P) and sex (SEX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e310130-48fb-4ffb-ad8c-543eeca9bb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>MAR</th>\n",
       "      <th>RELP</th>\n",
       "      <th>DIS</th>\n",
       "      <th>ESP</th>\n",
       "      <th>CIT</th>\n",
       "      <th>MIG</th>\n",
       "      <th>MIL</th>\n",
       "      <th>ANC</th>\n",
       "      <th>NATIVITY</th>\n",
       "      <th>DEAR</th>\n",
       "      <th>DEYE</th>\n",
       "      <th>DREM</th>\n",
       "      <th>SEX</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>ESR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGEP  SCHL  MAR  RELP  DIS  ESP  CIT  MIG  MIL  ANC  NATIVITY  DEAR  DEYE  \\\n",
       "0    27  17.0    5    16    2  NaN    1  3.0  4.0    4         1     2     2   \n",
       "1    42  19.0    5    16    2  NaN    1  3.0  4.0    1         1     2     2   \n",
       "2    20  19.0    5    17    2  NaN    1  1.0  4.0    2         1     2     2   \n",
       "3    26  17.0    5    16    1  NaN    1  1.0  4.0    1         1     2     2   \n",
       "4    37  16.0    5    16    1  NaN    1  3.0  4.0    1         1     2     1   \n",
       "\n",
       "   DREM  SEX  RAC1P  ESR  \n",
       "0   2.0    2      1  6.0  \n",
       "1   2.0    1      2  6.0  \n",
       "2   2.0    1      1  1.0  \n",
       "3   1.0    1      2  6.0  \n",
       "4   1.0    1      2  6.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\n",
    "acs_data[possible_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9bfab4a-0cd3-4d58-9d8f-cbf75d87fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_features = [\"ESR\", \"RAC1P\"]\n",
    "features_to_use = [f for f in possible_features if f not in target_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2720db34-a1d2-42a8-a843-ab926a4fb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EmploymentProblem = BasicProblem(\n",
    "    features=features_to_use,\n",
    "    target='ESR',\n",
    "    target_transform=lambda x: x == 1,\n",
    "    group='RAC1P',\n",
    "    preprocess=lambda x: x,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")\n",
    "\n",
    "features, label, group = EmploymentProblem.df_to_numpy(acs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099d04e",
   "metadata": {},
   "source": [
    "We have now transformed our data so that it can be processed in a classification algorithm. Our task is to predic the employment status of civilians at work (excluding military). Of course, we split the training and testing data, which in this case is an 80-20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "898b13c3-b412-4022-be02-d0b4456ec146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    features, label, group, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2daea5",
   "metadata": {},
   "source": [
    "Let's now examine some demographic characteristic of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4dfaac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(X_train, columns = features_to_use)\n",
    "df[\"group\"] = group_train\n",
    "df[\"label\"] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94296d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>Number</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White alone</td>\n",
       "      <td>43713</td>\n",
       "      <td>87.545061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Black or African American alone</td>\n",
       "      <td>3838</td>\n",
       "      <td>7.686454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American Indian alone</td>\n",
       "      <td>150</td>\n",
       "      <td>0.300409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alaska Native alone</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>American Indian and Alaska Native tribes speci...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.054074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Asian alone</td>\n",
       "      <td>762</td>\n",
       "      <td>1.526075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Native Hawaiian and Other Pacific Islander alone</td>\n",
       "      <td>51</td>\n",
       "      <td>0.102139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Some Other Race alone</td>\n",
       "      <td>316</td>\n",
       "      <td>0.632861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Two or More Races</td>\n",
       "      <td>1070</td>\n",
       "      <td>2.142914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               group  Number  Percentage\n",
       "1                                        White alone   43713   87.545061\n",
       "2                    Black or African American alone    3838    7.686454\n",
       "3                              American Indian alone     150    0.300409\n",
       "4                                Alaska Native alone       5    0.010014\n",
       "5  American Indian and Alaska Native tribes speci...      27    0.054074\n",
       "6                                        Asian alone     762    1.526075\n",
       "7   Native Hawaiian and Other Pacific Islander alone      51    0.102139\n",
       "8                              Some Other Race alone     316    0.632861\n",
       "9                                  Two or More Races    1070    2.142914"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nums = df.groupby([\"group\"]).size().reset_index(name = \"Number\")\n",
    "df_nums[\"group\"] = ['White alone','Black or African American alone','American Indian alone','Alaska Native alone','American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races','Asian alone','Native Hawaiian and Other Pacific Islander alone',' Some Other Race alone','Two or More Races']\n",
    "df_nums = df_nums.rename(index = {0:1,1:2,2:3,3:4,4:5,5:6,6:7,7:8,8:9})\n",
    "df_nums['Percentage'] = df_nums['Number']/df.shape[0]*100.0\n",
    "df_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77136c0a",
   "metadata": {},
   "source": [
    "So we can see that an overwhelming majority of our data is White, then Black, Two or More Races, and Asian. For my analysis, I will only focus on these groups as the others have too small a sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71a96933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall employment rate:  0.4467275494672755\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>Employed</th>\n",
       "      <th>Percentage</th>\n",
       "      <th>Diff. with Overall Avg.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White alone</td>\n",
       "      <td>True</td>\n",
       "      <td>45.439572</td>\n",
       "      <td>0.766817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Black or African American alone</td>\n",
       "      <td>True</td>\n",
       "      <td>39.577905</td>\n",
       "      <td>-5.094850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>American Indian alone</td>\n",
       "      <td>True</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.327245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alaska Native alone</td>\n",
       "      <td>True</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>15.327245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>American Indian and Alaska Native tribes speci...</td>\n",
       "      <td>True</td>\n",
       "      <td>40.740741</td>\n",
       "      <td>-3.932014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Asian alone</td>\n",
       "      <td>True</td>\n",
       "      <td>49.343832</td>\n",
       "      <td>4.671077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Native Hawaiian and Other Pacific Islander alone</td>\n",
       "      <td>True</td>\n",
       "      <td>52.941176</td>\n",
       "      <td>8.268422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Some Other Race alone</td>\n",
       "      <td>True</td>\n",
       "      <td>37.341772</td>\n",
       "      <td>-7.330983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Two or More Races</td>\n",
       "      <td>True</td>\n",
       "      <td>29.906542</td>\n",
       "      <td>-14.766213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                group  Employed  Percentage  \\\n",
       "1                                         White alone      True   45.439572   \n",
       "3                     Black or African American alone      True   39.577905   \n",
       "5                               American Indian alone      True   46.000000   \n",
       "7                                 Alaska Native alone      True   60.000000   \n",
       "9   American Indian and Alaska Native tribes speci...      True   40.740741   \n",
       "11                                        Asian alone      True   49.343832   \n",
       "13   Native Hawaiian and Other Pacific Islander alone      True   52.941176   \n",
       "15                              Some Other Race alone      True   37.341772   \n",
       "17                                  Two or More Races      True   29.906542   \n",
       "\n",
       "    Diff. with Overall Avg.  \n",
       "1                  0.766817  \n",
       "3                 -5.094850  \n",
       "5                  1.327245  \n",
       "7                 15.327245  \n",
       "9                 -3.932014  \n",
       "11                 4.671077  \n",
       "13                 8.268422  \n",
       "15                -7.330983  \n",
       "17               -14.766213  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_employed = df.groupby([\"group\",\"label\"]).size().groupby(\"group\", group_keys=False).apply(lambda x: 100 * x / x.sum()).reset_index(name = \"Percentage\").replace({1:'White alone',2:'Black or African American alone',3:' American Indian alone',4:'Alaska Native alone',5:'American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races',6:'Asian alone',7:'Native Hawaiian and Other Pacific Islander alone',8:' Some Other Race alone',9:'Two or More Races'})\n",
    "df_employed = df_employed.rename(columns = {'label':'Employed'})\n",
    "\n",
    "print(\"Overall employment rate: \",(df[\"label\"] == 1.0).mean())\n",
    "df_employed['Diff. with Overall Avg.'] = df_employed['Percentage'] - (df[\"label\"] == 1.0).mean()*np.ones(df_employed.shape[0])*100\n",
    "df_employed[df_employed[\"Employed\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad9e4c",
   "metadata": {},
   "source": [
    "So in comparison to the overall average, White individuals are about the same, Black individuals are less employed, individuals of two or more races are much less employed, Asian individuals are slightly more employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c4430c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='group', ylabel='Percentage'>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "df_plot = df.groupby([\"group\",\"SEX\",\"label\"]).size().groupby([\"group\",\"SEX\"], group_keys=False).apply(lambda x: 100 * x / x.sum()).reset_index(name = \"Percentage\")\n",
    "df_plot['SEX'] = df_plot['SEX'].replace([1.0,2.0],['Male','Female'])\n",
    "\n",
    "sb.barplot(x=\"group\",\n",
    "           y=\"Percentage\",\n",
    "           hue=\"SEX\",\n",
    "           data=df_plot[df_plot[\"label\"]==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85c6a3",
   "metadata": {},
   "source": [
    "Plotting the employment rates by sex, we can see that there are clear disparities between overall employment and male and female employment. \n",
    "\n",
    "For White and Asian individuals, males are more likely to be employed than females. For Black individuals and those from Two or More races, females are more likely to be employed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932261ce",
   "metadata": {},
   "source": [
    "## Training an Algorithm to Predict Employment Status\n",
    "\n",
    "The main classification algorithms we have on hand from Scikit Learn are Logistic Regression, Support Vector Machine, Decision Tree Classifier, and the Random Forest Classifier algorithm.\n",
    "\n",
    "Each model has a parameter that must be tuned. For Logistic Regression, this is the degree for polynomial features, for Support Vector Machine the regularization parameter, for Decision Trees and the Random Forest, the max depth. I define a function below that will take a model, tune its optimal parameter using cross validation, and output this optimal parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2eb7c3e6-afb4-4e47-8157-fc1568a9593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def train_opt_model(input_model, kwargs, params, X_train, y_train, X_test, y_test):\n",
    "    keys = list(kwargs.keys())\n",
    "    best_score = 0.0\n",
    "    best_param = params[0]\n",
    "    \n",
    "    for param in params:\n",
    "        kwargs[keys[0]] = param #should be only one argument in this case\n",
    "        model = make_pipeline(StandardScaler(), input_model(**kwargs))\n",
    "        train_score = cross_val_score(model, X_train, y_train, cv=5).mean()\n",
    "        if (train_score > best_score):\n",
    "            best_score = train_score\n",
    "            best_param = param\n",
    "                              \n",
    "    print(\"Best model parameter: \",best_param)    \n",
    "    kwargs[keys[0]] = best_param\n",
    "    best_model = make_pipeline(StandardScaler(), input_model(**kwargs))\n",
    "    best_model.fit(X_train,y_train)\n",
    "    print(\"Best model training score: \",best_model.score(X_train,y_train))\n",
    "    print(\"Best model test score: \",best_model.score(X_test,y_test))\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1814f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def poly_LR(degree, **kwargs):\n",
    "    plr = Pipeline([(\"poly\", PolynomialFeatures(degree = degree)),\n",
    "                    (\"LR\", LogisticRegression(**kwargs))])\n",
    "    return plr\n",
    "\n",
    "params = [i for i in range(3,4)]\n",
    "kwargs = {\"degree\":params[0],\"max_iter\":5000}\n",
    "kwargs = train_opt_model(poly_LR, kwargs, params, X_train, y_train, X_test, y_test)\n",
    "model = make_pipeline(StandardScaler(), poly_LR(**kwargs))\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "print(\"Overall employment accuracy: \",(y_hat == y_test).mean())\n",
    "print(\"Employment accuracy for white: \",(y_hat == y_test)[group_test == 1].mean())\n",
    "print(\"Employment accuracy for black:  \",(y_hat == y_test)[group_test == 2].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52431a17-4646-46b3-b923-6bc62243f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "params = [0.1]\n",
    "kwargs = {\"C\":params[0]}\n",
    "kwargs = train_opt_model(SVC, kwargs, params, X_train, y_train, X_test, y_test)\n",
    "model = make_pipeline(StandardScaler(), SVC(**kwargs))\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "print(\"Overall employment accuracy: \",(y_hat == y_test).mean())\n",
    "print(\"Employment accuracy for white: \",(y_hat == y_test)[group_test == 1].mean())\n",
    "print(\"Employment accuracy for black:  \",(y_hat == y_test)[group_test == 2].mean())\n",
    "\n",
    "confusion_matrix_SVC = confusion_matrix(y_test,y_hat,normalize='true')\n",
    "print(\"False positive rate: \",confusion_matrix_SVC[0][1])\n",
    "print(\"False negative rate: \",confusion_matrix_SVC[1][0])\n",
    "print(confusion_matrix(y_test,y_hat)[1][1]/(confusion_matrix(y_test,y_hat)[1][1]+confusion_matrix(y_test,y_hat)[0][1]))\n",
    "\n",
    "confusion_matrix_SVC = confusion_matrix(y_test[group_test == 1],y_hat[group_test == 1],normalize='true')\n",
    "print(\"White False positive rate: \",confusion_matrix_SVC[0][1])\n",
    "print(\"White False negative rate: \",confusion_matrix_SVC[1][0])\n",
    "print(confusion_matrix(y_test[group_test == 1],y_hat[group_test == 1])[1][1]/(confusion_matrix(y_test[group_test == 1],y_hat[group_test == 1])[1][1]+confusion_matrix(y_test[group_test == 1],y_hat[group_test == 1])[0][1]))\n",
    "\n",
    "confusion_matrix_SVC = confusion_matrix(y_test[group_test == 2],y_hat[group_test == 2],normalize='true')\n",
    "print(\"Black False positive rate: \",confusion_matrix_SVC[0][1])\n",
    "print(\"Black False negative rate: \",confusion_matrix_SVC[1][0])\n",
    "print(confusion_matrix(y_test[group_test == 2],y_hat[group_test == 2])[1][1]/(confusion_matrix(y_test[group_test == 2],y_hat[group_test == 2])[1][1]+confusion_matrix(y_test[group_test == 2],y_hat[group_test == 2])[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e22d8",
   "metadata": {},
   "source": [
    "Spoiler for these two algorithms - I tried running them on my machine, and they both took an extraordinarily long time! Testing with some parameters also yields a similar training score to the algorithm we will use anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f49e2fc8-de20-42a5-aaac-6d95597103c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameter:  9\n",
      "Best model training score:  0.8429263798766322\n",
      "Best model test score:  0.8404357577699455\n",
      "Overall employment accuracy:  0.8403556552387056\n",
      "Employment accuracy for white:  0.8408361602640506\n",
      "Employment accuracy for black:   0.841031149301826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "params = [i for i in range(5,15)]\n",
    "kwargs = {\"max_depth\":params[0]}\n",
    "kwargs = train_opt_model(DecisionTreeClassifier, kwargs, params, X_train, y_train, X_test, y_test)\n",
    "model = make_pipeline(StandardScaler(), DecisionTreeClassifier(**kwargs))\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "print(\"Overall employment accuracy: \",(y_hat == y_test).mean())\n",
    "print(\"Employment accuracy for white: \",(y_hat == y_test)[group_test == 1].mean())\n",
    "print(\"Employment accuracy for black:  \",(y_hat == y_test)[group_test == 2].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f57b43a9-97e1-4fef-9954-2ae0c74a4e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameter:  14\n",
      "Best model training score:  0.8644356324601458\n",
      "Best model test score:  0.8415571932073054\n",
      "Overall employment accuracy:  0.8416372957385453\n",
      "Employment accuracy for white:  0.8411112129824884\n",
      "Employment accuracy for black:   0.8431793770139635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "params = [i for i in range(5,15)]\n",
    "kwargs = {\"max_depth\":params[0]}\n",
    "kwargs = train_opt_model(RandomForestClassifier, kwargs, params, X_train, y_train, X_test, y_test)\n",
    "model = make_pipeline(StandardScaler(), RandomForestClassifier(**kwargs))\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "print(\"Overall employment accuracy: \",(y_hat == y_test).mean())\n",
    "print(\"Employment accuracy for white: \",(y_hat == y_test)[group_test == 1].mean())\n",
    "print(\"Employment accuracy for black:  \",(y_hat == y_test)[group_test == 2].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a69ddc",
   "metadata": {},
   "source": [
    "We can see that the training score for the Decision Tree and Random Forest algorithms are pretty similar. The Random Forest algorithm does have a slightly better training score and test score, but I will opt to use the Decision Tree algorithm because it is faster. Moreover, I think it would not be ethical to have randomness in an algorithm that predicts employment status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed6535",
   "metadata": {},
   "source": [
    "## Analyzing our Employment Classification Algorithm\n",
    "\n",
    "We can now define a function that will make some statistic easier to calculate, and run our Decision Tree with a max depth of 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50bf8fe9-4f9b-4d47-9ab3-cec908463bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(y_hat,y_test,group_test=None, group_test_val = None):\n",
    "    if (group_test_val == None):\n",
    "        y_t = y_test\n",
    "        y_h = y_hat\n",
    "    else:\n",
    "        y_t = y_test[group_test == group_test_val]\n",
    "        y_h = y_hat[group_test == group_test_val]\n",
    "        \n",
    "    CM =  confusion_matrix(y_t,y_h)\n",
    "    \n",
    "    p = (1*y_t).mean() #mean of how many in group are employed\n",
    "    PPV = CM[1][1]/(CM[1][1] + CM[0][1])\n",
    "    \n",
    "    FPR = CM[0][1]/(CM[0][1] + CM[0][0])\n",
    "    FNR = CM[1][0]/(CM[1][0] + CM[1][1])\n",
    "    \n",
    "    calibration_0 = ((y_t == 1)*(y_h == 0)).sum()/((y_h == 0).sum())\n",
    "    calibration_1 = ((y_t == 1)*(y_h == 1)).sum()/((y_h == 1).sum())\n",
    "    \n",
    "    parity = (y_h > 0).mean() #threshold taken to be 0\n",
    "    \n",
    "    return p, PPV, FPR, FNR, calibration_0, calibration_1, parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2bcd7edd-30d6-4a5d-9874-dcf7ae42259a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy:  0.841\n",
      "Overall prevalence:  0.453\n",
      "Overall False positive rate:  0.177\n",
      "Overall False negative rate:  0.138\n",
      "Overall Calibration (Score 0):  0.122\n",
      "Overall Calibration (Score 1):  0.801\n",
      "Overall Parity:  0.488 \n",
      "\n",
      "Accuracy for White:  0.841\n",
      "White prevalence:  0.462\n",
      "White False positive rate:  0.178\n",
      "White False negative rate:  0.136\n",
      "Calibration (Score 0):  0.125\n",
      "Calibration (Score 1):  0.806\n",
      "Parity:  0.495 \n",
      "\n",
      "Accuracy for Black:   0.841\n",
      "Black prevalence:  0.376\n",
      "Black False positive rate:  0.17\n",
      "Black False negative rate:  0.14\n",
      "Calibration (Score 0):  0.092\n",
      "Calibration (Score 1):  0.752\n",
      "Parity:  0.43 \n",
      "\n",
      "Accuracy for Two or More Races:   0.825\n",
      "Two or More Races prevalence:  0.326\n",
      "Two or More Races False positive rate:  0.151\n",
      "Two or More Races False negative rate:  0.226\n",
      "Calibration (Score 0):  0.114\n",
      "Calibration (Score 1):  0.713\n",
      "Parity:  0.354 \n",
      "\n",
      "Accuracy for Asian:   0.844\n",
      "Asian prevalence:  0.482\n",
      "Asian False positive rate:  0.212\n",
      "Asian False negative rate:  0.095\n",
      "Calibration (Score 0):  0.101\n",
      "Calibration (Score 1):  0.798\n",
      "Parity:  0.546\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = 9))\n",
    "model.fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "p, PPV, FPR, FNR, calibration_0, calibration_1, parity = get_statistics(y_hat,y_test)\n",
    "print(\"Overall accuracy: \",round((y_hat == y_test).mean(),3))\n",
    "print(\"Overall prevalence: \",round(p,3))\n",
    "print(\"Overall False positive rate: \",round(FPR,3))\n",
    "print(\"Overall False negative rate: \",round(FNR,3))\n",
    "print(\"Overall Calibration (Score 0): \",round(calibration_0,3))\n",
    "print(\"Overall Calibration (Score 1): \",round(calibration_1,3))\n",
    "print(\"Overall Parity: \",round(parity,3),\"\\n\")\n",
    "\n",
    "white_p, white_PPV, white_FPR, white_FNR, white_calibration_0, white_calibration_1, white_parity = get_statistics(y_hat,y_test,group_test,1)\n",
    "print(\"Accuracy for White: \",round((y_hat == y_test)[group_test == 1].mean(),3))\n",
    "print(\"White prevalence: \",round(white_p,3))\n",
    "print(\"White False positive rate: \",round(white_FPR,3))\n",
    "print(\"White False negative rate: \",round(white_FNR,3))\n",
    "print(\"Calibration (Score 0): \",round(white_calibration_0,3))\n",
    "print(\"Calibration (Score 1): \",round(white_calibration_1,3))\n",
    "print(\"Parity: \",round(white_parity,3),\"\\n\")\n",
    "\n",
    "black_p, black_PPV, black_FPR, black_FNR, black_calibration_0, black_calibration_1, black_parity = get_statistics(y_hat,y_test,group_test,2)\n",
    "print(\"Accuracy for Black:  \",round((y_hat == y_test)[group_test == 2].mean(),3))\n",
    "print(\"Black prevalence: \",round(black_p,3))\n",
    "print(\"Black False positive rate: \",round(black_FPR,3))\n",
    "print(\"Black False negative rate: \",round(black_FNR,3))\n",
    "print(\"Calibration (Score 0): \",round(black_calibration_0,3))\n",
    "print(\"Calibration (Score 1): \",round(black_calibration_1,3))\n",
    "print(\"Parity: \", round(black_parity,3),\"\\n\")\n",
    "\n",
    "two_p, two_PPV, two_FPR, two_FNR, two_calibration_0, two_calibration_1, two_parity = get_statistics(y_hat,y_test,group_test,9)\n",
    "print(\"Accuracy for Two or More Races:  \",round((y_hat == y_test)[group_test == 9].mean(),3))\n",
    "print(\"Two or More Races prevalence: \",round(two_p,3))\n",
    "print(\"Two or More Races False positive rate: \",round(two_FPR,3))\n",
    "print(\"Two or More Races False negative rate: \",round(two_FNR,3))\n",
    "print(\"Calibration (Score 0): \",round(two_calibration_0,3))\n",
    "print(\"Calibration (Score 1): \",round(two_calibration_1,3))\n",
    "print(\"Parity: \", round(two_parity,3),\"\\n\")\n",
    "\n",
    "asian_p, asian_PPV, asian_FPR, asian_FNR, asian_calibration_0, asian_calibration_1, asian_parity = get_statistics(y_hat,y_test,group_test,6)\n",
    "print(\"Accuracy for Asian:  \",round((y_hat == y_test)[group_test == 6].mean(),3))\n",
    "print(\"Asian prevalence: \",round(asian_p,3))\n",
    "print(\"Asian False positive rate: \",round(asian_FPR,3))\n",
    "print(\"Asian False negative rate: \",round(asian_FNR,3))\n",
    "print(\"Calibration (Score 0): \",round(asian_calibration_0,3))\n",
    "print(\"Calibration (Score 1): \",round(asian_calibration_1,3))\n",
    "print(\"Parity: \", round(asian_parity,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319d191",
   "metadata": {},
   "source": [
    "To explain, the accuracy measures how many predictions were correct, the prevalence is the amount employed, the calibration measures how many are actual employed given the score (within the given group), statistical parity measures the scores above 0 that are given.\n",
    "\n",
    "Notice that the overall employment accuracy is similar amongst all groups.\n",
    "\n",
    "Comparing White and Black populations, we can see that the false postive and negative rates closely agree. However, these scores are not calibrated. The calibration for score 0 is about 30 percent larger for White individuals and the calibration for the score of 1 or parity is about 7 percent larger for White individuals. The statistical parity, however, is quite similar. So, while there is error rate balance and statistical parity for these groups, the algorithm is not calibrated.\n",
    "\n",
    "Notice that for individuals or Two or More Races or Asian, the prevalences are significantly different from both the White and Black populations. For individuals of Two or More races, the false negative rate is significantly higher and the calibration is smaller than the White and Black groups. The parity is also significantly lower. For Asian individuals, the false positive rate is much higher, the false negative rate is much lower, and statistical parity is significantly higher than in the White and black groups. Since these groups are not as large as the others, these trends may not be as reliable.\n",
    "\n",
    "Perhaps overall it could be argued that the calibration is similar enough amongst groups. However, clearly the algorithm is not error rate balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34ea25",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Ultimately, the benefit of using these labels for employment status would depend on its usage. Perhaps someone would use this algorithm to predict if individuals of certain backgrounds would need unemployment aid. Perhaps a company would use this to decide if an individual is worth hiring.\n",
    "\n",
    "In any case, this algorithm does not work to combat structural discrimination. Notice that for individuals that are Black or or Two or More Races, the prevalences are markedly lower than for White and Asian individuals. This in itself is likely to self-perpetuate as communities with higher unemployment are more likely to remain so. Even if the error rates were exactly the same amongst groups, this would reinforce the existing distribution in employment. In fact, they would likely to falsely predict that an Asian individual is employed and that an individual of Two or More races is not employed. I find that Asian individuals have a much higher false positive rate to be interesting also - perhaps it is a manifestation of the Model Minority myth (which may stem from higher education amongst Asian groups). Using the examples above, an Asian individual could then mistakenly receive less aid for unemployment. A company using this as a hiring proxy could then underhire those of Two or More Races.\n",
    "\n",
    "I think that the idea of solely using demographic information to determine whether or not an individual is employed is also flawed. It misses any indication of the economy and the amount of opportunities a person may have to find a job. An extra feature accounting for the wealth in the surrounding community could help account for this. There also are not any features that would address the time. This is specific to the year this survey was taken. However, unemployment is not generally chronic thing and can fluctuate depending on the time. Some record of past employment or time unemployed would probably also be an important feature to add.\n",
    "\n",
    "Overall, this algorithm to determine employment status based on demographic characteristics seems to insufficiently capture the factors that would influence employment and, more concerningly, would likely reinforce existing discrimination."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
