[
  {
    "objectID": "posts/perceptron/perceptron_blog.html",
    "href": "posts/perceptron/perceptron_blog.html",
    "title": "Perceptron",
    "section": "",
    "text": "Link to code: https://github.com/jchung2020/jchung2020.github.io/tree/main/posts/perceptron"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#what-is-the-perceptron",
    "href": "posts/perceptron/perceptron_blog.html#what-is-the-perceptron",
    "title": "Perceptron",
    "section": "What is the Perceptron?",
    "text": "What is the Perceptron?\nThe Perceptron is a type of machine learning algorithm called a binary linear classifier. Given data with binary labels, the Perceptron can produce a hyperplane that separates the data according to each labels. Hence prediction only requires knowing the orientation of the point relative to the hyperplane. However, as we shall see, the Perceptron is limited by whether or not the data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#breakdown-of-the-perceptron-algorithm",
    "href": "posts/perceptron/perceptron_blog.html#breakdown-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Breakdown of the Perceptron Algorithm",
    "text": "Breakdown of the Perceptron Algorithm\nThe Perceptron functions by using a weight vector \\(\\bf{w}\\) to characterize the (hopefully) separating hyperplane. After starting with a random initial guess for the weights, we continually update the weights by first choosing a random index i and hence its random point X_i.\nAs a note in the code, I convert the \\(i^{th}\\) label y[i] to y_sign_i with 2*y[i]-1. This step just ensures that instead of being mapped to 0 or 1 as y is, y_sign_i will be mapped to -1 and 1.\nThe main idea of the update is to add y_sign_i*X_i to the weights for points X_i with incorrect labels. This step changes the weight so the label on point X_i will be closer to the correct one. We check if the predicted label is incorrect by checking if the dot product of w and X_i multiplied by y_sign_i is negative or positive. Then, if the signs are the same, this quantity will be positive, and hence the multiplier to the shift in weights is 0 (so no change). Otherwise the weight vector is updated with this shift.\n\nfrom perceptron import Perceptron\n\n#function used to find separating hyperplane\nperceptron.fit(X,y,max_steps)\n\n\"\"\"\nPerceptron update code\n\"\"\"\n\n#take a random index i\ni = np.random.randint(n-1)\n\n#choose point X_i\nX_i = X_[i]\n\n#convert label of point i to -1 or 1\ny_sign_i = 2*y[i]-1\n\n#update weight\nself.w = self.w + int(np.dot(self.w,X_i)*y_sign_i < 0  )*y_sign_i*X_i"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#experiments",
    "href": "posts/perceptron/perceptron_blog.html#experiments",
    "title": "Perceptron",
    "section": "Experiments",
    "text": "Experiments\n\n2D Linearly Separable Data\nBelow I have plotted the given example for running the Perceptron algorithm on data with 2 features.\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100 \np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) \nxlab = plt.xlabel(\"Feature 1\") \nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nRunning the code, we can see that a perfect accuracy of 1.0 is reached. The weight vector corresponds to the example given.\n\nfrom perceptron import Perceptron\n%load_ext autoreload\n%autoreload 2\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\nprint(\"Weight vector: \",p.w)\nprint(\"Final accuracy: \", p.score(X,y))\nprint(\"Last 10 accuracy scores: \",p.history[-10:]) #just the last few values\n\nScore is good enough!\nWeight vector:  [2.10557404 3.1165449  0.25079936]\nFinal accuracy:  1.0\nLast 10 accuracy scores:  [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nThis plot shows the evolution of the accuracy over iterations, which does not always increase.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThe data can now be visualized with the separating line between the two clusters of points.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n2D Non-linearly separable data\nBelow, I run the Perceptron algorithm on the same data, but shifted so that it is just barely not linearly separable.\n\nnp.random.seed(12345)\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.4, -1.4), (1.7, 1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y) \nxlab2 = plt.xlabel(\"Feature 1\") \nylab2 = plt.ylabel(\"Feature 2\")\n\n\n\n\nNow running the Perceptron algorithm:\n\np2 = Perceptron()\np2.fit(X2, y2, max_steps=1000)\nprint(\"Weight vector: \",p2.w)\nprint(\"Final accuracy: \", p2.score(X2,y2))\nprint(\"Last 10 accuracy scores: \",p2.history[-10:]) #just the last few values\n\nWeight vector:  [ 2.56926963  4.22077252 -0.74920064]\nFinal accuracy:  0.98\nLast 10 accuracy scores:  [0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n\n\nWe can see from plotting the accuracy that, while we never converge to a perfect classification after 1000 iterations, the score is still high.\n\nfig2 = plt.plot(p2.history)\nxlab2 = plt.xlabel(\"Iteration\")\nylab2 = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n6D Linearly Separable Data\nThese next points in 6D are not visualizable, but we can still run the Perceptron algorithm.\n\np_features = 7\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, 1.7, 1.7, -1.7, -1.7, 1.7), (1.7, 1.7, 1.7, 1.7, 1.7, 1.7)])\n\np3 = Perceptron()\np3.fit(X3, y3, max_steps=1000)\nprint(\"Weight vector: \",p3.w)\nprint(\"Final accuracy: \", p3.score(X3,y3))\nprint(\"Last 10 accuracy scores: \",p3.history[-10:]) #just the last few values\n\nScore is good enough!\nWeight vector:  [ 5.55526998  1.58997633 -1.00733643  1.04302711  3.54095849 -0.6594312\n  0.95889091]\nFinal accuracy:  1.0\nLast 10 accuracy scores:  [0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 1.0]\n\n\nIn this case, we do actually achieve a 100% classification! The data therefore is linearly separable.\n\nfig3 = plt.plot(p3.history)\nxlab3 = plt.xlabel(\"Iteration\")\nylab3 = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#runtime-complexity",
    "href": "posts/perceptron/perceptron_blog.html#runtime-complexity",
    "title": "Perceptron",
    "section": "Runtime Complexity",
    "text": "Runtime Complexity\nIn equation (1), we have a dot product between \\(\\bf{\\tilde{w}}^{(t)}\\) and \\(\\bf{x_i}\\). As both are dimension \\(p+1\\), this accounts for \\(O(p)\\) term by term multiplication and addition prodcedures. The addition of \\(\\bf{\\tilde{w}}^{(t)}\\) and \\(\\bf{x_i}\\) similarly consists of \\(O(p)\\) additions. So the total complexity is \\(O(p)\\). Since this is for a single index, the total number of points \\(n\\) is irrelevant."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Comparing the analytic and gradient descent methods, regularization, and implementing on some data\n\n\n\n\n\n\nMar 20, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nUsing Machine Learning to classify Antarctic Penguins.\n\n\n\n\n\n\nMar 9, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the convergence of Gradient Descent Algorithms.\n\n\n\n\n\n\nMar 1, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn implementation of the Perceptron Algorithm.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the About page for this blog."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html",
    "href": "posts/gradient_descent/gradient_descent.html",
    "title": "Variations on Gradient Descent",
    "section": "",
    "text": "Link to code: https://github.com/jchung2020/jchung2020.github.io/tree/main/posts/gradient_descent\nHere I will implement three different variations on gradient descent using logistic regression loss: regular gradient descent, stochastic gradient descent, and stochastic gradient descent with momentum.\nWith gradient descent, I begin with a random guess for the minimizer of the loss function. At each point, I compute the gradient and take a step (whose size is modulated by the stepping size) in that direction. This updates the new guess for the minimum, and I continue until the gradient is close to the zero vector.\nFor stochastic gradient descent, I use the same general approach but instead divide the data up into random batches. Additionally, I implemented stochastic gradient descent with momentum, which uses the difference between our current and previous guesses for the momentum update to inform the next update on our guess. This ensures that if we have a good guess for the minimizer, we more quickly head in that direction (and more quickly converge).\nStarting with simple 2D data:\nIn this case, notice that stochastic gradient descent with momentum converges fastest, then stochastic gradient descent, then regular gradient descent.\nAs we can see, the separating lines for all algorithm are quite similar."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html#altering-the-stepping-size",
    "href": "posts/gradient_descent/gradient_descent.html#altering-the-stepping-size",
    "title": "Variations on Gradient Descent",
    "section": "Altering the Stepping Size",
    "text": "Altering the Stepping Size\nBelow is a case I show some larger stepping sizes. It is suprisingly robust, however, as even a large stepping size of 30 yields a reasonable loss. Perhaps the gradient is naturally small.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 30, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stepping size 30\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 50, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stepping size 50\")\n\nlegend = plt.legend() \nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.title(\"Large Stepping Size Comparison on Gradient Descent\")\n\nText(0.5, 1.0, 'Large Stepping Size Comparison on Gradient Descent')\n\n\n\n\n\nAs we can see, the loss never converges for a stepping size of 50, as this proves to be too large for proper gradient descent."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html#altering-the-batch-size",
    "href": "posts/gradient_descent/gradient_descent.html#altering-the-batch-size",
    "title": "Variations on Gradient Descent",
    "section": "Altering the Batch Size",
    "text": "Altering the Batch Size\nHere I demonstrate that the batch size can affect how quickly stochastic gradient search will converge.\n\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 25, \n                  alpha = 0.1) \n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.1) \n\nnum_steps = len(LR1.loss_history)\n\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"loss (batch size 25)\")\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"loss (batch size 10)\")\n#plt.plot(np.arange(num_steps) + 1, LR.history, label = \"accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.title(\"Batch Size Comparison for Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Batch Size Comparison for Stochastic Gradient Descent')\n\n\n\n\n\nAs we can see, a smaller batch size allows for a faster convergence, perhaps because the gradient descent more frequently updates the point of the minimum loss."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html#gradient-descent-in-higher-dimensions",
    "href": "posts/gradient_descent/gradient_descent.html#gradient-descent-in-higher-dimensions",
    "title": "Variations on Gradient Descent",
    "section": "Gradient Descent in Higher Dimensions",
    "text": "Gradient Descent in Higher Dimensions\nBelow I have plotted comparisons of the gradient descent algorithms in 10 dimensions. Notice that the convergence is faster for stochastic gradient with momentum. In fact, all the gradient descent algorithms converge quite quickly.\nI initially used random vectors for the centers of the blobs but it seems that the data may be linearly separable (hence the fast convergence). Perhaps in higher dimensions, this method of generating data makes it easier to separate linearly. For this new data set, I randomized a vector to multiply to one data set so they are not as linearly separable.\n\np_features = 11\n\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(1.72810539, -0.76966727, -8.40792394,  0.00943856,  1.33926325,  1.80831111,\n -8.52324258,  1.39576299,  1.50974231, -6.81079023), (1.2532123,   -0.45271457, -4.67704148,  0.00651874,  1.5479877,   0.84146263,\n -3.45566331,  0.62523429,  0.684547,   -2.54964336)])\n\nfig, axs = plt.subplots(2, figsize=(10, 10))\nfig.suptitle('Vertically stacked subplots')\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.1)\n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"gradient\")\n\n#plt.loglog()\naxs[0].loglog()\naxs[1].loglog()\n\nlegend = axs[0].legend() \nlegend = axs[1].legend() \n\naxs[0].set_title(\"Loss Comparison for Gradient Descent Algorithms\")\naxs[0].set(xlabel = 'Iterations', ylabel= 'Loss')\naxs[1].set_title(\"Accuracy Comparison for Gradient Descent Algorithms\")\naxs[1].set(xlabel = 'Iterations', ylabel= 'Accuracy')\n\n[Text(0.5, 0, 'Iterations'), Text(0, 0.5, 'Accuracy')]\n\n\n\n\n\nStochastic gradient descent with momentum clearly converges the fastest. Stochastic gradient both with and without momentum appear to fluctuate in the loss (probably as a result of the random batches), but both converge faster than regular gradient descent and even reach 100% accuracy."
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Can Machines Recognize Penguins?",
    "section": "",
    "text": "If you are a person, you will have at one point been interested in classifying penguins into their respective species. Most of us can do this by sight, using our judgments of attributes. Maybe some species of penguins generally come in certain colors, maybe some penguins are smaller or larger than others, maybe some have distinguishing features like the chinstraps of the so named Chinstrap penguin.\nWe can do this because people are smart, but most machines or most programs are not really. It would take a lot of data for a machine to recognize a penguin the same way we do.\nI’m being harsh, really machines are good at recognizing certain types of patterns - ones that may be easy for us but hard for them or hard for us and easy for them. So the question stands:\nCan we take a minimal set of observations about penguins and use machine learning to classify them into species?"
  },
  {
    "objectID": "posts/penguins/penguins.html#preliminary-step---inspecting-our-data",
    "href": "posts/penguins/penguins.html#preliminary-step---inspecting-our-data",
    "title": "Can Machines Recognize Penguins?",
    "section": "Preliminary Step - Inspecting Our Data",
    "text": "Preliminary Step - Inspecting Our Data\nWe will focus on this data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\nShow the code\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\nAs we can see, there are quite a few qualitative observations and quantitative measurements tabulated here. We don’t need all the columns for analysis, like the studyName or Comments. It also helps to make attributes like the Island binary, so a 0 or 1 if it belongs to a particular island.\n\n\nShow the code\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\nShow the code\nX_train\n\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nThis is acceptable now.\nThe Culmen refers to the bill, the Delta 15N and 13C are references to blood measurements, and the Clutch refers to their eggs.\n\nIdentify Potential Patterns in Our Data\n\n\n\npenguin_images.png\n\n\nThese images of the three penguins are from https://www.researchgate.net/figure/On-site-images-of-gentoo-P-papua-chinstrap-P-antarctica-and-Adelie-P-adeliae_fig20_318281059. Left to right are the Gentoo, Chinstrap, and Adelie penguins.\nQualitatively, we can note some patterns. The Adelie has a short bill, or a culmen, while the Gentoo and Chinstrap have longer bills. Size wise, it looks like this Adelie is smaller than the others as well.\nWith our preliminary guesses on patterns, we can inspect the actual data. Here, I’ve grouped the species by their quantitative features and taken some averages:\n\n\nShow the code\ntrain_display = train.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\ntrain_display = train_display[train_display[\"Sex\"] != \".\"]\ntrain_display = train_display.dropna()\ntrain_display = pd.get_dummies(train_display,columns=[\"Sex\",\"Island\",\"Stage\",\"Clutch Completion\"])\ntrain_display[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)',\n       'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)',\n       'Delta 13 C (o/oo)']].groupby([\"Species\"]).aggregate(['mean','std'])\n\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n    \n      \n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      38.713208\n      2.787287\n      18.337736\n      1.228429\n      190.500000\n      6.640783\n      3668.160377\n      455.411026\n      8.854705\n      0.442560\n      -25.837840\n      0.574024\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      48.661818\n      3.164823\n      18.423636\n      1.156297\n      195.272727\n      6.948100\n      3713.636364\n      407.363309\n      9.338671\n      0.363423\n      -24.556617\n      0.219188\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      47.851579\n      3.179585\n      15.052632\n      0.970834\n      217.684211\n      6.543249\n      5140.526316\n      507.197910\n      8.245214\n      0.261554\n      -26.166240\n      0.547148\n    \n  \n\n\n\n\nSo we can indeed see the Adelie penguins have a shorter Culmen length than the others, Chinstrap and Gentoo are more similar. The Culmen Depth for Adelie and Chinstrap are more similar than the Gentoo, which has on average a smaller depth. Notice that Flipper Length follows this pattern as well. This is important, it seems that with Culment Length and depth alone we could classify these penguins.\nJust by eye, we can see that the Delta 15 N and Delta 13 C are more similar in values, especially given the standard deviations.\n\n\nShow the code\ntrain_display[['Species','Sex_FEMALE', 'Sex_MALE', 'Island_Biscoe',\n       'Island_Dream', 'Island_Torgersen', 'Stage_Adult, 1 Egg Stage',\n       'Clutch Completion_No', 'Clutch Completion_Yes']].groupby([\"Species\"]).aggregate(['mean','std'])\n\n\n\n\n\n\n  \n    \n      \n      Sex_FEMALE\n      Sex_MALE\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n    \n    \n      \n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      0.518868\n      0.502017\n      0.481132\n      0.502017\n      0.330189\n      0.472515\n      0.358491\n      0.481835\n      0.311321\n      0.465233\n      1.0\n      0.0\n      0.094340\n      0.293689\n      0.905660\n      0.293689\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      0.527273\n      0.503857\n      0.472727\n      0.503857\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      1.0\n      0.0\n      0.200000\n      0.403687\n      0.800000\n      0.403687\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      0.442105\n      0.499272\n      0.557895\n      0.499272\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      1.0\n      0.0\n      0.063158\n      0.244537\n      0.936842\n      0.244537\n    \n  \n\n\n\n\nWith the quantitative features, sex is unsurprisngly not very helpful, as are Clutch Completion and the Egg Stage. The Island, however, seems promising - all Gentoo penguins are on Biscoe, all Chinstraps are on Dream, and Adelie are on all three. This could be useful, maybe the penguins are regional. I’ll say this with caution though, maybe this is also a bias in the data set.\n\n\nShow the code\nimport seaborn as sns\n\ntrain_copy = train.drop([\"Comments\"],axis=1)\ntrain_copy = train_copy.dropna()\ntrain_copy = train_copy[train_copy[\"Sex\"] != \".\"]\ntrain_copy['Species'] = train_copy['Species'].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie')\ntrain_copy['Species'] = train_copy['Species'].replace('Gentoo penguin (Pygoscelis papua)','Gentoo')\ntrain_copy['Species'] = train_copy['Species'].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap')\n#train_copy\ng = sns.FacetGrid(train_copy, col='Species',hue=\"Sex\")\ng.map(sns.histplot, 'Culmen Length (mm)',hatch=\"//\")#,order = ['Adelie Penguin (Pygoscelis adeliae)','Chinstrap penguin (Pygoscelis antarctica)','Gentoo penguin (Pygoscelis papua)'])\n\ng.map(sns.histplot, 'Culmen Depth (mm)',hatch=None)#,order = ['Adelie Penguin (Pygoscelis adeliae)','Chinstrap penguin (Pygoscelis antarctica)','Gentoo penguin (Pygoscelis papua)'])\ng.set_axis_labels(x_var=\"Culmen Measurements (mm)\")\n\nimport matplotlib.patches as mpatches\n\nlength_patch = mpatches.Patch(hatch=\"///\", label='The red data')\ndepth_patch = mpatches.Patch(label='The red data')\n\nplt.legend(title='Legend', loc='upper left', labels=['Culmen Length','Culmen Depth'],handles=[length_patch,depth_patch])\ng.tight_layout()\ng.add_legend()\n\n\nplt.show(g)\n\n\n\n\n\nSeaborns is a helpful tool for visualizing our data instead of trying to conceptualize numbers.\nIgnoring the code, here I have the Culmen Length and Depth for each penguin as histograms. I’ve separated with colors the sexes into male and female as well.\nI’ve separated male and female because of sexual dimorphism - in many animals, the different sexes tend to be different sizes. In fact, I would guess that the males tend to be larger, or at least have larger bill sizes. It seems this is a variable that is important to separate. Without this distinction, our data is strangely bimodal, and has a larger spread than perhaps it should when separated as such.\nSo we can see, Gentoo penguins have Culmen Depths at around 10 mm, Length around 50 mm. The Adelie penguins have larger Culmen Depths closer to 20 mm, and smaller Lengths around 40 mm. The Chinstrap penguins have Culment Depths also around 20 mm, but larger lengths around 50 mm.\nSo just with Culment Depth and Length, we could make some guesses about the species. A large depth but small length is probably and Adelie, a large depth and large length is probably a Chinstrap, a small depth but large length is probably a Gentoo. Our hypothesis: the bill size is important! Darwin would be proud.\n\n\nShow the code\ng = sns.FacetGrid(train_copy, col='Species',row='Sex')\ng.map(sns.histplot, 'Body Mass (g)')\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fbe1a2cef40>\n\n\n\n\n\nSo the males are indeed generally heavier than the females. Penguins are probably sexually dimorphic.\n\n\nShow the code\ng = sns.FacetGrid(train_copy, col='Species',hue=\"Island\")\ng.map(sns.scatterplot, 'Culmen Length (mm)','Flipper Length (mm)')#,order = ['Adelie Penguin (Pygoscelis adeliae)','Chinstrap penguin (Pygoscelis antarctica)','Gentoo penguin (Pygoscelis papua)'])\n#g.map(sns.scatterplot, 'Culmen Length (mm)','Flipper Length (mm)')#,order = ['Adelie Penguin (Pygoscelis adeliae)','Chinstrap penguin (Pygoscelis antarctica)','Gentoo penguin (Pygoscelis papua)'])\ng.add_legend()\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fbe2a585520>\n\n\n\n\n\nI also think Flipper Length and with Culmen Length can separate our species, and it seems that generally the Gentoos are on the top right, Adelies somewhere bottom left, Chinstraps in the middle of the bottom.\nIt seems also that Island, as we observed before, can help classify our species. Biscoe penguins could be Gentoo, Dream penguins could be Chinstrap or Adelie (which flipper/culmen length can distinguish), the rest are Adelies.\n\n\nShow the code\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import SelectPercentile, chi2\n\nX_trans = SelectKBest(f_classif, k=2)\nX_new = X_trans.fit_transform(X_train.loc[:, ~X_train.columns.isin(['Stage_Adult, 1 Egg Stage'])], y_train)\n#print(X_trans.feature_names_in_)\n#print(X_trans.scores_)\n#print(X_trans.pvalues_)\n\nX_feature_scores = pd.DataFrame(X_trans.scores_, index=X_trans.feature_names_in_, columns = [\"Scores\"])\nX_feature_scores = X_feature_scores.sort_values(\"Scores\",ascending=False)\nprint(X_feature_scores)\n\nprint(\"Features selected: \",X_trans.get_feature_names_out())\n\n\n                           Scores\nFlipper Length (mm)    447.490189\nCulmen Length (mm)     304.461212\nBody Mass (g)          292.246094\nCulmen Depth (mm)      260.180770\nIsland_Biscoe          218.758143\nDelta 13 C (o/oo)      182.985274\nIsland_Dream           180.780124\nDelta 15 N (o/oo)      164.389072\nIsland_Torgersen        33.506796\nClutch Completion_No     3.635284\nClutch Completion_Yes    3.635284\nSex_MALE                 0.758649\nSex_FEMALE               0.758649\nFeatures selected:  ['Culmen Length (mm)' 'Flipper Length (mm)']\n\n\nWithout too much detail, this ranks the features in terms of importance. A bit surprising and unsurprising that Culmen Length and Flipper Length rank the highest. Body Mass and Island are also notably important features."
  },
  {
    "objectID": "posts/penguins/penguins.html#choose-a-model",
    "href": "posts/penguins/penguins.html#choose-a-model",
    "title": "Can Machines Recognize Penguins?",
    "section": "Choose A Model",
    "text": "Choose A Model\n\n\nShow the code\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n\nSome test data has been reserved for us, which I will not touch or examine.\nThe models I’ll examine are the Support Vector Machine, Decision Tree Classifier, and the Random Forest Classifier.\nFor Support Vector Machine I modulate the gamma parameter from \\(1^{-5}\\) to \\(10^{5}\\), and for the Decision Tree and Random Forest Classifiers, I iterate out to a maximum depth of 10. My reasoning, which may be flawed, is that there are at most around 6 features (if Island is included), so the depth should be a similar order.\n\n\nShow the code\n#for test all features \nfrom itertools import combinations \n\n#models\nfrom sklearn.svm import SVC #Support Vector Machine\nfrom sklearn.tree import DecisionTreeClassifier #Decision Trees\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\n\nfrom sklearn.model_selection import cross_val_score #cross-validation package\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)']\n\nsvc_model_test_scores = np.array([])\ndecisiontree_model_test_scores = np.array([])\nrandomforest_model_test_scores = np.array([])\n\nsvc_model_max_params = []\ndecisiontree_model_max_params = []\nrandomforest_model_max_params = []\n\ncols_list = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    \n    #for SVC\n    svc_params = 10.0**np.arange(-5, 5)\n    \n    #for DecisionTreeClassifier\n    decisiontree_params = [i for i in range(1,11)]\n    #decisiontree_params = [2]\n    \n    svc_max_score = 0.0\n    svc_max_score_param = 0.0\n    \n    decisiontree_max_score = 0.0\n    decisiontree_max_score_param = 0.0\n    \n    randomforest_max_score = 0.0\n    randomforest_max_score_param = 0.0\n    \n    scores = np.array([])\n    \n    for param in svc_params:\n        loop_model = SVC(gamma  = param)\n        for i in range(10):\n            \n            cv_scores = cross_val_score(loop_model, X_train[cols], y_train, cv = 5)\n            scores = np.append(scores,cv_scores)\n        \n        cv_scores_mean = scores.mean()\n        if ( cv_scores_mean > max_score):\n            svc_max_score = cv_scores_mean\n            svc_max_score_param = param\n            \n    scores = np.array([])\n            \n    for param in decisiontree_params:\n        loop_model = DecisionTreeClassifier(max_depth  = param)\n        \n        for i in range(10):\n            \n            cv_scores = cross_val_score(loop_model, X_train[cols], y_train, cv = 5)\n            scores = np.append(scores,cv_scores)\n        \n        cv_scores_mean = scores.mean()\n        \n        if ( cv_scores_mean > decisiontree_max_score):\n            decisiontree_max_score = cv_scores_mean\n            decisiontree_max_score_param = param\n            \n    scores = np.array([])\n            \n    for param in decisiontree_params:\n        loop_model = RandomForestClassifier(max_depth = param)\n        for i in range(10):\n            \n            cv_scores = cross_val_score(loop_model, X_train[cols], y_train, cv = 5)\n            scores = np.append(scores,cv_scores)\n        \n        cv_scores_mean = scores.mean()\n        \n        if ( cv_scores_mean > randomforest_max_score):\n            randomforest_max_score = cv_scores_mean\n            randomforest_max_score_param = param\n            \n    \n      \n    svc_model = SVC(gamma = svc_max_score_param)\n    decisiontree_model = DecisionTreeClassifier(max_depth = decisiontree_max_score_param)\n    randomforest_model = RandomForestClassifier(max_depth = randomforest_max_score_param)\n    \n    svc_model.fit(X_train[cols],y_train)\n    svc_model_score = svc_model.score(X_test[cols], y_test)\n    \n    decisiontree_model.fit(X_train[cols],y_train)\n    decisiontree_model_score = decisiontree_model.score(X_test[cols], y_test)\n    \n    randomforest_model.fit(X_train[cols],y_train)\n    randomforest_model_score = randomforest_model.score(X_test[cols], y_test)\n    \n    svc_model_test_scores = np.append(svc_model_test_scores,svc_model_score)\n    decisiontree_model_test_scores = np.append(decisiontree_model_test_scores,decisiontree_model_score)\n    randomforest_model_test_scores = np.append(randomforest_model_test_scores,randomforest_model_score)\n    \n    svc_model_max_params.append(str(svc_max_score_param))\n    decisiontree_model_max_params.append(str(decisiontree_max_score_param))\n    randomforest_model_max_params.append(str(randomforest_max_score_param))\n    \n    cols_list.append(cols)\n\n\nOur data set does not have a lot of features, so we can actually train different types of models to see what the best features for classifying penguins are. Essentially, we are testing our hypothesis. By choosing two quantitative and one qualitative feature, we can train our models, use cross validation to find the best model parameters, and output the features corresponding to the highest test scores.\nIf our features are indeed related to the species of the penguin, they should have a high test score. Unless, of course, our data is biased and the features simply recognize the patterns in the bias.\n\n\nShow the code\nsvc_results = sorted(zip(svc_model_test_scores, svc_model_max_params, cols_list), reverse=True)\ndecisiontree_results = sorted(zip(decisiontree_model_test_scores, decisiontree_model_max_params, cols_list), reverse=True)\nrandomforest_results = sorted(zip(randomforest_model_test_scores, randomforest_model_max_params, cols_list), reverse=True)[:3]\n\ndef print_results(results,name):\n    for i in range(3):\n        print(name,\" - Max Test Score: \",results[i][0],\", Parameter: \",results[i][1],\", Features: \",results[i][2])\n\nprint_results(svc_results,\"SVC\")\nprint_results(decisiontree_results,\"DecisionTreeClassifier\")\nprint_results(randomforest_results,\"RandomForestClassifier\")\n\n\nSVC  - Max Test Score:  0.9264705882352942 , Parameter:  1.0 , Features:  ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\nSVC  - Max Test Score:  0.9117647058823529 , Parameter:  1.0 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\nSVC  - Max Test Score:  0.9117647058823529 , Parameter:  1.0 , Features:  ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\nDecisionTreeClassifier  - Max Test Score:  1.0 , Parameter:  10 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\nDecisionTreeClassifier  - Max Test Score:  0.9852941176470589 , Parameter:  10 , Features:  ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\nDecisionTreeClassifier  - Max Test Score:  0.9852941176470589 , Parameter:  10 , Features:  ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nRandomForestClassifier  - Max Test Score:  1.0 , Parameter:  10 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\nRandomForestClassifier  - Max Test Score:  0.9852941176470589 , Parameter:  10 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nRandomForestClassifier  - Max Test Score:  0.9852941176470589 , Parameter:  10 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n\n\nAs we can see, the best model that reaches 100% accuracy on the test data is the RandomForestClassifier. The best features are the sex, Culmen Length, and Flipper Length. Island is a close second for the DecisionTreeClassifier, and first for the Support Vector Machine.\nI do find the sex being important feature interesting. I would have expected Island to be a much better predictor. Perhaps it is because of the bimodal effect that sex has on Culmen Length and Flipper Length. Without accounting for sex, it may be that the Culment Length and Flipper Length, which are primary predictors of the species, have too large a spread to accurately separate."
  },
  {
    "objectID": "posts/penguins/penguins.html#conclusions",
    "href": "posts/penguins/penguins.html#conclusions",
    "title": "Can Machines Recognize Penguins?",
    "section": "Conclusions",
    "text": "Conclusions\nRunning our Random Forest Classifier model, we achieve a perfect testing accuracy.\n\n\nShow the code\npenguin_model = RandomForestClassifier(max_depth = 9)\npenguin_cols = ['Culmen Length (mm)', 'Flipper Length (mm)','Sex_FEMALE', 'Sex_MALE']\npenguin_model.fit(X_train[penguin_cols],y_train)\n\nprint(\"Training accuracy: \",penguin_model.score(X_train[penguin_cols],y_train))\nprint(\"Training accuracy (cross-validation): \",cross_val_score(penguin_model,X_train[penguin_cols],y_train,cv=5).mean())\nprint(\"Testing accuracy: \",penguin_model.score(X_test[penguin_cols],y_test))\n\n\nTraining accuracy:  1.0\nTraining accuracy (cross-validation):  0.9650829562594267\nTesting accuracy:  1.0\n\n\nPerhaps the perfect training accuracy is not so\n\n\nShow the code\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n    \n      print(qual_features[i])\n        \n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n        \n      axarr[i].set_title(str(qual_features[i]))\n\n      plt.legend(title = \"Species\", handles = patches, loc = 'upper left')\n      \n      plt.tight_layout()\n\n\n\n\nShow the code\nplot_regions(penguin_model, X_train[penguin_cols], y_train)\n\n\nSex_FEMALE\nSex_MALE\n\n\n\n\n\nPlotting the decision boundary, we see that it is fairly faithful to the data, whether the sex of the penguins is male or female. Perhaps there are a couple of data points for which it is overfit.\nRegardless, we have demonstrated that for this Palmer Penguins data set, it is possible to use machine learning to accurately classify the species of the penguin - and using on three features!"
  },
  {
    "objectID": "posts/linear_regression/linear_regression.html",
    "href": "posts/linear_regression/linear_regression.html",
    "title": "Investigating Linear Regression",
    "section": "",
    "text": "Link to code: https://github.com/jchung2020/jchung2020.github.io/tree/main/posts/linear_regression"
  },
  {
    "objectID": "posts/linear_regression/linear_regression.html#linear-regression",
    "href": "posts/linear_regression/linear_regression.html#linear-regression",
    "title": "Investigating Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear Regression Algorithms take our data, encoded in the usual feature matrix \\(\\textbf{X}\\), and a target vector \\(\\textbf{y}\\) of values, and creates a prediction for these values.\nThe goal of this blog post is to compare the analytic and gradient methods of Linear Regression, implement LASSO regularization, and implement our algorithms to some data.\n\nAnalytic and Gradient Methods\nAs usual, we start by getting our data. Notice here we have generated the training and validation data randomly. We generate a random weight vector, and with some noise, some data around that weight vector.\n\nfrom linear_regression import LinearRegression\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n%load_ext autoreload\n%autoreload 2\n\nFor simplicity, we start with 2D linear data.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nThe analytic method uses matrix multiplication. As long as \\(\\mathbf{X^{T} X}\\) is invertible, it is possible to find exact solution that will minimize the linear-regression loss. This, however, is computationally costly. If there are \\(p\\) features and \\(n\\) data points, the algorithm will take time \\(O(np^2)\\).\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\nprint(\"Weight vector: \",LR.w)\n\nTraining score = 0.5345\nValidation score = 0.4582\nWeight vector:  [0.74340828 0.67994067]\n\n\nOn the other, using gradient descent can be much faster as each step only takes time \\(O(p^2)\\) and potentially will not need as many steps to achieve a good result.\nAs we can see below, with 100 maximum iterations and a learning rate of 0.001, we achieve a training and validation score that is similar to the analytic algorithm.\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train,max_iter = int(1e2), alpha = 0.001)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\nprint(\"Weight vector: \",LR2.w)\n\nTraining score = 0.5343\nValidation score = 0.462\nWeight vector:  [0.73177951 0.68551826]\n\n\n\nplt.plot(LR2.score_history,label=\"Gradient descent score\")\nplt.hlines(y=LR2.score(X_val, y_val), xmin=0, xmax=int(1e2), linewidth=2, color='r',label=\"Validation Score\")\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\nplt.legend()\nplt.title(\"Linear Regression Gradient Descent Score\")\n\nText(0.5, 1.0, 'Linear Regression Gradient Descent Score')\n\n\n\n\n\n\nfor i in range(len(LR2.score_history)-1):\n    if (LR2.score_history[i+1] < LR2.score_history[i]):\n        print(\"ISSUE!\")\n\nComparing the fits of the analytic and gradient descent methods, we can see that both are close and appear to follow the trend of the data.\n\ndef draw_line(w, x_min, x_max,title,c):\n  x = np.linspace(x_min, x_max, 101)\n  #y = -(w[0]*x + w[2])/w[1]\n  y = (w[0]*x + w[1])\n  plt.plot(x, y, color = c,label=title)\n\n# plot it\nfig = plt.scatter(X_train, y_train)\nfig = draw_line(LR.w, 0, 1,\"analytic\",\"green\")\nfig = draw_line(LR2.w, 0, 1,\"gradient\",\"red\")\n#labs = plt.set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\n#labs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\nplt.legend()\nplt.title(\"Analytic and Gradient Descent Fit Comparison\")\nlabels = plt.gca().set(xlabel = \"x\", ylabel = \"y\")\n\n\n\n\n\n\nBreaking Linear Regression\nIn the above case, we only used 1 feature with 100 data points. However, we can also experiment with the success of the algorithm when we increase the number of features from 1 to 99.\nNote that I use the analytic algorithm so that this reflects the limitations of the linear regression algorithm itself.\n\nn_train = 100\nn_val = 100\n#p_features = 1\nnoise = 0.2\n\np_features_list = [i for i in range(1,n_train)]\n\ntrain_scores = []\nval_scores = []\n\nfor p_features_test in p_features_list:\n    \n    # create some data\n    X_train_test, y_train_test, X_val_test, y_val_test = LR_data(n_train, n_val, p_features_test, noise)\n    \n    LR_test = LinearRegression()\n    LR_test.fit_analytic(X_train_test, y_train_test)\n    \n    train_score = LR_test.score(X_train_test, y_train_test)\n    val_score = LR_test.score(X_val_test, y_val_test)\n    \n    train_scores.append(train_score)\n    val_scores.append(val_score)\n\n\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score\")\n#plt.scatter(p_features_list[80:95],train_scores[80:95])\n#plt.scatter(p_features_list[80:95],val_scores[80:95])\nplt.scatter(p_features_list,train_scores)\nplt.scatter(p_features_list,val_scores)\nplt.legend(labels=[\"Training Score\",\"Validation Score\"])\n#print(train_scores)\n\n<matplotlib.legend.Legend at 0x7fda025f0130>\n\n\n\n\n\nHere I have plotted the training and validation scores. So we can see that the training score continually increases, reaching a perfect score of 1.0 at 99 features.\nAs expected, the training score is always larger than the validation score. We can see that after about 50 features, the validation score drops, dropping to a negative value when close to 99 features are reached. This is a consequence of overfitting. It seems that linear regression naturally overfits if the number of features is close to the number of data points. Perhaps this is not unsurprising since information wise, if we as many features as our data, we can describe our data completely. Of course, this would not predict the trend when we use our training data, hence the overfitting.\nNote that a negative validation score means that our fit is worse than just using the average. We are minimizing the loss \\(|| \\textbf{X}\\textbf{w} - \\textbf{y}||^{2}_{2}\\). But if there are 99 features and 100 data points, really \\(\\textbf{X}\\) is 100 by 100 in dimension (since it is padded by a column of 1s). Hence it could be possible to invert \\(\\textbf{X}\\), a square matrix, which would give a unique solution to \\(\\textbf{X}\\textbf{w} = \\textbf{y}\\). This solution would minimize loss. This may sound good, but this means that our algorithm perfectly minimizes loss to 0 on the training data, hence the perfect 100% score on the training data and the overfitting or poor score on the validation data.\n\n\nFixing Linear Regression with LASSO Regularization\nIt is not great that we can break Linear Regression, since in some cases we may want to use it when we have a lot of features. We can fix this with LASSO Regularization, an addition to the loss function that will specify the weights to be small or zero in value (I assume LASSO is a play on the fact that it lassoes the weights, keeping them small). This should prevent overfitting by practically reducing the actual number of parameters that are significant to our predictions.\nNo surprise here, we use sklearn for our LASSO module.\n\nfrom sklearn.linear_model import Lasso\n\nn_train = 100\nn_val = 100\n#p_features = 1\nnoise = 0.2\n\np_features_list = [i for i in range(1,n_train)]\n\nLasso_train_scores = []\nLasso_val_scores = []\n\nalpha_list = [0.0005,0.001,0.005,0.01]\n\ntrain_scores = []\nval_scores = []\n\nfor p_features_test in p_features_list:\n    \n    # create some data\n    X_train_test, y_train_test, X_val_test, y_val_test = LR_data(n_train = n_train, n_val = n_val, p_features = p_features_test, noise = noise)\n    \n    for alpha in alpha_list:\n        L_test = Lasso(alpha = alpha)\n        L_test.fit(X_train_test, y_train_test)\n        \n        train_score = L_test.score(X_train_test, y_train_test)\n        val_score = L_test.score(X_val_test, y_val_test)\n        \n        Lasso_train_scores.append(train_score)\n        Lasso_val_scores.append(val_score)\n    \n    LR_test = LinearRegression()\n    LR_test.fit_analytic(X_train_test,y_train_test)\n    \n    train_score = LR_test.score(X_train_test, y_train_test)\n    val_score = LR_test.score(X_val_test, y_val_test)\n    \n    train_scores.append(train_score)\n    val_scores.append(val_score)\n\n\nplt.scatter(p_features_list,[Lasso_val_scores[4*i] for i in range(len(p_features_list))]) #alpha = 0.0005\nplt.scatter(p_features_list,[Lasso_val_scores[4*i+1] for i in range(len(p_features_list))]) #alpha = 0.001\nplt.scatter(p_features_list,[Lasso_val_scores[4*i+2] for i in range(len(p_features_list))]) #alpha = 0.005\nplt.scatter(p_features_list,[Lasso_val_scores[4*i+3] for i in range(len(p_features_list))]) #alpha = 0.01\nplt.legend(labels=[\"alpha = 0.0005\",\"alpha = 0.001\",\"alpha = 0.005\",\"alpha = 0.01\"])\n\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\n\nText(0, 0.5, 'Validation Score')\n\n\n\n\n\nVarying the Lasso algorithm over a variety of alpha or regularization parameterizing values, we see that a value of 0.0005 produces the best validation score.\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\n\naxarr[0].scatter(p_features_list,[Lasso_train_scores[4*i] for i in range(len(p_features_list))]) #alpha = 0.0005\naxarr[0].scatter(p_features_list,[Lasso_val_scores[4*i] for i in range(len(p_features_list))]) #alpha = 0.0005\n\naxarr[0].legend(labels=[\"Training Score (Lasso)\",\"Validation Score (Lasso)\"])\n\naxarr[1].scatter(p_features_list,[Lasso_train_scores[4*i] for i in range(len(p_features_list))])\naxarr[1].scatter(p_features_list,[Lasso_val_scores[4*i] for i in range(len(p_features_list))])\naxarr[1].scatter(p_features_list,train_scores)\naxarr[1].scatter(p_features_list[:95],val_scores[:95])\n\naxarr[1].legend(labels=[\"Training Score (Lasso)\",\"Validation Score (Lasso)\",\"Training Score\",\"Validation Score\"])\n\nlabs = axarr[0].set(title = \"LASSO with alpha = 0.0005\", xlabel = \"Number of Features\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"LASSO and Linear Regression Comparison\", xlabel = \"Number of Features\", ylabel = \"Score\")\nplt.tight_layout()\n\n\n\n\nSo we can see that the validation score never becomes negative, though it still drops after about 50 features.\nI have excluded the validation scores that are negative for regular linear regression, and we can see that LASSO is pretty similar for a large number of features (just not very close to 99). Perhaps it is not much better at controlling for overfitting except for a large number of features.\n\n\nApplying Linear Regression to the Bikeshare Data Set\nThe bikeshare data set predicts the number of bicycle riders each day. Some features including the date, temperature, holiday, etc. are given.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n  \n    \n      \n      instant\n      dteday\n      season\n      yr\n      mnth\n      holiday\n      weekday\n      workingday\n      weathersit\n      temp\n      atemp\n      hum\n      windspeed\n      casual\n      registered\n      cnt\n    \n  \n  \n    \n      0\n      1\n      2011-01-01\n      1\n      0\n      1\n      0\n      6\n      0\n      2\n      0.344167\n      0.363625\n      0.805833\n      0.160446\n      331\n      654\n      985\n    \n    \n      1\n      2\n      2011-01-02\n      1\n      0\n      1\n      0\n      0\n      0\n      2\n      0.363478\n      0.353739\n      0.696087\n      0.248539\n      131\n      670\n      801\n    \n    \n      2\n      3\n      2011-01-03\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      0.196364\n      0.189405\n      0.437273\n      0.248309\n      120\n      1229\n      1349\n    \n    \n      3\n      4\n      2011-01-04\n      1\n      0\n      1\n      0\n      2\n      1\n      1\n      0.200000\n      0.212122\n      0.590435\n      0.160296\n      108\n      1454\n      1562\n    \n    \n      4\n      5\n      2011-01-05\n      1\n      0\n      1\n      0\n      3\n      1\n      1\n      0.226957\n      0.229270\n      0.436957\n      0.186900\n      82\n      1518\n      1600\n    \n  \n\n\n\n\n\n# import datetime\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\nPlotting the data, we can see a general trend that during the warmer months from May to September, the number of casual users increases.\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n  \n    \n      \n      casual\n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      331\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      131\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      120\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      108\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      82\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      726\n      247\n      2\n      1\n      1\n      0.254167\n      0.652917\n      0.350133\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      727\n      644\n      2\n      1\n      1\n      0.253333\n      0.590000\n      0.155471\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      728\n      159\n      2\n      0\n      1\n      0.253333\n      0.752917\n      0.124383\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      729\n      364\n      1\n      0\n      1\n      0.255833\n      0.483333\n      0.350754\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      730\n      439\n      2\n      1\n      1\n      0.215833\n      0.577500\n      0.154846\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n731 rows × 19 columns\n\n\n\nHere we limit the features we examine to the month, weather situation, whether it is a working day, the year, temperature, humidity, windspeed, and whether it is a holiday.\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\n\nLR_bike = LinearRegression()\nLR_bike.fit_gradient(X_train,y_train,max_iter=int(1e3),alpha=0.0001)\nprint(\"Training score: \",LR_bike.score(X_train,y_train))\nprint(\"Validation score: \",LR_bike.score(X_test,y_test))\n\nTraining score:  0.721339425993998\nValidation score:  0.6864400405277891\n\n\nI run gradient descent Linear Regression since the number of data points and features is large. With the learning rate of 0.0001, I obtain a training score of about 0.721 and a validation score of about 0.686, which are fairly similar in value.\n\nsorted(zip(list(X_train.columns),LR_bike.w[:len(LR_bike.w)-1]), key=lambda t: abs(t[1]), reverse=True)\n\n[('temp', 1314.3015146737282),\n ('workingday', -783.3759482971046),\n ('mnth_5', 503.01862204941847),\n ('mnth_4', 461.73548463956627),\n ('windspeed', -414.5421760436485),\n ('mnth_6', 382.71007778877566),\n ('mnth_10', 376.9543423574158),\n ('mnth_9', 342.48781356946813),\n ('mnth_3', 309.68758610555506),\n ('yr', 285.99078218441787),\n ('mnth_7', 268.4808717110184),\n ('mnth_8', 241.4745985044163),\n ('holiday', -212.75044485066735),\n ('weathersit', -179.2212011520085),\n ('mnth_11', 178.171611817008),\n ('mnth_2', -73.76995003073193),\n ('hum', -54.32445162745159),\n ('mnth_12', 24.49581557237994)]\n\n\nWe can see that, ordering the parameters in terms of largest magnitude of weight, temperature has the greatest positive influence (higher temperature means more users). Working day is next, being negative (so if not a working day, less of an impact). Whether the month is in April or May has a large impact - riders like to go out in spring, perhaps because it is warmer, perhaps because the rejuvenating landscapes are prettier. A higher windspeed has a negative impact on bike riding.\nIt is a bit interesting that holidays do not have that much of an influence on ridership. Perhaps riders want to rest on holidays instead. The humidity in this model does not matter much either, though it is a negative influence the more humid it is.\n\nplt.scatter(y_train,LR_bike.predict(X_train))\nplt.plot([i for i in range(1,3500)], [i for i in range(1,3500)],color=\"red\")\nplt.xlabel(\"Actual Ridership\")\nplt.ylabel(\"Predicted Ridership\")\n\nText(0, 0.5, 'Predicted Ridership')\n\n\n\n\n\nNotice that the predicted ridership sometimes is negative. The red line indicates when the predicted and actual ridership is equal. About half of the data points are above and below this line, though perhaps it seems the predicted ridership skews a bit higher when actual ridership is lwower.\n\nplt.hist((LR_bike.predict(X_train)-y_train),bins=20)\nplt.xlabel(\"Difference in Predicted and Actual Riders\")\nplt.ylabel(\"Frequency\")\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\nAnd with this histogram, we can see that the difference between our predicted number of riders and the actual is roughly normally distributed, perhaps with some key on the negative side.\nI suppose this is a good sign, it seems to indicate that there is not a strong bias in our data in terms of over or underpredicting."
  }
]