[
  {
    "objectID": "posts/perceptron/perceptron_blog.html",
    "href": "posts/perceptron/perceptron_blog.html",
    "title": "Perceptron",
    "section": "",
    "text": "Link to code: https://github.com/jchung2020/jchung2020.github.io/tree/main/posts/perceptron"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#what-is-the-perceptron",
    "href": "posts/perceptron/perceptron_blog.html#what-is-the-perceptron",
    "title": "Perceptron",
    "section": "What is the Perceptron?",
    "text": "What is the Perceptron?\nThe Perceptron is a type of machine learning algorithm called a binary linear classifier. Given data with binary labels, the Perceptron can produce a hyperplane that separates the data according to each labels. Hence prediction only requires knowing the orientation of the point relative to the hyperplane. However, as we shall see, the Perceptron is limited by whether or not the data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#breakdown-of-the-perceptron-algorithm",
    "href": "posts/perceptron/perceptron_blog.html#breakdown-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Breakdown of the Perceptron Algorithm",
    "text": "Breakdown of the Perceptron Algorithm\nThe Perceptron functions by using a weight vector \\(\\bf{w}\\) to characterize the (hopefully) separating hyperplane. After starting with a random initial guess for the weights, we continually update the weights by first choosing a random index i and hence its random point X_i.\nAs a note in the code, I convert the \\(i^{th}\\) label y[i] to y_sign_i with 2*y[i]-1. This step just ensures that instead of being mapped to 0 or 1 as y is, y_sign_i will be mapped to -1 and 1.\nThe main idea of the update is to add y_sign_i*X_i to the weights for points X_i with incorrect labels. This step changes the weight so the label on point X_i will be closer to the correct one. We check if the predicted label is incorrect by checking if the dot product of w and X_i multiplied by y_sign_i is negative or positive. Then, if the signs are the same, this quantity will be positive, and hence the multiplier to the shift in weights is 0 (so no change). Otherwise the weight vector is updated with this shift.\n\nfrom perceptron import Perceptron\n\n#function used to find separating hyperplane\nperceptron.fit(X,y,max_steps)\n\n\"\"\"\nPerceptron update code\n\"\"\"\n\n#take a random index i\ni = np.random.randint(n-1)\n\n#choose point X_i\nX_i = X_[i]\n\n#convert label of point i to -1 or 1\ny_sign_i = 2*y[i]-1\n\n#update weight\nself.w = self.w + int(np.dot(self.w,X_i)*y_sign_i < 0  )*y_sign_i*X_i"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#experiments",
    "href": "posts/perceptron/perceptron_blog.html#experiments",
    "title": "Perceptron",
    "section": "Experiments",
    "text": "Experiments\n\n2D Linearly Separable Data\nBelow I have plotted the given example for running the Perceptron algorithm on data with 2 features.\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100 \np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) \nxlab = plt.xlabel(\"Feature 1\") \nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nRunning the code, we can see that a perfect accuracy of 1.0 is reached. The weight vector corresponds to the example given.\n\nfrom perceptron import Perceptron\n%load_ext autoreload\n%autoreload 2\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\nprint(\"Weight vector: \",p.w)\nprint(\"Final accuracy: \", p.score(X,y))\nprint(\"Last 10 accuracy scores: \",p.history[-10:]) #just the last few values\n\nScore is good enough!\nWeight vector:  [2.10557404 3.1165449  0.25079936]\nFinal accuracy:  1.0\nLast 10 accuracy scores:  [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nThis plot shows the evolution of the accuracy over iterations, which does not always increase.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThe data can now be visualized with the separating line between the two clusters of points.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n2D Non-linearly separable data\nBelow, I run the Perceptron algorithm on the same data, but shifted so that it is just barely not linearly separable.\n\nnp.random.seed(12345)\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.4, -1.4), (1.7, 1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y) \nxlab2 = plt.xlabel(\"Feature 1\") \nylab2 = plt.ylabel(\"Feature 2\")\n\n\n\n\nNow running the Perceptron algorithm:\n\np2 = Perceptron()\np2.fit(X2, y2, max_steps=1000)\nprint(\"Weight vector: \",p2.w)\nprint(\"Final accuracy: \", p2.score(X2,y2))\nprint(\"Last 10 accuracy scores: \",p2.history[-10:]) #just the last few values\n\nWeight vector:  [ 2.56926963  4.22077252 -0.74920064]\nFinal accuracy:  0.98\nLast 10 accuracy scores:  [0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n\n\nWe can see from plotting the accuracy that, while we never converge to a perfect classification after 1000 iterations, the score is still high.\n\nfig2 = plt.plot(p2.history)\nxlab2 = plt.xlabel(\"Iteration\")\nylab2 = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n6D Linearly Separable Data\nThese next points in 6D are not visualizable, but we can still run the Perceptron algorithm.\n\np_features = 7\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, 1.7, 1.7, -1.7, -1.7, 1.7), (1.7, 1.7, 1.7, 1.7, 1.7, 1.7)])\n\np3 = Perceptron()\np3.fit(X3, y3, max_steps=1000)\nprint(\"Weight vector: \",p3.w)\nprint(\"Final accuracy: \", p3.score(X3,y3))\nprint(\"Last 10 accuracy scores: \",p3.history[-10:]) #just the last few values\n\nScore is good enough!\nWeight vector:  [ 5.55526998  1.58997633 -1.00733643  1.04302711  3.54095849 -0.6594312\n  0.95889091]\nFinal accuracy:  1.0\nLast 10 accuracy scores:  [0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 1.0]\n\n\nIn this case, we do actually achieve a 100% classification! The data therefore is linearly separable.\n\nfig3 = plt.plot(p3.history)\nxlab3 = plt.xlabel(\"Iteration\")\nylab3 = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#runtime-complexity",
    "href": "posts/perceptron/perceptron_blog.html#runtime-complexity",
    "title": "Perceptron",
    "section": "Runtime Complexity",
    "text": "Runtime Complexity\nIn equation (1), we have a dot product between \\(\\bf{\\tilde{w}}^{(t)}\\) and \\(\\bf{x_i}\\). As both are dimension \\(p+1\\), this accounts for \\(O(p)\\) term by term multiplication and addition prodcedures. The addition of \\(\\bf{\\tilde{w}}^{(t)}\\) and \\(\\bf{x_i}\\) similarly consists of \\(O(p)\\) additions. So the total complexity is \\(O(p)\\). Since this is for a single index, the total number of points \\(n\\) is irrelevant."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "We used deep learning to guess a patient’s race based on their chest X-ray.\n\n\n\n\n\n\nMay 10, 2023\n\n\nTrong Le, Jay-U Chung, Kent Canonigo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearching Dr. Gebru’s works for her upcoming talk in Middlebury.\n\n\n\n\n\n\nApr 19, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere we will examine some employment status data from Missouri and demonstrate that it exhibits bias by ethnicity.\n\n\n\n\n\n\nApr 4, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the analytic and gradient descent methods, regularization, and implementing on some data\n\n\n\n\n\n\nMar 20, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nUsing Machine Learning to classify Antarctic Penguins.\n\n\n\n\n\n\nMar 9, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the convergence of Gradient Descent Algorithms.\n\n\n\n\n\n\nMar 1, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the convergence of Gradient Descent Algorithms.\n\n\n\n\n\n\nMar 1, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn implementation of the Perceptron Algorithm.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the About page for this blog."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html",
    "href": "posts/gradient_descent/gradient_descent.html",
    "title": "Variations on Gradient Descent",
    "section": "",
    "text": "Link to code: https://github.com/jchung2020/jchung2020.github.io/tree/main/posts/gradient_descent\nHere I will implement three different variations on gradient descent using logistic regression loss: regular gradient descent, stochastic gradient descent, and stochastic gradient descent with momentum.\nWith gradient descent, I begin with a random guess for the minimizer of the loss function. At each point, I compute the gradient and take a step (whose size is modulated by the stepping size) in that direction. This updates the new guess for the minimum, and I continue until the gradient is close to the zero vector.\nFor stochastic gradient descent, I use the same general approach but instead divide the data up into random batches. Additionally, I implemented stochastic gradient descent with momentum, which uses the difference between our current and previous guesses for the momentum update to inform the next update on our guess. This ensures that if we have a good guess for the minimizer, we more quickly head in that direction (and more quickly converge).\nStarting with simple 2D data:\nIn this case, notice that stochastic gradient descent with momentum converges fastest, then stochastic gradient descent, then regular gradient descent.\nAs we can see, the separating lines for all algorithm are quite similar."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html#altering-the-stepping-size",
    "href": "posts/gradient_descent/gradient_descent.html#altering-the-stepping-size",
    "title": "Variations on Gradient Descent",
    "section": "Altering the Stepping Size",
    "text": "Altering the Stepping Size\nBelow is a case I show some larger stepping sizes. It is suprisingly robust, however, as even a large stepping size of 30 yields a reasonable loss. Perhaps the gradient is naturally small.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 30, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stepping size 30\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 50, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stepping size 50\")\n\nlegend = plt.legend() \nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.title(\"Large Stepping Size Comparison on Gradient Descent\")\n\nText(0.5, 1.0, 'Large Stepping Size Comparison on Gradient Descent')\n\n\n\n\n\nAs we can see, the loss never converges for a stepping size of 50, as this proves to be too large for proper gradient descent."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html#altering-the-batch-size",
    "href": "posts/gradient_descent/gradient_descent.html#altering-the-batch-size",
    "title": "Variations on Gradient Descent",
    "section": "Altering the Batch Size",
    "text": "Altering the Batch Size\nHere I demonstrate that the batch size can affect how quickly stochastic gradient search will converge.\n\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 25, \n                  alpha = 0.1) \n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.1) \n\nnum_steps = len(LR1.loss_history)\n\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"loss (batch size 25)\")\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"loss (batch size 10)\")\n#plt.plot(np.arange(num_steps) + 1, LR.history, label = \"accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.title(\"Batch Size Comparison for Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Batch Size Comparison for Stochastic Gradient Descent')\n\n\n\n\n\nAs we can see, a smaller batch size allows for a faster convergence, perhaps because the gradient descent more frequently updates the point of the minimum loss."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html#gradient-descent-in-higher-dimensions",
    "href": "posts/gradient_descent/gradient_descent.html#gradient-descent-in-higher-dimensions",
    "title": "Variations on Gradient Descent",
    "section": "Gradient Descent in Higher Dimensions",
    "text": "Gradient Descent in Higher Dimensions\nBelow I have plotted comparisons of the gradient descent algorithms in 10 dimensions. Notice that the convergence is faster for stochastic gradient with momentum. In fact, all the gradient descent algorithms converge quite quickly.\nI initially used random vectors for the centers of the blobs but it seems that the data may be linearly separable (hence the fast convergence). Perhaps in higher dimensions, this method of generating data makes it easier to separate linearly. For this new data set, I randomized a vector to multiply to one data set so they are not as linearly separable.\n\np_features = 11\n\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(1.72810539, -0.76966727, -8.40792394,  0.00943856,  1.33926325,  1.80831111,\n -8.52324258,  1.39576299,  1.50974231, -6.81079023), (1.2532123,   -0.45271457, -4.67704148,  0.00651874,  1.5479877,   0.84146263,\n -3.45566331,  0.62523429,  0.684547,   -2.54964336)])\n\nfig, axs = plt.subplots(2, figsize=(10, 10))\nfig.suptitle('Vertically stacked subplots')\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.1)\n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"gradient\")\n\n#plt.loglog()\naxs[0].loglog()\naxs[1].loglog()\n\nlegend = axs[0].legend() \nlegend = axs[1].legend() \n\naxs[0].set_title(\"Loss Comparison for Gradient Descent Algorithms\")\naxs[0].set(xlabel = 'Iterations', ylabel= 'Loss')\naxs[1].set_title(\"Accuracy Comparison for Gradient Descent Algorithms\")\naxs[1].set(xlabel = 'Iterations', ylabel= 'Accuracy')\n\n[Text(0.5, 0, 'Iterations'), Text(0, 0.5, 'Accuracy')]\n\n\n\n\n\nStochastic gradient descent with momentum clearly converges the fastest. Stochastic gradient both with and without momentum appear to fluctuate in the loss (probably as a result of the random batches), but both converge faster than regular gradient descent and even reach 100% accuracy."
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Can Machines Recognize Penguins?",
    "section": "",
    "text": "If you are a person, you will have at one point been interested in classifying penguins into their respective species. Most of us can do this by sight, using our judgments of attributes. Maybe some species of penguins generally come in certain colors, maybe some penguins are smaller or larger than others, maybe some have distinguishing features like the chinstraps of the so named Chinstrap penguin.\nWe can do this because people are smart, but most machines or most programs are not really. It would take a lot of data for a machine to recognize a penguin the same way we do.\nI’m being harsh, really machines are good at recognizing certain types of patterns - ones that may be easy for us but hard for them or hard for us and easy for them. So the question stands:\nCan we take a minimal set of observations about penguins and use machine learning to classify them into species?"
  },
  {
    "objectID": "posts/penguins/penguins.html#preliminary-step---inspecting-our-data",
    "href": "posts/penguins/penguins.html#preliminary-step---inspecting-our-data",
    "title": "Can Machines Recognize Penguins?",
    "section": "Preliminary Step - Inspecting Our Data",
    "text": "Preliminary Step - Inspecting Our Data\nWe will focus on this data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n\n\nShow the code\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\n\nShow the code\ntrain.head()\n\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\nAs we can see, there are quite a few qualitative observations and quantitative measurements tabulated here. We don’t need all the columns for analysis, like the studyName or Comments. It also helps to make attributes like the Island binary, so a 0 or 1 if it belongs to a particular island.\n\n\nShow the code\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\n\n\nShow the code\nX_train\n\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nThis is acceptable now.\nThe Culmen refers to the bill, the Delta 15N and 13C are references to blood measurements, and the Clutch refers to their eggs.\n\nIdentify Potential Patterns in Our Data\n\n\n\npenguin_images.png\n\n\nThese images of the three penguins are from https://www.researchgate.net/figure/On-site-images-of-gentoo-P-papua-chinstrap-P-antarctica-and-Adelie-P-adeliae_fig20_318281059. Left to right are the Gentoo, Chinstrap, and Adelie penguins.\nQualitatively, we can note some patterns. The Adelie has a short bill, or a culmen, while the Gentoo and Chinstrap have longer bills. Size wise, it looks like this Adelie is smaller than the others as well.\nWith our preliminary guesses on patterns, we can inspect the actual data. Here, I’ve grouped the species by their quantitative features and taken some averages:\n\n\nShow the code\ntrain_display = train.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\ntrain_display = train_display[train_display[\"Sex\"] != \".\"]\ntrain_display = train_display.dropna()\ntrain_display = pd.get_dummies(train_display,columns=[\"Sex\",\"Island\",\"Stage\",\"Clutch Completion\"])\ntrain_display[['Species', 'Culmen Length (mm)', 'Culmen Depth (mm)',\n       'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)',\n       'Delta 13 C (o/oo)']].groupby([\"Species\"]).aggregate(['mean','std'])\n\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n    \n      \n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      38.713208\n      2.787287\n      18.337736\n      1.228429\n      190.500000\n      6.640783\n      3668.160377\n      455.411026\n      8.854705\n      0.442560\n      -25.837840\n      0.574024\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      48.661818\n      3.164823\n      18.423636\n      1.156297\n      195.272727\n      6.948100\n      3713.636364\n      407.363309\n      9.338671\n      0.363423\n      -24.556617\n      0.219188\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      47.851579\n      3.179585\n      15.052632\n      0.970834\n      217.684211\n      6.543249\n      5140.526316\n      507.197910\n      8.245214\n      0.261554\n      -26.166240\n      0.547148\n    \n  \n\n\n\n\nSo we can indeed see the Adelie penguins have a shorter Culmen length than the others, Chinstrap and Gentoo are more similar. The Culmen Depth for Adelie and Chinstrap are more similar than the Gentoo, which has on average a smaller depth. Notice that Flipper Length follows this pattern as well. This is important, it seems that with Culment Length and depth alone we could classify these penguins.\nJust by eye, we can see that the Delta 15 N and Delta 13 C are more similar in values, especially given the standard deviations.\n\n\nShow the code\ntrain_display[['Species','Sex_FEMALE', 'Sex_MALE', 'Island_Biscoe',\n       'Island_Dream', 'Island_Torgersen', 'Stage_Adult, 1 Egg Stage',\n       'Clutch Completion_No', 'Clutch Completion_Yes']].groupby([\"Species\"]).aggregate(['mean','std'])\n\n\n\n\n\n\n  \n    \n      \n      Sex_FEMALE\n      Sex_MALE\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n    \n    \n      \n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n      mean\n      std\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      0.518868\n      0.502017\n      0.481132\n      0.502017\n      0.330189\n      0.472515\n      0.358491\n      0.481835\n      0.311321\n      0.465233\n      1.0\n      0.0\n      0.094340\n      0.293689\n      0.905660\n      0.293689\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      0.527273\n      0.503857\n      0.472727\n      0.503857\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      1.0\n      0.0\n      0.200000\n      0.403687\n      0.800000\n      0.403687\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      0.442105\n      0.499272\n      0.557895\n      0.499272\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      1.0\n      0.0\n      0.063158\n      0.244537\n      0.936842\n      0.244537\n    \n  \n\n\n\n\nWith the quantitative features, sex is unsurprisngly not very helpful, as are Clutch Completion and the Egg Stage. The Island, however, seems promising - all Gentoo penguins are on Biscoe, all Chinstraps are on Dream, and Adelie are on all three. This could be useful, maybe the penguins are regional. I’ll say this with caution though, maybe this is also a bias in the data set.\n\n\nShow the code\nimport seaborn as sns\n\ntrain_copy = train.drop([\"Comments\"],axis=1)\ntrain_copy = train_copy.dropna()\ntrain_copy = train_copy[train_copy[\"Sex\"] != \".\"]\ntrain_copy['Species'] = train_copy['Species'].replace('Adelie Penguin (Pygoscelis adeliae)','Adelie')\ntrain_copy['Species'] = train_copy['Species'].replace('Gentoo penguin (Pygoscelis papua)','Gentoo')\ntrain_copy['Species'] = train_copy['Species'].replace('Chinstrap penguin (Pygoscelis antarctica)','Chinstrap')\n#train_copy\ng = sns.FacetGrid(train_copy, col='Species',hue=\"Sex\")\ng.map(sns.histplot, 'Culmen Length (mm)',hatch=\"//\")#,order = ['Adelie Penguin (Pygoscelis adeliae)','Chinstrap penguin (Pygoscelis antarctica)','Gentoo penguin (Pygoscelis papua)'])\n\ng.map(sns.histplot, 'Culmen Depth (mm)',hatch=None)#,order = ['Adelie Penguin (Pygoscelis adeliae)','Chinstrap penguin (Pygoscelis antarctica)','Gentoo penguin (Pygoscelis papua)'])\ng.set_axis_labels(x_var=\"Culmen Measurements (mm)\")\n\nimport matplotlib.patches as mpatches\n\nlength_patch = mpatches.Patch(hatch=\"///\", label='The red data')\ndepth_patch = mpatches.Patch(label='The red data')\n\nplt.legend(title='Legend', loc='upper left', labels=['Culmen Length','Culmen Depth'],handles=[length_patch,depth_patch])\ng.tight_layout()\ng.add_legend()\n\n\nplt.show(g)\n\n\n\n\n\nSeaborns is a helpful tool for visualizing our data instead of trying to conceptualize numbers.\nIgnoring the code, here I have the Culmen Length and Depth for each penguin as histograms. I’ve separated with colors the sexes into male and female as well.\nI’ve separated male and female because of sexual dimorphism - in many animals, the different sexes tend to be different sizes. In fact, I would guess that the males tend to be larger, or at least have larger bill sizes. It seems this is a variable that is important to separate. Without this distinction, our data is strangely bimodal, and has a larger spread than perhaps it should when separated as such.\nSo we can see, Gentoo penguins have Culmen Depths at around 10 mm, Length around 50 mm. The Adelie penguins have larger Culmen Depths closer to 20 mm, and smaller Lengths around 40 mm. The Chinstrap penguins have Culment Depths also around 20 mm, but larger lengths around 50 mm.\nSo just with Culment Depth and Length, we could make some guesses about the species. A large depth but small length is probably and Adelie, a large depth and large length is probably a Chinstrap, a small depth but large length is probably a Gentoo. Our hypothesis: the bill size is important! Darwin would be proud.\n\n\nShow the code\ng = sns.FacetGrid(train_copy, col='Species',row='Sex')\ng.map(sns.histplot, 'Body Mass (g)')\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fbe1a2cef40>\n\n\n\n\n\nSo the males are indeed generally heavier than the females. Penguins are probably sexually dimorphic.\n\n\nShow the code\ng = sns.FacetGrid(train_copy, col='Species',hue=\"Island\")\ng.map(sns.scatterplot, 'Culmen Length (mm)','Flipper Length (mm)')#,order = ['Adelie Penguin (Pygoscelis adeliae)','Chinstrap penguin (Pygoscelis antarctica)','Gentoo penguin (Pygoscelis papua)'])\n#g.map(sns.scatterplot, 'Culmen Length (mm)','Flipper Length (mm)')#,order = ['Adelie Penguin (Pygoscelis adeliae)','Chinstrap penguin (Pygoscelis antarctica)','Gentoo penguin (Pygoscelis papua)'])\ng.add_legend()\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fbe2a585520>\n\n\n\n\n\nI also think Flipper Length and with Culmen Length can separate our species, and it seems that generally the Gentoos are on the top right, Adelies somewhere bottom left, Chinstraps in the middle of the bottom.\nIt seems also that Island, as we observed before, can help classify our species. Biscoe penguins could be Gentoo, Dream penguins could be Chinstrap or Adelie (which flipper/culmen length can distinguish), the rest are Adelies.\n\n\nShow the code\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import SelectPercentile, chi2\n\nX_trans = SelectKBest(f_classif, k=2)\nX_new = X_trans.fit_transform(X_train.loc[:, ~X_train.columns.isin(['Stage_Adult, 1 Egg Stage'])], y_train)\n#print(X_trans.feature_names_in_)\n#print(X_trans.scores_)\n#print(X_trans.pvalues_)\n\nX_feature_scores = pd.DataFrame(X_trans.scores_, index=X_trans.feature_names_in_, columns = [\"Scores\"])\nX_feature_scores = X_feature_scores.sort_values(\"Scores\",ascending=False)\nprint(X_feature_scores)\n\nprint(\"Features selected: \",X_trans.get_feature_names_out())\n\n\n                           Scores\nFlipper Length (mm)    447.490189\nCulmen Length (mm)     304.461212\nBody Mass (g)          292.246094\nCulmen Depth (mm)      260.180770\nIsland_Biscoe          218.758143\nDelta 13 C (o/oo)      182.985274\nIsland_Dream           180.780124\nDelta 15 N (o/oo)      164.389072\nIsland_Torgersen        33.506796\nClutch Completion_No     3.635284\nClutch Completion_Yes    3.635284\nSex_MALE                 0.758649\nSex_FEMALE               0.758649\nFeatures selected:  ['Culmen Length (mm)' 'Flipper Length (mm)']\n\n\nWithout too much detail, this ranks the features in terms of importance. A bit surprising and unsurprising that Culmen Length and Flipper Length rank the highest. Body Mass and Island are also notably important features."
  },
  {
    "objectID": "posts/penguins/penguins.html#choose-a-model",
    "href": "posts/penguins/penguins.html#choose-a-model",
    "title": "Can Machines Recognize Penguins?",
    "section": "Choose A Model",
    "text": "Choose A Model\n\n\nShow the code\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\n\n\nSome test data has been reserved for us, which I will not touch or examine.\nThe models I’ll examine are the Support Vector Machine, Decision Tree Classifier, and the Random Forest Classifier.\nFor Support Vector Machine I modulate the gamma parameter from \\(10^{-5}\\) to \\(10^{5}\\), and for the Decision Tree and Random Forest Classifiers, I iterate out to a maximum depth of 10. My reasoning, which may be flawed, is that there are at most around 6 features (if Island is included), so the depth should be a similar order.\n\n\nShow the code\n#for test all features \nfrom itertools import combinations \n\n#models\nfrom sklearn.svm import SVC #Support Vector Machine\nfrom sklearn.tree import DecisionTreeClassifier #Decision Trees\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\n\nfrom sklearn.model_selection import cross_val_score #cross-validation package\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)','Body Mass (g)']\n\nsvc_model_test_scores = np.array([])\ndecisiontree_model_test_scores = np.array([])\nrandomforest_model_test_scores = np.array([])\n\nsvc_model_max_params = []\ndecisiontree_model_max_params = []\nrandomforest_model_max_params = []\n\ncols_list = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score. \n    \n    #for SVC\n    svc_params = 10.0**np.arange(-5, 5)\n    \n    #for DecisionTreeClassifier\n    decisiontree_params = [i for i in range(1,11)]\n    #decisiontree_params = [2]\n    \n    svc_max_score = 0.0\n    svc_max_score_param = 0.0\n    \n    decisiontree_max_score = 0.0\n    decisiontree_max_score_param = 0.0\n    \n    randomforest_max_score = 0.0\n    randomforest_max_score_param = 0.0\n    \n    scores = np.array([])\n    \n    for param in svc_params:\n        loop_model = SVC(gamma  = param)\n        for i in range(10):\n            \n            cv_scores = cross_val_score(loop_model, X_train[cols], y_train, cv = 5)\n            scores = np.append(scores,cv_scores)\n        \n        cv_scores_mean = scores.mean()\n        if ( cv_scores_mean > max_score):\n            svc_max_score = cv_scores_mean\n            svc_max_score_param = param\n            \n    scores = np.array([])\n            \n    for param in decisiontree_params:\n        loop_model = DecisionTreeClassifier(max_depth  = param)\n        \n        for i in range(10):\n            \n            cv_scores = cross_val_score(loop_model, X_train[cols], y_train, cv = 5)\n            scores = np.append(scores,cv_scores)\n        \n        cv_scores_mean = scores.mean()\n        \n        if ( cv_scores_mean > decisiontree_max_score):\n            decisiontree_max_score = cv_scores_mean\n            decisiontree_max_score_param = param\n            \n    scores = np.array([])\n            \n    for param in decisiontree_params:\n        loop_model = RandomForestClassifier(max_depth = param)\n        for i in range(10):\n            \n            cv_scores = cross_val_score(loop_model, X_train[cols], y_train, cv = 5)\n            scores = np.append(scores,cv_scores)\n        \n        cv_scores_mean = scores.mean()\n        \n        if ( cv_scores_mean > randomforest_max_score):\n            randomforest_max_score = cv_scores_mean\n            randomforest_max_score_param = param\n            \n    \n      \n    svc_model = SVC(gamma = svc_max_score_param)\n    decisiontree_model = DecisionTreeClassifier(max_depth = decisiontree_max_score_param)\n    randomforest_model = RandomForestClassifier(max_depth = randomforest_max_score_param)\n    \n    svc_model.fit(X_train[cols],y_train)\n    svc_model_score = svc_model.score(X_test[cols], y_test)\n    \n    decisiontree_model.fit(X_train[cols],y_train)\n    decisiontree_model_score = decisiontree_model.score(X_test[cols], y_test)\n    \n    randomforest_model.fit(X_train[cols],y_train)\n    randomforest_model_score = randomforest_model.score(X_test[cols], y_test)\n    \n    svc_model_test_scores = np.append(svc_model_test_scores,svc_model_score)\n    decisiontree_model_test_scores = np.append(decisiontree_model_test_scores,decisiontree_model_score)\n    randomforest_model_test_scores = np.append(randomforest_model_test_scores,randomforest_model_score)\n    \n    svc_model_max_params.append(str(svc_max_score_param))\n    decisiontree_model_max_params.append(str(decisiontree_max_score_param))\n    randomforest_model_max_params.append(str(randomforest_max_score_param))\n    \n    cols_list.append(cols)\n\n\nOur data set does not have a lot of features, so we can actually train different types of models to see what the best features for classifying penguins are. Essentially, we are testing our hypothesis. By choosing two quantitative and one qualitative feature, we can train our models, use cross validation to find the best model parameters, and output the features corresponding to the highest test scores.\nIf our features are indeed related to the species of the penguin, they should have a high test score. Unless, of course, our data is biased and the features simply recognize the patterns in the bias.\n\n\nShow the code\nsvc_results = sorted(zip(svc_model_test_scores, svc_model_max_params, cols_list), reverse=True)\ndecisiontree_results = sorted(zip(decisiontree_model_test_scores, decisiontree_model_max_params, cols_list), reverse=True)\nrandomforest_results = sorted(zip(randomforest_model_test_scores, randomforest_model_max_params, cols_list), reverse=True)[:3]\n\ndef print_results(results,name):\n    for i in range(3):\n        print(name,\" - Max Test Score: \",results[i][0],\", Parameter: \",results[i][1],\", Features: \",results[i][2])\n\nprint_results(svc_results,\"SVC\")\nprint_results(decisiontree_results,\"DecisionTreeClassifier\")\nprint_results(randomforest_results,\"RandomForestClassifier\")\n\n\nSVC  - Max Test Score:  0.9264705882352942 , Parameter:  1.0 , Features:  ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\nSVC  - Max Test Score:  0.9117647058823529 , Parameter:  1.0 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\nSVC  - Max Test Score:  0.9117647058823529 , Parameter:  1.0 , Features:  ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\nDecisionTreeClassifier  - Max Test Score:  1.0 , Parameter:  10 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\nDecisionTreeClassifier  - Max Test Score:  0.9852941176470589 , Parameter:  10 , Features:  ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\nDecisionTreeClassifier  - Max Test Score:  0.9852941176470589 , Parameter:  10 , Features:  ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nRandomForestClassifier  - Max Test Score:  1.0 , Parameter:  10 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\nRandomForestClassifier  - Max Test Score:  0.9852941176470589 , Parameter:  10 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nRandomForestClassifier  - Max Test Score:  0.9852941176470589 , Parameter:  10 , Features:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n\n\nAs we can see, the best model that reaches 100% accuracy on the test data is the RandomForestClassifier. The best features are the sex, Culmen Length, and Flipper Length. Island is a close second for the DecisionTreeClassifier, and first for the Support Vector Machine.\nI do find the sex being important feature interesting. I would have expected Island to be a much better predictor. Perhaps it is because of the bimodal effect that sex has on Culmen Length and Flipper Length. Without accounting for sex, it may be that the Culment Length and Flipper Length, which are primary predictors of the species, have too large a spread to accurately separate."
  },
  {
    "objectID": "posts/penguins/penguins.html#conclusions",
    "href": "posts/penguins/penguins.html#conclusions",
    "title": "Can Machines Recognize Penguins?",
    "section": "Conclusions",
    "text": "Conclusions\nRunning our Random Forest Classifier model, we achieve a perfect testing accuracy!\n\n\nShow the code\npenguin_model = RandomForestClassifier(max_depth = 9)\npenguin_cols = ['Culmen Length (mm)', 'Flipper Length (mm)','Sex_FEMALE', 'Sex_MALE']\npenguin_model.fit(X_train[penguin_cols],y_train)\n\nprint(\"Training accuracy: \",penguin_model.score(X_train[penguin_cols],y_train))\nprint(\"Training accuracy (cross-validation): \",cross_val_score(penguin_model,X_train[penguin_cols],y_train,cv=5).mean())\nprint(\"Testing accuracy: \",penguin_model.score(X_test[penguin_cols],y_test))\n\n\nTraining accuracy:  1.0\nTraining accuracy (cross-validation):  0.9650829562594267\nTesting accuracy:  1.0\n\n\nPerhaps the perfect training accuracy is not so ideal. It may be an indicator of overfitting.\n\n\nShow the code\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n    \n      print(qual_features[i])\n        \n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n        \n      axarr[i].set_title(str(qual_features[i]))\n\n      plt.legend(title = \"Species\", handles = patches, loc = 'upper left')\n      \n      plt.tight_layout()\n\n\n\n\nShow the code\nplot_regions(penguin_model, X_train[penguin_cols], y_train)\n\n\nSex_FEMALE\nSex_MALE\n\n\n\n\n\nPlotting the decision boundary, we see that it is fairly faithful to the data, whether the sex of the penguins is male or female. Perhaps there are a couple of data points for which it is overfit.\nRegardless, we have demonstrated that for this Palmer Penguins data set, it is possible to use machine learning to accurately classify the species of the penguin - and using only three features!"
  },
  {
    "objectID": "posts/linear_regression/linear_regression.html",
    "href": "posts/linear_regression/linear_regression.html",
    "title": "Investigating Linear Regression",
    "section": "",
    "text": "Link to code: https://github.com/jchung2020/jchung2020.github.io/tree/main/posts/linear_regression"
  },
  {
    "objectID": "posts/linear_regression/linear_regression.html#linear-regression",
    "href": "posts/linear_regression/linear_regression.html#linear-regression",
    "title": "Investigating Linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear Regression Algorithms take our data, encoded in the usual feature matrix \\(\\textbf{X}\\), and a target vector \\(\\textbf{y}\\) of values, and creates a prediction for these values.\nThe goal of this blog post is to compare the analytic and gradient methods of Linear Regression, implement LASSO regularization, and implement our algorithms to some data.\n\nAnalytic and Gradient Methods\nAs usual, we start by getting our data. Notice here we have generated the training and validation data randomly. We generate a random weight vector, and with some noise, some data around that weight vector.\n\nfrom linear_regression import LinearRegression\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n%load_ext autoreload\n%autoreload 2\n\nFor simplicity, we start with 2D linear data.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nThe analytic method uses matrix multiplication. As long as \\(\\mathbf{X^{T} X}\\) is invertible, it is possible to find exact solution that will minimize the linear-regression loss. This, however, is computationally costly. If there are \\(p\\) features and \\(n\\) data points, the algorithm will take time \\(O(np^2)\\).\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\nprint(\"Weight vector: \",LR.w)\n\nTraining score = 0.5345\nValidation score = 0.4582\nWeight vector:  [0.74340828 0.67994067]\n\n\nOn the other, using gradient descent can be much faster as each step only takes time \\(O(p^2)\\) and potentially will not need as many steps to achieve a good result.\nAs we can see below, with 100 maximum iterations and a learning rate of 0.001, we achieve a training and validation score that is similar to the analytic algorithm.\n\nLR2 = LinearRegression()\n\nLR2.fit_gradient(X_train, y_train,max_iter = int(1e2), alpha = 0.001)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\nprint(\"Weight vector: \",LR2.w)\n\nTraining score = 0.5343\nValidation score = 0.462\nWeight vector:  [0.73177951 0.68551826]\n\n\n\nplt.plot(LR2.score_history,label=\"Gradient descent score\")\nplt.hlines(y=LR2.score(X_val, y_val), xmin=0, xmax=int(1e2), linewidth=2, color='r',label=\"Validation Score\")\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\nplt.legend()\nplt.title(\"Linear Regression Gradient Descent Score\")\n\nText(0.5, 1.0, 'Linear Regression Gradient Descent Score')\n\n\n\n\n\n\nfor i in range(len(LR2.score_history)-1):\n    if (LR2.score_history[i+1] < LR2.score_history[i]):\n        print(\"ISSUE!\")\n\nComparing the fits of the analytic and gradient descent methods, we can see that both are close and appear to follow the trend of the data.\n\ndef draw_line(w, x_min, x_max,title,c):\n  x = np.linspace(x_min, x_max, 101)\n  #y = -(w[0]*x + w[2])/w[1]\n  y = (w[0]*x + w[1])\n  plt.plot(x, y, color = c,label=title)\n\n# plot it\nfig = plt.scatter(X_train, y_train)\nfig = draw_line(LR.w, 0, 1,\"analytic\",\"green\")\nfig = draw_line(LR2.w, 0, 1,\"gradient\",\"red\")\n#labs = plt.set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\n#labs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\nplt.legend()\nplt.title(\"Analytic and Gradient Descent Fit Comparison\")\nlabels = plt.gca().set(xlabel = \"x\", ylabel = \"y\")\n\n\n\n\n\n\nBreaking Linear Regression\nIn the above case, we only used 1 feature with 100 data points. However, we can also experiment with the success of the algorithm when we increase the number of features from 1 to 99.\nNote that I use the analytic algorithm so that this reflects the limitations of the linear regression algorithm itself.\n\nn_train = 100\nn_val = 100\n#p_features = 1\nnoise = 0.2\n\np_features_list = [i for i in range(1,n_train)]\n\ntrain_scores = []\nval_scores = []\n\nfor p_features_test in p_features_list:\n    \n    # create some data\n    X_train_test, y_train_test, X_val_test, y_val_test = LR_data(n_train, n_val, p_features_test, noise)\n    \n    LR_test = LinearRegression()\n    LR_test.fit_analytic(X_train_test, y_train_test)\n    \n    train_score = LR_test.score(X_train_test, y_train_test)\n    val_score = LR_test.score(X_val_test, y_val_test)\n    \n    train_scores.append(train_score)\n    val_scores.append(val_score)\n\n\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score\")\n#plt.scatter(p_features_list[80:95],train_scores[80:95])\n#plt.scatter(p_features_list[80:95],val_scores[80:95])\nplt.scatter(p_features_list,train_scores)\nplt.scatter(p_features_list,val_scores)\nplt.legend(labels=[\"Training Score\",\"Validation Score\"])\n#print(train_scores)\n\n<matplotlib.legend.Legend at 0x7fda025f0130>\n\n\n\n\n\nHere I have plotted the training and validation scores. So we can see that the training score continually increases, reaching a perfect score of 1.0 at 99 features.\nAs expected, the training score is always larger than the validation score. We can see that after about 50 features, the validation score drops, dropping to a negative value when close to 99 features are reached. This is a consequence of overfitting. It seems that linear regression naturally overfits if the number of features is close to the number of data points. Perhaps this is not unsurprising since information wise, if we as many features as our data, we can describe our data completely. Of course, this would not predict the trend when we use our training data, hence the overfitting.\nNote that a negative validation score means that our fit is worse than just using the average. We are minimizing the loss \\(|| \\textbf{X}\\textbf{w} - \\textbf{y}||^{2}_{2}\\). But if there are 99 features and 100 data points, really \\(\\textbf{X}\\) is 100 by 100 in dimension (since it is padded by a column of 1s). Hence it could be possible to invert \\(\\textbf{X}\\), a square matrix, which would give a unique solution to \\(\\textbf{X}\\textbf{w} = \\textbf{y}\\). This solution would minimize loss. This may sound good, but this means that our algorithm perfectly minimizes loss to 0 on the training data, hence the perfect 100% score on the training data and the overfitting or poor score on the validation data.\n\n\nFixing Linear Regression with LASSO Regularization\nIt is not great that we can break Linear Regression, since in some cases we may want to use it when we have a lot of features. We can fix this with LASSO Regularization, an addition to the loss function that will specify the weights to be small or zero in value (I assume LASSO is a play on the fact that it lassoes the weights, keeping them small). This should prevent overfitting by practically reducing the actual number of parameters that are significant to our predictions.\nNo surprise here, we use sklearn for our LASSO module.\n\nfrom sklearn.linear_model import Lasso\n\nn_train = 100\nn_val = 100\n#p_features = 1\nnoise = 0.2\n\np_features_list = [i for i in range(1,n_train)]\n\nLasso_train_scores = []\nLasso_val_scores = []\n\nalpha_list = [0.0005,0.001,0.005,0.01]\n\ntrain_scores = []\nval_scores = []\n\nfor p_features_test in p_features_list:\n    \n    # create some data\n    X_train_test, y_train_test, X_val_test, y_val_test = LR_data(n_train = n_train, n_val = n_val, p_features = p_features_test, noise = noise)\n    \n    for alpha in alpha_list:\n        L_test = Lasso(alpha = alpha)\n        L_test.fit(X_train_test, y_train_test)\n        \n        train_score = L_test.score(X_train_test, y_train_test)\n        val_score = L_test.score(X_val_test, y_val_test)\n        \n        Lasso_train_scores.append(train_score)\n        Lasso_val_scores.append(val_score)\n    \n    LR_test = LinearRegression()\n    LR_test.fit_analytic(X_train_test,y_train_test)\n    \n    train_score = LR_test.score(X_train_test, y_train_test)\n    val_score = LR_test.score(X_val_test, y_val_test)\n    \n    train_scores.append(train_score)\n    val_scores.append(val_score)\n\n\nplt.scatter(p_features_list,[Lasso_val_scores[4*i] for i in range(len(p_features_list))]) #alpha = 0.0005\nplt.scatter(p_features_list,[Lasso_val_scores[4*i+1] for i in range(len(p_features_list))]) #alpha = 0.001\nplt.scatter(p_features_list,[Lasso_val_scores[4*i+2] for i in range(len(p_features_list))]) #alpha = 0.005\nplt.scatter(p_features_list,[Lasso_val_scores[4*i+3] for i in range(len(p_features_list))]) #alpha = 0.01\nplt.legend(labels=[\"alpha = 0.0005\",\"alpha = 0.001\",\"alpha = 0.005\",\"alpha = 0.01\"])\n\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Validation Score\")\n\nText(0, 0.5, 'Validation Score')\n\n\n\n\n\nVarying the Lasso algorithm over a variety of alpha or regularization parameterizing values, we see that a value of 0.0005 produces the best validation score.\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\n\naxarr[0].scatter(p_features_list,[Lasso_train_scores[4*i] for i in range(len(p_features_list))]) #alpha = 0.0005\naxarr[0].scatter(p_features_list,[Lasso_val_scores[4*i] for i in range(len(p_features_list))]) #alpha = 0.0005\n\naxarr[0].legend(labels=[\"Training Score (Lasso)\",\"Validation Score (Lasso)\"])\n\naxarr[1].scatter(p_features_list,[Lasso_train_scores[4*i] for i in range(len(p_features_list))])\naxarr[1].scatter(p_features_list,[Lasso_val_scores[4*i] for i in range(len(p_features_list))])\naxarr[1].scatter(p_features_list,train_scores)\naxarr[1].scatter(p_features_list[:95],val_scores[:95])\n\naxarr[1].legend(labels=[\"Training Score (Lasso)\",\"Validation Score (Lasso)\",\"Training Score\",\"Validation Score\"])\n\nlabs = axarr[0].set(title = \"LASSO with alpha = 0.0005\", xlabel = \"Number of Features\", ylabel = \"Score\")\nlabs = axarr[1].set(title = \"LASSO and Linear Regression Comparison\", xlabel = \"Number of Features\", ylabel = \"Score\")\nplt.tight_layout()\n\n\n\n\nSo we can see that the validation score never becomes negative, though it still drops after about 50 features.\nI have excluded the validation scores that are negative for regular linear regression, and we can see that LASSO is pretty similar for a large number of features (just not very close to 99). Perhaps it is not much better at controlling for overfitting except for a large number of features.\n\n\nApplying Linear Regression to the Bikeshare Data Set\nThe bikeshare data set predicts the number of bicycle riders each day. Some features including the date, temperature, holiday, etc. are given.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbikeshare = pd.read_csv(\"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\nbikeshare.head()\n\n\n\n\n\n  \n    \n      \n      instant\n      dteday\n      season\n      yr\n      mnth\n      holiday\n      weekday\n      workingday\n      weathersit\n      temp\n      atemp\n      hum\n      windspeed\n      casual\n      registered\n      cnt\n    \n  \n  \n    \n      0\n      1\n      2011-01-01\n      1\n      0\n      1\n      0\n      6\n      0\n      2\n      0.344167\n      0.363625\n      0.805833\n      0.160446\n      331\n      654\n      985\n    \n    \n      1\n      2\n      2011-01-02\n      1\n      0\n      1\n      0\n      0\n      0\n      2\n      0.363478\n      0.353739\n      0.696087\n      0.248539\n      131\n      670\n      801\n    \n    \n      2\n      3\n      2011-01-03\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      0.196364\n      0.189405\n      0.437273\n      0.248309\n      120\n      1229\n      1349\n    \n    \n      3\n      4\n      2011-01-04\n      1\n      0\n      1\n      0\n      2\n      1\n      1\n      0.200000\n      0.212122\n      0.590435\n      0.160296\n      108\n      1454\n      1562\n    \n    \n      4\n      5\n      2011-01-05\n      1\n      0\n      1\n      0\n      3\n      1\n      1\n      0.226957\n      0.229270\n      0.436957\n      0.186900\n      82\n      1518\n      1600\n    \n  \n\n\n\n\n\n# import datetime\nfig, ax = plt.subplots(1, figsize = (7, 3))\nax.plot(pd.to_datetime(bikeshare['dteday']), bikeshare['casual'])\nax.set(xlabel = \"Day\", ylabel = \"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\nPlotting the data, we can see a general trend that during the warmer months from May to September, the number of casual users increases.\n\ncols = [\"casual\", \n        \"mnth\", \n        \"weathersit\", \n        \"workingday\",\n        \"yr\",\n        \"temp\", \n        \"hum\", \n        \"windspeed\",\n        \"holiday\"]\n\nbikeshare = bikeshare[cols]\n\nbikeshare = pd.get_dummies(bikeshare, columns = ['mnth'], drop_first = \"if_binary\")\nbikeshare\n\n\n\n\n\n  \n    \n      \n      casual\n      weathersit\n      workingday\n      yr\n      temp\n      hum\n      windspeed\n      holiday\n      mnth_2\n      mnth_3\n      mnth_4\n      mnth_5\n      mnth_6\n      mnth_7\n      mnth_8\n      mnth_9\n      mnth_10\n      mnth_11\n      mnth_12\n    \n  \n  \n    \n      0\n      331\n      2\n      0\n      0\n      0.344167\n      0.805833\n      0.160446\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      131\n      2\n      0\n      0\n      0.363478\n      0.696087\n      0.248539\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      120\n      1\n      1\n      0\n      0.196364\n      0.437273\n      0.248309\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      108\n      1\n      1\n      0\n      0.200000\n      0.590435\n      0.160296\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      82\n      1\n      1\n      0\n      0.226957\n      0.436957\n      0.186900\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      726\n      247\n      2\n      1\n      1\n      0.254167\n      0.652917\n      0.350133\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      727\n      644\n      2\n      1\n      1\n      0.253333\n      0.590000\n      0.155471\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      728\n      159\n      2\n      0\n      1\n      0.253333\n      0.752917\n      0.124383\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      729\n      364\n      1\n      0\n      1\n      0.255833\n      0.483333\n      0.350754\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n    \n      730\n      439\n      2\n      1\n      1\n      0.215833\n      0.577500\n      0.154846\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n731 rows × 19 columns\n\n\n\nHere we limit the features we examine to the month, weather situation, whether it is a working day, the year, temperature, humidity, windspeed, and whether it is a holiday.\n\ntrain, test = train_test_split(bikeshare, test_size = .2, shuffle = False)\n\nX_train = train.drop([\"casual\"], axis = 1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\"], axis = 1)\ny_test = test[\"casual\"]\n\n\nLR_bike = LinearRegression()\nLR_bike.fit_gradient(X_train,y_train,max_iter=int(1e3),alpha=0.0001)\nprint(\"Training score: \",LR_bike.score(X_train,y_train))\nprint(\"Validation score: \",LR_bike.score(X_test,y_test))\n\nTraining score:  0.721339425993998\nValidation score:  0.6864400405277891\n\n\nI run gradient descent Linear Regression since the number of data points and features is large. With the learning rate of 0.0001, I obtain a training score of about 0.721 and a validation score of about 0.686, which are fairly similar in value.\n\nsorted(zip(list(X_train.columns),LR_bike.w[:len(LR_bike.w)-1]), key=lambda t: abs(t[1]), reverse=True)\n\n[('temp', 1314.3015146737282),\n ('workingday', -783.3759482971046),\n ('mnth_5', 503.01862204941847),\n ('mnth_4', 461.73548463956627),\n ('windspeed', -414.5421760436485),\n ('mnth_6', 382.71007778877566),\n ('mnth_10', 376.9543423574158),\n ('mnth_9', 342.48781356946813),\n ('mnth_3', 309.68758610555506),\n ('yr', 285.99078218441787),\n ('mnth_7', 268.4808717110184),\n ('mnth_8', 241.4745985044163),\n ('holiday', -212.75044485066735),\n ('weathersit', -179.2212011520085),\n ('mnth_11', 178.171611817008),\n ('mnth_2', -73.76995003073193),\n ('hum', -54.32445162745159),\n ('mnth_12', 24.49581557237994)]\n\n\nWe can see that, ordering the parameters in terms of largest magnitude of weight, temperature has the greatest positive influence (higher temperature means more users). Working day is next, being negative (so if not a working day, less of an impact). Whether the month is in April or May has a large impact - riders like to go out in spring, perhaps because it is warmer, perhaps because the rejuvenating landscapes are prettier. A higher windspeed has a negative impact on bike riding.\nIt is a bit interesting that holidays do not have that much of an influence on ridership. Perhaps riders want to rest on holidays instead. The humidity in this model does not matter much either, though it is a negative influence the more humid it is.\n\nplt.scatter(y_train,LR_bike.predict(X_train))\nplt.plot([i for i in range(1,3500)], [i for i in range(1,3500)],color=\"red\")\nplt.xlabel(\"Actual Ridership\")\nplt.ylabel(\"Predicted Ridership\")\n\nText(0, 0.5, 'Predicted Ridership')\n\n\n\n\n\nNotice that the predicted ridership sometimes is negative. The red line indicates when the predicted and actual ridership is equal. About half of the data points are above and below this line, though perhaps it seems the predicted ridership skews a bit higher when actual ridership is lwower.\n\nplt.hist((LR_bike.predict(X_train)-y_train),bins=20)\nplt.xlabel(\"Difference in Predicted and Actual Riders\")\nplt.ylabel(\"Frequency\")\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\nAnd with this histogram, we can see that the difference between our predicted number of riders and the actual is roughly normally distributed, perhaps with some key on the negative side.\nI suppose this is a good sign, it seems to indicate that there is not a strong bias in our data in terms of over or underpredicting."
  },
  {
    "objectID": "posts/bias_audit/bias_audit.html",
    "href": "posts/bias_audit/bias_audit.html",
    "title": "A Biased Employment Status Algorithm",
    "section": "",
    "text": "In this blog post, we will be auditing the allocative bias on our algorithm. We will examine data on the employment status in Missouri, focusing on the various self-reported ethnicities. We will then train an algorithm to predict employment status and examine any biases our algorithm might exhibit with respect to these groups."
  },
  {
    "objectID": "posts/bias_audit/bias_audit.html#examining-the-data",
    "href": "posts/bias_audit/bias_audit.html#examining-the-data",
    "title": "A Biased Employment Status Algorithm",
    "section": "Examining the Data",
    "text": "Examining the Data\nOur data can be obtained with the folktables package.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MO\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000034\n      4\n      1\n      400\n      2\n      29\n      1013097\n      77\n      27\n      ...\n      123\n      128\n      134\n      70\n      77\n      74\n      70\n      70\n      16\n      139\n    \n    \n      1\n      P\n      2018GQ0000067\n      4\n      1\n      1001\n      2\n      29\n      1013097\n      73\n      42\n      ...\n      70\n      130\n      72\n      125\n      73\n      77\n      70\n      69\n      127\n      128\n    \n    \n      2\n      P\n      2018GQ0000097\n      4\n      1\n      600\n      2\n      29\n      1013097\n      85\n      20\n      ...\n      86\n      84\n      85\n      12\n      88\n      13\n      90\n      83\n      13\n      14\n    \n    \n      3\n      P\n      2018GQ0000113\n      4\n      1\n      1002\n      2\n      29\n      1013097\n      78\n      26\n      ...\n      80\n      79\n      6\n      75\n      78\n      6\n      77\n      76\n      78\n      78\n    \n    \n      4\n      P\n      2018GQ0000159\n      4\n      1\n      200\n      2\n      29\n      1013097\n      55\n      37\n      ...\n      11\n      57\n      101\n      56\n      57\n      108\n      54\n      13\n      13\n      13\n    \n  \n\n5 rows × 286 columns\n\n\n\nHere we select the possible features, most notably the race (RAC1P) and sex (SEX).\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      27\n      17.0\n      5\n      16\n      2\n      NaN\n      1\n      3.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      1\n      42\n      19.0\n      5\n      16\n      2\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      1\n      2\n      6.0\n    \n    \n      2\n      20\n      19.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      2.0\n      1\n      1\n      1.0\n    \n    \n      3\n      26\n      17.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      1\n      2\n      6.0\n    \n    \n      4\n      37\n      16.0\n      5\n      16\n      1\n      NaN\n      1\n      3.0\n      4.0\n      1\n      1\n      2\n      1\n      1.0\n      1\n      2\n      6.0\n    \n  \n\n\n\n\n\ntarget_features = [\"ESR\", \"RAC1P\"]\nfeatures_to_use = [f for f in possible_features if f not in target_features]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nWe have now transformed our data so that it can be processed in a classification algorithm. Our task is to predic the employment status of civilians at work (excluding military). Of course, we split the training and testing data, which in this case is an 80-20 split.\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nLet’s now examine some demographic characteristic of our training data.\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\ndf_nums = df.groupby([\"group\"]).size().reset_index(name = \"Number\")\ndf_nums[\"group\"] = ['White alone','Black or African American alone','American Indian alone','Alaska Native alone','American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races','Asian alone','Native Hawaiian and Other Pacific Islander alone',' Some Other Race alone','Two or More Races']\ndf_nums = df_nums.rename(index = {0:1,1:2,2:3,3:4,4:5,5:6,6:7,7:8,8:9})\ndf_nums['Percentage'] = df_nums['Number']/df.shape[0]*100.0\ndf_nums\n\n\n\n\n\n  \n    \n      \n      group\n      Number\n      Percentage\n    \n  \n  \n    \n      1\n      White alone\n      43713\n      87.545061\n    \n    \n      2\n      Black or African American alone\n      3838\n      7.686454\n    \n    \n      3\n      American Indian alone\n      150\n      0.300409\n    \n    \n      4\n      Alaska Native alone\n      5\n      0.010014\n    \n    \n      5\n      American Indian and Alaska Native tribes speci...\n      27\n      0.054074\n    \n    \n      6\n      Asian alone\n      762\n      1.526075\n    \n    \n      7\n      Native Hawaiian and Other Pacific Islander alone\n      51\n      0.102139\n    \n    \n      8\n      Some Other Race alone\n      316\n      0.632861\n    \n    \n      9\n      Two or More Races\n      1070\n      2.142914\n    \n  \n\n\n\n\nSo we can see that an overwhelming majority of our data is White, then Black, Two or More Races, and Asian. For my analysis, I will only focus on these groups as the others have too small a sample size.\n\ndf_employed = df.groupby([\"group\",\"label\"]).size().groupby(\"group\", group_keys=False).apply(lambda x: 100 * x / x.sum()).reset_index(name = \"Percentage\").replace({1:'White alone',2:'Black or African American alone',3:' American Indian alone',4:'Alaska Native alone',5:'American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races',6:'Asian alone',7:'Native Hawaiian and Other Pacific Islander alone',8:' Some Other Race alone',9:'Two or More Races'})\ndf_employed = df_employed.rename(columns = {'label':'Employed'})\n\nprint(\"Overall employment rate: \",(df[\"label\"] == 1.0).mean())\ndf_employed['Diff. with Overall Avg.'] = df_employed['Percentage'] - (df[\"label\"] == 1.0).mean()*np.ones(df_employed.shape[0])*100\ndf_employed[df_employed[\"Employed\"] == True]\n\nOverall employment rate:  0.4467275494672755\n\n\n\n\n\n\n  \n    \n      \n      group\n      Employed\n      Percentage\n      Diff. with Overall Avg.\n    \n  \n  \n    \n      1\n      White alone\n      True\n      45.439572\n      0.766817\n    \n    \n      3\n      Black or African American alone\n      True\n      39.577905\n      -5.094850\n    \n    \n      5\n      American Indian alone\n      True\n      46.000000\n      1.327245\n    \n    \n      7\n      Alaska Native alone\n      True\n      60.000000\n      15.327245\n    \n    \n      9\n      American Indian and Alaska Native tribes speci...\n      True\n      40.740741\n      -3.932014\n    \n    \n      11\n      Asian alone\n      True\n      49.343832\n      4.671077\n    \n    \n      13\n      Native Hawaiian and Other Pacific Islander alone\n      True\n      52.941176\n      8.268422\n    \n    \n      15\n      Some Other Race alone\n      True\n      37.341772\n      -7.330983\n    \n    \n      17\n      Two or More Races\n      True\n      29.906542\n      -14.766213\n    \n  \n\n\n\n\nSo in comparison to the overall average, White individuals are about the same, Black individuals are less employed, individuals of two or more races are much less employed, Asian individuals are slightly more employed.\n\nimport seaborn as sb\ndf_plot = df.groupby([\"group\",\"SEX\",\"label\"]).size().groupby([\"group\",\"SEX\"], group_keys=False).apply(lambda x: 100 * x / x.sum()).reset_index(name = \"Percentage\")\ndf_plot['SEX'] = df_plot['SEX'].replace([1.0,2.0],['Male','Female'])\n\nsb.barplot(x=\"group\",\n           y=\"Percentage\",\n           hue=\"SEX\",\n           data=df_plot[df_plot[\"label\"]==True])\n\n<AxesSubplot: xlabel='group', ylabel='Percentage'>\n\n\n\n\n\nPlotting the employment rates by sex, we can see that there are clear disparities between overall employment and male and female employment.\nFor White and Asian individuals, males are more likely to be employed than females. For Black individuals and those from Two or More races, females are more likely to be employed."
  },
  {
    "objectID": "posts/bias_audit/bias_audit.html#training-an-algorithm-to-predict-employment-status",
    "href": "posts/bias_audit/bias_audit.html#training-an-algorithm-to-predict-employment-status",
    "title": "A Biased Employment Status Algorithm",
    "section": "Training an Algorithm to Predict Employment Status",
    "text": "Training an Algorithm to Predict Employment Status\nThe main classification algorithms we have on hand from Scikit Learn are Logistic Regression, Support Vector Machine, Decision Tree Classifier, and the Random Forest Classifier algorithm.\nEach model has a parameter that must be tuned. For Logistic Regression, this is the degree for polynomial features, for Support Vector Machine the regularization parameter, for Decision Trees and the Random Forest, the max depth. I define a function below that will take a model, tune its optimal parameter using cross validation, and output this optimal parameter.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\ndef train_opt_model(input_model, kwargs, params, X_train, y_train, X_test, y_test):\n    keys = list(kwargs.keys())\n    best_score = 0.0\n    best_param = params[0]\n    \n    for param in params:\n        kwargs[keys[0]] = param #should be only one argument in this case\n        model = make_pipeline(StandardScaler(), input_model(**kwargs))\n        train_score = cross_val_score(model, X_train, y_train, cv=5).mean()\n        if (train_score > best_score):\n            best_score = train_score\n            best_param = param\n                              \n    print(\"Best model parameter: \",best_param)    \n    kwargs[keys[0]] = best_param\n    best_model = make_pipeline(StandardScaler(), input_model(**kwargs))\n    best_model.fit(X_train,y_train)\n    print(\"Best model training score: \",best_model.score(X_train,y_train))\n    print(\"Best model test score: \",best_model.score(X_test,y_test))\n    return kwargs\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef poly_LR(degree, **kwargs):\n    plr = Pipeline([(\"poly\", PolynomialFeatures(degree = degree)),\n                    (\"LR\", LogisticRegression(**kwargs))])\n    return plr\n\nparams = [i for i in range(3,4)]\nkwargs = {\"degree\":params[0],\"max_iter\":5000}\nkwargs = train_opt_model(poly_LR, kwargs, params, X_train, y_train, X_test, y_test)\nmodel = make_pipeline(StandardScaler(), poly_LR(**kwargs))\nmodel.fit(X_train, y_train)\ny_hat = model.predict(X_test)\nprint(\"Overall employment accuracy: \",(y_hat == y_test).mean())\nprint(\"Employment accuracy for white: \",(y_hat == y_test)[group_test == 1].mean())\nprint(\"Employment accuracy for black:  \",(y_hat == y_test)[group_test == 2].mean())\n\n\nfrom sklearn.svm import SVC\nparams = [0.1]\nkwargs = {\"C\":params[0]}\nkwargs = train_opt_model(SVC, kwargs, params, X_train, y_train, X_test, y_test)\nmodel = make_pipeline(StandardScaler(), SVC(**kwargs))\nmodel.fit(X_train, y_train)\ny_hat = model.predict(X_test)\nprint(\"Overall employment accuracy: \",(y_hat == y_test).mean())\nprint(\"Employment accuracy for white: \",(y_hat == y_test)[group_test == 1].mean())\nprint(\"Employment accuracy for black:  \",(y_hat == y_test)[group_test == 2].mean())\n\nconfusion_matrix_SVC = confusion_matrix(y_test,y_hat,normalize='true')\nprint(\"False positive rate: \",confusion_matrix_SVC[0][1])\nprint(\"False negative rate: \",confusion_matrix_SVC[1][0])\nprint(confusion_matrix(y_test,y_hat)[1][1]/(confusion_matrix(y_test,y_hat)[1][1]+confusion_matrix(y_test,y_hat)[0][1]))\n\nconfusion_matrix_SVC = confusion_matrix(y_test[group_test == 1],y_hat[group_test == 1],normalize='true')\nprint(\"White False positive rate: \",confusion_matrix_SVC[0][1])\nprint(\"White False negative rate: \",confusion_matrix_SVC[1][0])\nprint(confusion_matrix(y_test[group_test == 1],y_hat[group_test == 1])[1][1]/(confusion_matrix(y_test[group_test == 1],y_hat[group_test == 1])[1][1]+confusion_matrix(y_test[group_test == 1],y_hat[group_test == 1])[0][1]))\n\nconfusion_matrix_SVC = confusion_matrix(y_test[group_test == 2],y_hat[group_test == 2],normalize='true')\nprint(\"Black False positive rate: \",confusion_matrix_SVC[0][1])\nprint(\"Black False negative rate: \",confusion_matrix_SVC[1][0])\nprint(confusion_matrix(y_test[group_test == 2],y_hat[group_test == 2])[1][1]/(confusion_matrix(y_test[group_test == 2],y_hat[group_test == 2])[1][1]+confusion_matrix(y_test[group_test == 2],y_hat[group_test == 2])[0][1]))\n\nSpoiler for these two algorithms - I tried running them on my machine, and they both took an extraordinarily long time! Testing with some parameters also yields a similar training score to the algorithm we will use anyway.\n\nfrom sklearn.tree import DecisionTreeClassifier\nparams = [i for i in range(5,15)]\nkwargs = {\"max_depth\":params[0]}\nkwargs = train_opt_model(DecisionTreeClassifier, kwargs, params, X_train, y_train, X_test, y_test)\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(**kwargs))\nmodel.fit(X_train, y_train)\ny_hat = model.predict(X_test)\nprint(\"Overall employment accuracy: \",(y_hat == y_test).mean())\nprint(\"Employment accuracy for white: \",(y_hat == y_test)[group_test == 1].mean())\nprint(\"Employment accuracy for black:  \",(y_hat == y_test)[group_test == 2].mean())\n\nBest model parameter:  9\nBest model training score:  0.8429263798766322\nBest model test score:  0.8404357577699455\nOverall employment accuracy:  0.8403556552387056\nEmployment accuracy for white:  0.8408361602640506\nEmployment accuracy for black:   0.841031149301826\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nparams = [i for i in range(5,15)]\nkwargs = {\"max_depth\":params[0]}\nkwargs = train_opt_model(RandomForestClassifier, kwargs, params, X_train, y_train, X_test, y_test)\nmodel = make_pipeline(StandardScaler(), RandomForestClassifier(**kwargs))\nmodel.fit(X_train, y_train)\ny_hat = model.predict(X_test)\nprint(\"Overall employment accuracy: \",(y_hat == y_test).mean())\nprint(\"Employment accuracy for white: \",(y_hat == y_test)[group_test == 1].mean())\nprint(\"Employment accuracy for black:  \",(y_hat == y_test)[group_test == 2].mean())\n\nBest model parameter:  14\nBest model training score:  0.8644356324601458\nBest model test score:  0.8415571932073054\nOverall employment accuracy:  0.8416372957385453\nEmployment accuracy for white:  0.8411112129824884\nEmployment accuracy for black:   0.8431793770139635\n\n\nWe can see that the training score for the Decision Tree and Random Forest algorithms are pretty similar. The Random Forest algorithm does have a slightly better training score and test score, but I will opt to use the Decision Tree algorithm because it is faster. Moreover, I think it would not be ethical to have randomness in an algorithm that predicts employment status."
  },
  {
    "objectID": "posts/bias_audit/bias_audit.html#analyzing-our-employment-classification-algorithm",
    "href": "posts/bias_audit/bias_audit.html#analyzing-our-employment-classification-algorithm",
    "title": "A Biased Employment Status Algorithm",
    "section": "Analyzing our Employment Classification Algorithm",
    "text": "Analyzing our Employment Classification Algorithm\nWe can now define a function that will make some statistic easier to calculate, and run our Decision Tree with a max depth of 9.\n\ndef get_statistics(y_hat,y_test,group_test=None, group_test_val = None):\n    if (group_test_val == None):\n        y_t = y_test\n        y_h = y_hat\n    else:\n        y_t = y_test[group_test == group_test_val]\n        y_h = y_hat[group_test == group_test_val]\n        \n    CM =  confusion_matrix(y_t,y_h)\n    \n    p = (1*y_t).mean() #mean of how many in group are employed\n    PPV = CM[1][1]/(CM[1][1] + CM[0][1])\n    \n    FPR = CM[0][1]/(CM[0][1] + CM[0][0])\n    FNR = CM[1][0]/(CM[1][0] + CM[1][1])\n    \n    calibration_0 = ((y_t == 1)*(y_h == 0)).sum()/((y_h == 0).sum())\n    calibration_1 = ((y_t == 1)*(y_h == 1)).sum()/((y_h == 1).sum())\n    \n    parity = (y_h > 0).mean() #threshold taken to be 0\n    \n    return p, PPV, FPR, FNR, calibration_0, calibration_1, parity\n\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = 9))\nmodel.fit(X_train, y_train)\ny_hat = model.predict(X_test)\n\np, PPV, FPR, FNR, calibration_0, calibration_1, parity = get_statistics(y_hat,y_test)\nprint(\"Overall accuracy: \",round((y_hat == y_test).mean(),3))\nprint(\"Overall prevalence: \",round(p,3))\nprint(\"Overall False positive rate: \",round(FPR,3))\nprint(\"Overall False negative rate: \",round(FNR,3))\nprint(\"Overall Calibration (Score 0): \",round(calibration_0,3))\nprint(\"Overall Calibration (Score 1): \",round(calibration_1,3))\nprint(\"Overall Parity: \",round(parity,3),\"\\n\")\n\nwhite_p, white_PPV, white_FPR, white_FNR, white_calibration_0, white_calibration_1, white_parity = get_statistics(y_hat,y_test,group_test,1)\nprint(\"Accuracy for White: \",round((y_hat == y_test)[group_test == 1].mean(),3))\nprint(\"White prevalence: \",round(white_p,3))\nprint(\"White False positive rate: \",round(white_FPR,3))\nprint(\"White False negative rate: \",round(white_FNR,3))\nprint(\"Calibration (Score 0): \",round(white_calibration_0,3))\nprint(\"Calibration (Score 1): \",round(white_calibration_1,3))\nprint(\"Parity: \",round(white_parity,3),\"\\n\")\n\nblack_p, black_PPV, black_FPR, black_FNR, black_calibration_0, black_calibration_1, black_parity = get_statistics(y_hat,y_test,group_test,2)\nprint(\"Accuracy for Black:  \",round((y_hat == y_test)[group_test == 2].mean(),3))\nprint(\"Black prevalence: \",round(black_p,3))\nprint(\"Black False positive rate: \",round(black_FPR,3))\nprint(\"Black False negative rate: \",round(black_FNR,3))\nprint(\"Calibration (Score 0): \",round(black_calibration_0,3))\nprint(\"Calibration (Score 1): \",round(black_calibration_1,3))\nprint(\"Parity: \", round(black_parity,3),\"\\n\")\n\ntwo_p, two_PPV, two_FPR, two_FNR, two_calibration_0, two_calibration_1, two_parity = get_statistics(y_hat,y_test,group_test,9)\nprint(\"Accuracy for Two or More Races:  \",round((y_hat == y_test)[group_test == 9].mean(),3))\nprint(\"Two or More Races prevalence: \",round(two_p,3))\nprint(\"Two or More Races False positive rate: \",round(two_FPR,3))\nprint(\"Two or More Races False negative rate: \",round(two_FNR,3))\nprint(\"Calibration (Score 0): \",round(two_calibration_0,3))\nprint(\"Calibration (Score 1): \",round(two_calibration_1,3))\nprint(\"Parity: \", round(two_parity,3),\"\\n\")\n\nasian_p, asian_PPV, asian_FPR, asian_FNR, asian_calibration_0, asian_calibration_1, asian_parity = get_statistics(y_hat,y_test,group_test,6)\nprint(\"Accuracy for Asian:  \",round((y_hat == y_test)[group_test == 6].mean(),3))\nprint(\"Asian prevalence: \",round(asian_p,3))\nprint(\"Asian False positive rate: \",round(asian_FPR,3))\nprint(\"Asian False negative rate: \",round(asian_FNR,3))\nprint(\"Calibration (Score 0): \",round(asian_calibration_0,3))\nprint(\"Calibration (Score 1): \",round(asian_calibration_1,3))\nprint(\"Parity: \", round(asian_parity,3))\n\nOverall accuracy:  0.841\nOverall prevalence:  0.453\nOverall False positive rate:  0.177\nOverall False negative rate:  0.138\nOverall Calibration (Score 0):  0.122\nOverall Calibration (Score 1):  0.801\nOverall Parity:  0.488 \n\nAccuracy for White:  0.841\nWhite prevalence:  0.462\nWhite False positive rate:  0.178\nWhite False negative rate:  0.136\nCalibration (Score 0):  0.125\nCalibration (Score 1):  0.806\nParity:  0.495 \n\nAccuracy for Black:   0.841\nBlack prevalence:  0.376\nBlack False positive rate:  0.17\nBlack False negative rate:  0.14\nCalibration (Score 0):  0.092\nCalibration (Score 1):  0.752\nParity:  0.43 \n\nAccuracy for Two or More Races:   0.825\nTwo or More Races prevalence:  0.326\nTwo or More Races False positive rate:  0.151\nTwo or More Races False negative rate:  0.226\nCalibration (Score 0):  0.114\nCalibration (Score 1):  0.713\nParity:  0.354 \n\nAccuracy for Asian:   0.844\nAsian prevalence:  0.482\nAsian False positive rate:  0.212\nAsian False negative rate:  0.095\nCalibration (Score 0):  0.101\nCalibration (Score 1):  0.798\nParity:  0.546\n\n\nTo explain, the accuracy measures how many predictions were correct, the prevalence is the amount employed, the calibration measures how many are actual employed given the score (within the given group), statistical parity measures the scores above 0 that are given.\nNotice that the overall employment accuracy is similar amongst all groups.\nComparing White and Black populations, we can see that the false postive and negative rates closely agree. However, these scores are not calibrated. The calibration for score 0 is about 30 percent larger for White individuals and the calibration for the score of 1 or parity is about 7 percent larger for White individuals. The statistical parity, however, is quite similar. So, while there is error rate balance and statistical parity for these groups, the algorithm is not calibrated.\nNotice that for individuals or Two or More Races or Asian, the prevalences are significantly different from both the White and Black populations. For individuals of Two or More races, the false negative rate is significantly higher and the calibration is smaller than the White and Black groups. The parity is also significantly lower. For Asian individuals, the false positive rate is much higher, the false negative rate is much lower, and statistical parity is significantly higher than in the White and black groups. Since these groups are not as large as the others, these trends may not be as reliable.\nPerhaps overall it could be argued that the calibration is similar enough amongst groups. However, clearly the algorithm is not error rate balanced."
  },
  {
    "objectID": "posts/bias_audit/bias_audit.html#conclusions",
    "href": "posts/bias_audit/bias_audit.html#conclusions",
    "title": "A Biased Employment Status Algorithm",
    "section": "Conclusions",
    "text": "Conclusions\nUltimately, the benefit of using these labels for employment status would depend on its usage. Perhaps someone would use this algorithm to predict if individuals of certain backgrounds would need unemployment aid. Perhaps a company would use this to decide if an individual is worth hiring.\nIn any case, this algorithm does not work to combat structural discrimination. Notice that for individuals that are Black or or Two or More Races, the prevalences are markedly lower than for White and Asian individuals. This in itself is likely to self-perpetuate as communities with higher unemployment are more likely to remain so. Even if the error rates were exactly the same amongst groups, this would reinforce the existing distribution in employment. In fact, they would likely to falsely predict that an Asian individual is employed and that an individual of Two or More races is not employed. I find that Asian individuals have a much higher false positive rate to be interesting also - perhaps it is a manifestation of the Model Minority myth (which may stem from higher education amongst Asian groups). Using the examples above, an Asian individual could then mistakenly receive less aid for unemployment. A company using this as a hiring proxy could then underhire those of Two or More Races.\nI think that the idea of solely using demographic information to determine whether or not an individual is employed is also flawed. It misses any indication of the economy and the amount of opportunities a person may have to find a job. An extra feature accounting for the wealth in the surrounding community could help account for this. There also are not any features that would address the time. This is specific to the year this survey was taken. However, unemployment is not generally chronic thing and can fluctuate depending on the time. Some record of past employment or time unemployed would probably also be an important feature to add.\nOverall, this algorithm to determine employment status based on demographic characteristics seems to insufficiently capture the factors that would influence employment and, more concerningly, would likely reinforce existing discrimination."
  },
  {
    "objectID": "posts/dr_gebru/dr_gebru.html",
    "href": "posts/dr_gebru/dr_gebru.html",
    "title": "Dr. Timnit Gebru and AI Ethics",
    "section": "",
    "text": "Dr. Timnit Gebru is an acclaimed computer scientist whose work focuses on algorithmic bias and fairness in computing. She has worked as an AI researcher for Microsoft and on Google’s team on ethics in artificial intelligence. Some of her most notable papers include the Gender Shades project, which showed that facial recognition software from IBM, Microsoft, Face++, and Google all demonstrated systematic bias by performing worse on darker skinned people, especially women of color. Another prominent paper, yet to be published, was “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”, which examined how Natural Language Processing models (clearly a hot topic then and now) have been focused on becoming larger in size and more powerful without taking into account the data that they were using to train. In particular, they scraped text from online which included white supremacist, misognyistic, ageist, etc. views (https://time.com/6132399/timnit-gebru-ai-google/). This was a very problematic result that given that, as in ProPublica or Gender Shades, biased training data clearly have very negative implications for marginalized groups. This paper caused enough discontent at Google that they tried to convince her to remove Google co-authors from this paper before firing her shortly after. As a personal aside, this reflects badly on Google - Gebru’s own concerns were about how this push for the development of AI and the competition with other companies focuses on growth rather than ethics, and her firing proves this point.\nThis next part will mostly be my own interpretation. I think she is important as a researcher because she has exposes uncomfortable truths about how Big Tech, academia, and governments perpetuate social inequality. She has legitmized the field of social responsibility by showing concretely that some of the most important tech projects exhibit systematic racial or gender bias. She has also spoken out personally about her own experiences with injustices. This includes personal incidents of racial harassment from the police or colleagues, the culture of sexual harassment and negligence in companies or academic conferences, and the striking lack of people of color (especially Black people and Black women) in tech conferences. In short, the people in charge of the largest AI projects which have the strongest influence on our lives are not very responsible, belonging to a culture of misogyny and racism which their algorithms themselves also exhibit. She has pushed to make the space more equitable by founding projects as Black in AI and DAIR, which seeks to create community funded AI research that values ethical concerns first. To this extent, her status in the field is also a great inspiration to others and an example that it can be possible for marginalized people to achieve success and combat the inequity in academia and business and our general society."
  },
  {
    "objectID": "posts/dr_gebru/dr_gebru.html#summarizing-dr.-gebrus-tutorial-on-fairness-accountability-transparency-and-ethics-fate-in-computer-vision",
    "href": "posts/dr_gebru/dr_gebru.html#summarizing-dr.-gebrus-tutorial-on-fairness-accountability-transparency-and-ethics-fate-in-computer-vision",
    "title": "Dr. Timnit Gebru and AI Ethics",
    "section": "Summarizing Dr. Gebru’s Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision",
    "text": "Summarizing Dr. Gebru’s Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision\nDr. Gebru’s talk was on algorithmic bias in computer vision. She notes that algorithms cannot be vetted, there is no regulation on what constitutes acceptable algorithms despite the great harm they can pose to society. As a concrete example, she points to the Baltimore police’s use of facial recognition to arrest those in the Freddie Gray protest, an undemocratic action that suppresses political participation. One danger she sees is that computer science abstracts people away from research problems, not considering the context of algorithms. This is deeper than making data fairer or more inclusive as this does not negate any harmful impacts algorithms like facial recognition can have - whether on identifying protestors or promoting gender stereotyped advertising, or even views that gender is something beyond a social construct. Data is often used without consent of individuals, as in IBM’s diversified facial recognition dataset, and does not benefit the people using it, as in China Big Brother tech using faces from Zimbabweans. Law enforcement even collects data to harm immigrant or black and brown communities. Another issue is automation bias - people trust algorithms, robots more because they don’t question it or know how it works. A final point she makes is that the people working on computer vision themselves are not diverse. Even ethical boards for Stanford are mostly white and work to marginalize others.\nI think everyone needs to understand that computer vision algorithms can be incredibly harmful to marginalized groups and that computer scientists need to be very aware of structural biases and therefore be responsible for the impact of their work, intended or not."
  },
  {
    "objectID": "posts/dr_gebru/dr_gebru.html#question-for-dr.-gebru",
    "href": "posts/dr_gebru/dr_gebru.html#question-for-dr.-gebru",
    "title": "Dr. Timnit Gebru and AI Ethics",
    "section": "Question for Dr. Gebru",
    "text": "Question for Dr. Gebru\nDr. Gebru advocates against Big Tech’s view on the benefits of AI, which ignore the damage it does to marginalize communities. What can people do to change this? She’s talked about harmful views like longtermism, how workers should have more rights so they can prioritize ethics over deadlines, more education centering on ethics in computing, or government regulations. Can these incremental changes stop this harm or should it be down to an individual’s decision not to participate in harmful systems?"
  },
  {
    "objectID": "posts/adam/adam.html",
    "href": "posts/adam/adam.html",
    "title": "Variations on Gradient Descent",
    "section": "",
    "text": "Link to code: https://github.com/jchung2020/jchung2020.github.io/tree/main/posts/gradient_descent\nHere I will implement three different variations on gradient descent using logistic regression loss: regular gradient descent, stochastic gradient descent, and stochastic gradient descent with momentum.\nWith gradient descent, I begin with a random guess for the minimizer of the loss function. At each point, I compute the gradient and take a step (whose size is modulated by the stepping size) in that direction. This updates the new guess for the minimum, and I continue until the gradient is close to the zero vector.\nFor stochastic gradient descent, I use the same general approach but instead divide the data up into random batches. Additionally, I implemented stochastic gradient descent with momentum, which uses the difference between our current and previous guesses for the momentum update to inform the next update on our guess. This ensures that if we have a good guess for the minimizer, we more quickly head in that direction (and more quickly converge).\nStarting with simple 2D data:\nIn this case, notice that stochastic gradient descent with momentum converges fastest, then stochastic gradient descent, then regular gradient descent.\nAs we can see, the separating lines for all algorithm are quite similar."
  },
  {
    "objectID": "posts/adam/adam.html#altering-the-stepping-size",
    "href": "posts/adam/adam.html#altering-the-stepping-size",
    "title": "Variations on Gradient Descent",
    "section": "Altering the Stepping Size",
    "text": "Altering the Stepping Size\nBelow is a case I show some larger stepping sizes. It is suprisingly robust, however, as even a large stepping size of 30 yields a reasonable loss. Perhaps the gradient is naturally small.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 30, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stepping size 30\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 50, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stepping size 50\")\n\nlegend = plt.legend() \nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.title(\"Large Stepping Size Comparison on Gradient Descent\")\n\nText(0.5, 1.0, 'Large Stepping Size Comparison on Gradient Descent')\n\n\n\n\n\nAs we can see, the loss never converges for a stepping size of 50, as this proves to be too large for proper gradient descent."
  },
  {
    "objectID": "posts/adam/adam.html#altering-the-batch-size",
    "href": "posts/adam/adam.html#altering-the-batch-size",
    "title": "Variations on Gradient Descent",
    "section": "Altering the Batch Size",
    "text": "Altering the Batch Size\nHere I demonstrate that the batch size can affect how quickly stochastic gradient search will converge.\n\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 25, \n                  alpha = 0.1) \n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.1) \n\nnum_steps = len(LR1.loss_history)\n\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"loss (batch size 25)\")\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"loss (batch size 10)\")\n#plt.plot(np.arange(num_steps) + 1, LR.history, label = \"accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.title(\"Batch Size Comparison for Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Batch Size Comparison for Stochastic Gradient Descent')\n\n\n\n\n\nAs we can see, a smaller batch size allows for a faster convergence, perhaps because the gradient descent more frequently updates the point of the minimum loss."
  },
  {
    "objectID": "posts/adam/adam.html#gradient-descent-in-higher-dimensions",
    "href": "posts/adam/adam.html#gradient-descent-in-higher-dimensions",
    "title": "Variations on Gradient Descent",
    "section": "Gradient Descent in Higher Dimensions",
    "text": "Gradient Descent in Higher Dimensions\nBelow I have plotted comparisons of the gradient descent algorithms in 10 dimensions. Notice that the convergence is faster for stochastic gradient with momentum. In fact, all the gradient descent algorithms converge quite quickly.\nI initially used random vectors for the centers of the blobs but it seems that the data may be linearly separable (hence the fast convergence). Perhaps in higher dimensions, this method of generating data makes it easier to separate linearly. For this new data set, I randomized a vector to multiply to one data set so they are not as linearly separable.\n\np_features = 11\n\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(1.72810539, -0.76966727, -8.40792394,  0.00943856,  1.33926325,  1.80831111,\n -8.52324258,  1.39576299,  1.50974231, -6.81079023), (1.2532123,   -0.45271457, -4.67704148,  0.00651874,  1.5479877,   0.84146263,\n -3.45566331,  0.62523429,  0.684547,   -2.54964336)])\n\nfig, axs = plt.subplots(2, figsize=(10, 10))\nfig.suptitle('Vertically stacked subplots')\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.1)\n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"gradient\")\n\n#plt.loglog()\naxs[0].loglog()\naxs[1].loglog()\n\nlegend = axs[0].legend() \nlegend = axs[1].legend() \n\naxs[0].set_title(\"Loss Comparison for Gradient Descent Algorithms\")\naxs[0].set(xlabel = 'Iterations', ylabel= 'Loss')\naxs[1].set_title(\"Accuracy Comparison for Gradient Descent Algorithms\")\naxs[1].set(xlabel = 'Iterations', ylabel= 'Accuracy')\n\n[Text(0.5, 0, 'Iterations'), Text(0, 0.5, 'Accuracy')]\n\n\n\n\n\nStochastic gradient descent with momentum clearly converges the fastest. Stochastic gradient both with and without momentum appear to fluctuate in the loss (probably as a result of the random batches), but both converge faster than regular gradient descent and even reach 100% accuracy."
  },
  {
    "objectID": "posts/project_blogpost/project.html",
    "href": "posts/project_blogpost/project.html",
    "title": "Determining Race from Chest X-Rays",
    "section": "",
    "text": "The GitHub repository for the project code can be found here."
  },
  {
    "objectID": "posts/project_blogpost/project.html#does-race-exist",
    "href": "posts/project_blogpost/project.html#does-race-exist",
    "title": "Determining Race from Chest X-Rays",
    "section": "Does Race Exist?",
    "text": "Does Race Exist?\nWhether race exists as a biological phenomenon, and not as a social construct, is a hotly debated issue. As Cerdeña, Plaisime, and Tsai (2020) note, “race was developed as a tool to divide and control populations worldwide. Race is thus a social and power construct with meanings that have shifted over time to suit political goals, including to assert biological inferiority of dark-skinned populations.”\nOne justification for the biological reality of races is based on the assumption that different races have distinct genetics from one another, and can be fit into genetic groups. However, Maglo, Mersha, and Martin (2016) note that humans are not distinct by evolutionary criteria and genetic similarities between “human races, understood as continental clusters, have no taxonomic meaning”, with there being “tremendous diversity within groups” [2]. Whether race defines a genetic profile is therefore unclear at best, with correlations between race and disease being confounded by variables such as the association between race and socioeconomic variables."
  },
  {
    "objectID": "posts/project_blogpost/project.html#what-is-race-based-medicine",
    "href": "posts/project_blogpost/project.html#what-is-race-based-medicine",
    "title": "Determining Race from Chest X-Rays",
    "section": "What is Race-based Medicine?",
    "text": "What is Race-based Medicine?\nIt is possible that some may be interested in using this algorithm to deduce the race of an individual and use this as part of medical decisions. There are some correlations between disease prevalence and race. Maglo, Mersha, and Martin (2016) note that “Recent studies showed that ancestry mapping has been successfully applied for disease in which prevalence is significantly different between the ancestral populations to identify genomic regions harboring diseases susceptibility loci for cardiovascular disease (Tang et al. (2005)), multiple sclerosis (Reich et al. (2005)), prostate cancer (Freedman et al. (2006)), obesity (Cheng et al. (2009)), and asthma (Vergara et al. (2009))” [2].\nThese practices would be characteristic of race-based medicine. As Cerdeña, Plaisime, and Tsai (2020) argue, this is “the system by which research characterizing race as an essential, biological variable, [which] translates into clinical practice, leading to inequitable care” [1]. Notably, then, race-based medicine has come under heavy criticism."
  },
  {
    "objectID": "posts/project_blogpost/project.html#the-harms-of-race-based-medicine",
    "href": "posts/project_blogpost/project.html#the-harms-of-race-based-medicine",
    "title": "Determining Race from Chest X-Rays",
    "section": "The Harms of Race-based Medicine",
    "text": "The Harms of Race-based Medicine\nAs stated above, race is not an accurate proxy for genetics. Cerdeña, Plaisime, and Tsai (2020) note that in medical practices, race is used as an inaccurate guideline for medical care: “Black patients are presumed to have greater muscle mass …On the basis of the understanding that Asian patients have higher visceral body fat than do people of other races, they are considered to be at risk for diabetes at lower body-mass indices” [1]. As they note, race-based medicine can be founded more on racial stereotypes and generalizations rather than.\nMoreover, race-based medicine can lead to ineffective treatements. Apeles (2022) summarizes the results of a study on race-based prescriptions for Black patients for high blood pressure. While this study demonstrates that alternative prescriptions for Black patients with high blood pressure have been shown to be ineffective, “Practice guidelines have long recommended that Black patients with high blood pressure and no comorbidities be treated initially with a thiazide diuretic or a calcium channel blocker (CCB) instead of an angiotensin converting enzyme inhibitor (ACEI) and/or angiotensin receptor blocker (ARB). By contrast, non-Black patients can be prescribed any of those medicines regardless of comorbidities.” In addition, the authors of the study found that “other factors may be more important than considerations of race, such as dose, the addition of second or third drugs, medication adherence, and dietary and lifestyle interventions. Follow-up care was important, and the Black patients who had more frequent clinical encounters tended to have better control of their blood pressure.”\nIn addition, Vyas, Eisenstein, and Jones (2020) argue that race is ill-suited as a correction factor for medical algorithms. As they found, algorithms as the American Heart Association (AHA) Get with the Guidelines–Heart Failure Risk Score, which predicts the likelihood of death from heart failure, the Vaginal Birth after Cesarean (VBAC), which predicts the risk of labor for someone with a previous cesarean section, and STONE score, which predicts the likelihood of kidney stones in patients with flank pain, all used race to change their predictions of the likelihood or morbidities. However, they find that these algorithms were not sufficiently evidence based as “Some algorithm developers offer no explanation of why racial or ethnic differences might exist. Others offer rationales, but when these are traced to their origins, they lead to outdated, suspect racial science or to biased data”. Using race can then discourage racial minorities from receiving the proper treatment based on their scores, exacerbating already existing problems of unequal health outcomes."
  },
  {
    "objectID": "posts/project_blogpost/project.html#conclusion",
    "href": "posts/project_blogpost/project.html#conclusion",
    "title": "Determining Race from Chest X-Rays",
    "section": "Conclusion",
    "text": "Conclusion\nSo it is clear that anyone who intends to use race for diagnosis could harm racial minority groups. Race inherently is a complex social and economic phenomenon and cannot be said to be a clear biological variable. Hence anyone intending to use or create algorithms will run the risk of creating dangerous biases in treatment; ones that could worsen the existing disparities in care for vulnerable populations."
  },
  {
    "objectID": "posts/project_blogpost/project.html#data-subsetting",
    "href": "posts/project_blogpost/project.html#data-subsetting",
    "title": "Determining Race from Chest X-Rays",
    "section": "Data Subsetting",
    "text": "Data Subsetting\nWe trained our model using 10,000 frontal chest X-rays, such as the one in the following figure. The only input features are from the image itself, and the only target is race.\nWe created the train and test datasets by taking on the Black, White, and Asian populations. The dataset in total is 90000 images, with 72,000 in the train set and 18,000 in the test set. We created the equal proportion training set by taking the first 6000 images from each race group (this includes almost all the Black patients, the smallest group) in the training set and randomizing the order. A similar process held for the test set, but in total it is only 3000 images.\nAside from potential bias, subsetting the data makes it easier to train and evaluate an algorithm. Especially since we are not using a very large amount of data, using a large model can easily overfit to the demographics of the patient. Attempts to use the unbalanced training set essentially resulted in a algorithm that would always guess White. By using a balanced set, it is easier for the model to learn about the features without this frequency bias.\nWe created more training sets that also included only the frontal scans. We noticed that this is something that Gichoya et. al did. Whether it actually makes a meaningful difference is not clear, but since the lateral scans are not as common, it is possible that this hinders the models from learning.\nFor actual training, we only use 10,000 images due to the lack of computing power. Our training was done on the standard T4 GPUs available in Google Colab. Again, this is significant difference from Gichoya et. al, who use more than 100,000 images in their algorithm.\n\n\n\nchest X-ray"
  },
  {
    "objectID": "posts/project_blogpost/project.html#image-transforms",
    "href": "posts/project_blogpost/project.html#image-transforms",
    "title": "Determining Race from Chest X-Rays",
    "section": "Image Transforms",
    "text": "Image Transforms\nWe also used the image transforms that Gichoya et. al implmeneted. This includes an image resize to (224 by 244), image normalization (to the ImageNet mean and standard deviation), random horizontal flips, random rotations at a maximum of 15 degrees. We did not include a random zoom as in the paper however. In general, these are a good way of preventing overfitting. Some transformation like the random rotations may also prevent the model from generalizing some specific features, like a patient’s lean or posture."
  },
  {
    "objectID": "posts/project_blogpost/project.html#models",
    "href": "posts/project_blogpost/project.html#models",
    "title": "Determining Race from Chest X-Rays",
    "section": "Models",
    "text": "Models\nAs for our models themselves, we used ResNet and EfficientNet. These are popular deep learning architectures for image classification, and have both achieved high accuracies of around 70-80% when evaluated on the ImageNet data base. Specifically, we used pretrained EfficientNetB0 and ResNet18 models. We chose these specific variations because they have a low number of parameters (which means faster training time and a smaller chance of overfitting). Moreover, these models are trained on the 224 by 224 image size that we used.\nWe also implemented some ResNet18 models on our own but achieved a lower accuracy. A problem with deep neural networks is that performance degrades with more layers. This occurs even when an identity operation is conudcted. Hence ResNet combats this issue by using skip connections, which saves the output from previous layers and adds it to the current output, avoiding this degradation.\nTo gauge what the ResNet18 model is “looking” at, we extracted the filters from the first convolutional layer to visualize the feature maps across three convolutional layers: layer 0, layer 8, and the last layer 16.\nTo optimize a model, we would train it on 10,000 images in a loop, using different learning rates for the Adam optimizer and \\(\\gamma\\) values for the exponential scheduler. We trained all the parameters of our pretrained model. We assumed that since there is no reason that the ImageNet database would contain X-ray like images, it would be best to tune all parameters.\nWe note that the training process for the self-implemented Resnet18 model was different. We trained first with an exponential learning rate scheduler for 30 epochs, then 15 epochs on the learning rate plateu scheduler as the loss did not decrease otherwise.\nIn the same loop, we would then validate the model on 2,500 other images from the training set to find the optimal hyperparameters. Cross entropy loss was used for all models.\nWe define accuracy as the proportion of correct guesses, and we analyze the confusion matrices of our model.\nAs mentioned before, there may be a gender bias in our model because there are more male than female patients in our training dataset. We inspected this by splitting our test set into male and female counterparts and testing the model on each subset. Gender bias is then examined by looking at the score and confusion matrix for each gendered subset."
  },
  {
    "objectID": "posts/project_blogpost/project.html#loss-and-accuracy-history",
    "href": "posts/project_blogpost/project.html#loss-and-accuracy-history",
    "title": "Determining Race from Chest X-Rays",
    "section": "Loss and Accuracy History",
    "text": "Loss and Accuracy History\nOur best models achieved an accuracy of about 70-75% on the equal testing set. Pretrained ResNet and EfficientNet models obtained similar accuracies and losses, so we will display only EfficientNet results.\nAfter optimizing the pretrained EfficientNetB0 algorithm with an initial learning rate at 0.001 and an exponential scheduler with \\(\\gamma = 0.735\\), we achieved an accuracy of 74% on the balanced test set.\nAs we can see in the following figures, the training score and loss gradually improved, while the validation score and loss plateau after a few epochs. This is likely a sign of overfitting. We tried to optimize the model by altering the scheduler type, varying the Adam learning rate from 0.001 to 0.01, but the overfitting persisted.\n \nWe also did our own implementation of ResNet18, and obtained comparable results. The learning rate was also set to 0.001, and the exponential scheduler at \\(\\gamma = 0.735\\). The issue of overfitting remained, and our model achieved a score of 68% when tested on unseen data."
  },
  {
    "objectID": "posts/project_blogpost/project.html#confusion-matrix",
    "href": "posts/project_blogpost/project.html#confusion-matrix",
    "title": "Determining Race from Chest X-Rays",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nShown below are the confusion matrices for the EfficientNetB0 model with \\(\\gamma = 0.735\\) over a subset of 2500 male and female patients for both frontal and lateral images. The horizontal axis represents the predicted classes (White, Black, Asian), and the vertical axis represents the true labels.\n\n\n\n\n\n\n\nConfusion matrix for male patients\nConfusion matrix for female patients\n\n\n\n\n\n\n\n\n\nFrom the confusion matrices above, we notice that for male patients, the model classified White the best class, with 78% of the predictions to be true. Whereas, for the female patients, the model classified Asian the best class, with 79% of the predictions to be true. However, across all classes (White, Black, Asian), the percentage of true-positives are relatively the same and can be interpreted to be equivalent.\nFor male patients, 19% of the Black class and 18% of the Asian class were predicted as White. For female patients, 17% of the Black class and 15% of the Asian class were predicted as White. These rates are similar for the misclassification of White patients predicted as Asian — 12% of the male White class and 16% of the female White class were predicted as Asian. In contrast, the model has a significantly lower percentage of classifying the Asian and White class as part of the Black class, especially in falsely predicting Black as Asian and vice versa.\nThus, although we have trained on an equal subset of each class (White, Black, Asian), the confusion matrix suggests that there may be representational bias with how a certain class such as the White class will be more likely to be predicted than the Black and Asian class. Thus, though the model does classify each race at a relatively moderate accuracy (up to 80% accuracy), it may also be at the cost of producing inaccurate classifications to a patient’s true race — which is highly favored for those belonging in the White class. This can result in heavy implications if an algorithm like this were to be commercialized and probes the validity of similar algorithms that yield a “high” accuracy rates for the true races. The causes are not clear with our current analysis, but perhaps the number of distinct patients represented amongst those images is not equal - White patients on average do have more images, which could mean a smaller number of distinct patients during training.\nTo compare, we also show the self-implemented Resnet18 confusion matrices. Notice that the correct predictions remain the same even for the unequal testing set. Of course, the accuracy of 71% is not quite good enough to match the base accuracy of 78% (if the algorithm were to guess all White).\n\n\n\n\n\n\n\nSelf-implemented Resnet18, Equal Set\nSelf-implemented Resnet18, Unequal Set\n\n\n\n\n\n\n\n\n\nInterestingly here, the correct predictions for Black and Asian are significantly worse than for EfficientNetB0. The general trend of the false predictions, while higher than in EfficientNetB0, do adhere to the similar trends discussed."
  },
  {
    "objectID": "posts/project_blogpost/project.html#visualized-feature-map",
    "href": "posts/project_blogpost/project.html#visualized-feature-map",
    "title": "Determining Race from Chest X-Rays",
    "section": "Visualized Feature Map",
    "text": "Visualized Feature Map\nUsing our self-implemented ResNet18 model, we demonstrate below the first convolutional layer’s filters, and the feature maps of three convolutional layers — layer 0, layer 8, and layer 16 (the last layer).\nThe patient used for this demonstration is a White, non-hispanic Female. Shown below is the patient’s frontal-view image used for this experiment.\n\n\n\nFrontal-view image of patient\n\n\n\n\n\n\n\n\nAnd here is the first convolutional layer’s filter:\n\n\n\n\n\n\nFirst convolutional layer filter of the self-implemented ResNet-18 neural network model\n\n\n\n\n\n\n\n\nThen, we passed the filter through each of the convolutional layers of the model. For simplicity, we only showed three of the total layers. It is interesting to see that there is varied noise as to what the model “looks” at, highlighted by the whiter patches of the image — which also corresponds to which part of the image is “activated”!\n\n\n\n\n\n\nFeature maps from the first convolutional layer (layer 0) of self-implemented ResNet18 model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature maps from the ninth convolutional layer (layer 8) of self-implemented ResNet18 model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature maps from the last convolutional layer (layer 16) of self-implemented ResNet18 model\n\n\n\n\n\n\n\n\nAfter displaying the three layers, we observed that the last layer is highly disoriented, and the regular human eye can no longer distinguish the original image (the patient’s frontal-view). This last layer serves high importance as it is used to deduce what the model is actually classifying based off “features” it has learned. Additionally, we also observe that the model focused on different aspects of the image as the filters used to create the feature map vary.\nWhat exactly it is observing is unclear, perhaps we can say that it is able to distinguish between the bones and lungs of an X-ray."
  },
  {
    "objectID": "code/CS451.html",
    "href": "code/CS451.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "#need to import Drive\n\nfrom google.colab import drive\ndrive.mount('/content/drive', force_remount=True)\n\nMounted at /content/drive"
  },
  {
    "objectID": "code/CS451.html#image-visualization",
    "href": "code/CS451.html#image-visualization",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Image Visualization",
    "text": "Image Visualization\n\n#This just will show the images\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimg = cv2.imread('/content/drive/Shareddrives/CSCI451/train/patient00142/study3/view1_frontal.jpg')\n#img = cv2.imread('/content/drive/MyDrive/chexpert_small/train/patient00974/study1/view1_frontal.jpg')\n# cv2.imshow()\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "code/CS451.html#file-reading-tests",
    "href": "code/CS451.html#file-reading-tests",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "File reading tests",
    "text": "File reading tests\n\nfrom os import walk\nfor (dirpath, dirnames, filenames) in walk(\"../input/\"):\n    print(\"Directory path: \", dirpath)\n    print(\"Folder name: \", dirnames)\n#     print(\"File name: \", filenames)\n\n\n# DO NOT RUN THIS\n\n\"\"\"\nimport torch\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([transforms.Resize(255),\n                                 transforms.CenterCrop(224),\n                                 transforms.ToTensor()])\n\ndataset = datasets.ImageFolder('/content/drive/MyDrive/chexpert_small/train/', transform=transform)\n\"\"\""
  },
  {
    "objectID": "code/CS451.html#data-analysis",
    "href": "code/CS451.html#data-analysis",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Data Analysis",
    "text": "Data Analysis\ndf_train_race holds our data set of patient ID and race.\n\n# import our race data set and load as a dataframe\n\nimport pandas as pd\n\ndf_train_race = pd.read_excel('/content/drive/Shareddrives/CSCI451/chexpert_race.xlsx')\ndf_train_race\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PATIENT\n      GENDER\n      AGE_AT_CXR\n      PRIMARY_RACE\n      ETHNICITY\n    \n  \n  \n    \n      0\n      patient24428\n      Male\n      61\n      White\n      Non-Hispanic/Non-Latino\n    \n    \n      1\n      patient48289\n      Female\n      39\n      Other\n      Hispanic/Latino\n    \n    \n      2\n      patient33856\n      Female\n      81\n      White\n      Non-Hispanic/Non-Latino\n    \n    \n      3\n      patient41673\n      Female\n      42\n      Unknown\n      Unknown\n    \n    \n      4\n      patient48493\n      Male\n      71\n      White\n      Non-Hispanic/Non-Latino\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      65396\n      patient65702\n      Male\n      1\n      Other\n      Hispanic/Latino\n    \n    \n      65397\n      patient04979\n      Female\n      27\n      Other\n      Hispanic/Latino\n    \n    \n      65398\n      patient11445\n      Female\n      29\n      Unknown\n      Unknown\n    \n    \n      65399\n      patient23235\n      Female\n      41\n      Other, Hispanic\n      Hispanic/Latino\n    \n    \n      65400\n      patient05143\n      Male\n      24\n      White\n      Non-Hispanic/Non-Latino\n    \n  \n\n65401 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# print the unique labels for race - we need to make some determinations here\nprint(df_train_race['PRIMARY_RACE'].unique())\nwhite_no = (df_train_race['PRIMARY_RACE'].str.contains('White')).sum()\nblack_no = (df_train_race['PRIMARY_RACE'].str.contains('Black')).sum()\nasian_no = (df_train_race['PRIMARY_RACE'].str.contains('Asian')).sum()\nnative_no = (df_train_race['PRIMARY_RACE'].str.contains('American')).sum()\nislander_no = (df_train_race['PRIMARY_RACE'].str.contains('Pacific')).sum()\nrest_no = len(df_train_race) - white_no - black_no - asian_no - native_no - islander_no\n\nprint('White: ' + str(white_no) + '\\n Asian: ' + str(asian_no) + '\\n Black: ' + str(black_no) + '\\n Native American: ' + str(native_no) + '\\n Pacific Islander: '\n+ str(islander_no) + '\\n Other/Unknown: ' + str(rest_no))\n\nlabels = ['White', 'Asian', 'Black', 'Native', 'Pacific Islander', 'Other/Unknown']\nsizes = [white_no, asian_no, black_no, native_no, islander_no, rest_no]\n\nplt.pie(sizes, labels = labels, autopct='%1.1f%%',\n       pctdistance=1.25, labeldistance=.6)\nplt.show()\n\n['White' 'Other' 'Unknown' 'White, non-Hispanic' 'Asian' nan\n 'Black or African American' 'Black, non-Hispanic' 'Other, Hispanic'\n 'Race and Ethnicity Unknown' 'Asian, non-Hispanic'\n 'Pacific Islander, non-Hispanic'\n 'Native Hawaiian or Other Pacific Islander' 'Other, non-Hispanic'\n 'Patient Refused' 'White, Hispanic' 'Black, Hispanic' 'Asian, Hispanic'\n 'American Indian or Alaska Native' 'Native American, Hispanic'\n 'Native American, non-Hispanic' 'Pacific Islander, Hispanic'\n 'Asian - Historical Conv' 'White or Caucasian']\nWhite: 36772\n Asian: 7061\n Black: 3147\n Native American: 2794\n Pacific Islander: 881\n Other/Unknown: 14746\n\n\n\n\n\n\n#print the unique labels for ethnicity\nprint(df_train_race['ETHNICITY'].unique())\n\n['Non-Hispanic/Non-Latino' 'Hispanic/Latino' 'Unknown' nan\n 'Patient Refused' 'Hispanic' 'Not Hispanic']\n\n\n\n#subset data by the \"Other\" listed for Race\ndf_train_race[df_train_race['PRIMARY_RACE'] == 'Other']\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PATIENT\n      GENDER\n      AGE_AT_CXR\n      PRIMARY_RACE\n      ETHNICITY\n    \n  \n  \n    \n      1\n      patient48289\n      Female\n      39\n      Other\n      Hispanic/Latino\n    \n    \n      13\n      patient51575\n      Male\n      68\n      Other\n      Hispanic/Latino\n    \n    \n      32\n      patient28670\n      Male\n      63\n      Other\n      Hispanic/Latino\n    \n    \n      53\n      patient65275\n      Female\n      59\n      Other\n      Non-Hispanic/Non-Latino\n    \n    \n      61\n      patient54887\n      Male\n      74\n      Other\n      Hispanic/Latino\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      65386\n      patient33297\n      Female\n      40\n      Other\n      Hispanic/Latino\n    \n    \n      65390\n      patient15562\n      Female\n      50\n      Other\n      Hispanic/Latino\n    \n    \n      65395\n      patient32620\n      Female\n      21\n      Other\n      Hispanic/Latino\n    \n    \n      65396\n      patient65702\n      Male\n      1\n      Other\n      Hispanic/Latino\n    \n    \n      65397\n      patient04979\n      Female\n      27\n      Other\n      Hispanic/Latino\n    \n  \n\n8510 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\ndf_patients holds a csv of image links with sex, age, other features related to their hospitalization. Notice that the patient ID is inside the text for the links. Also, one patient can have multiple images from different times. Images are in two categories frontal and lateral (front of body vs. side of body).\n\ndf_patients = pd.read_csv('/content/drive/Shareddrives/CSCI451/train.csv')\ndf_patients\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Path\n      Sex\n      Age\n      Frontal/Lateral\n      AP/PA\n      No Finding\n      Enlarged Cardiomediastinum\n      Cardiomegaly\n      Lung Opacity\n      Lung Lesion\n      Edema\n      Consolidation\n      Pneumonia\n      Atelectasis\n      Pneumothorax\n      Pleural Effusion\n      Pleural Other\n      Fracture\n      Support Devices\n    \n  \n  \n    \n      0\n      CheXpert-v1.0-small/train/patient00001/study1/...\n      Female\n      68\n      Frontal\n      AP\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      NaN\n      NaN\n      NaN\n      1.0\n    \n    \n      1\n      CheXpert-v1.0-small/train/patient00002/study2/...\n      Female\n      87\n      Frontal\n      AP\n      NaN\n      NaN\n      -1.0\n      1.0\n      NaN\n      -1.0\n      -1.0\n      NaN\n      -1.0\n      NaN\n      -1.0\n      NaN\n      1.0\n      NaN\n    \n    \n      2\n      CheXpert-v1.0-small/train/patient00002/study1/...\n      Female\n      83\n      Frontal\n      AP\n      NaN\n      NaN\n      NaN\n      1.0\n      NaN\n      NaN\n      -1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      NaN\n    \n    \n      3\n      CheXpert-v1.0-small/train/patient00002/study1/...\n      Female\n      83\n      Lateral\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      NaN\n      NaN\n      -1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      NaN\n    \n    \n      4\n      CheXpert-v1.0-small/train/patient00003/study1/...\n      Male\n      41\n      Frontal\n      AP\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      223409\n      CheXpert-v1.0-small/train/patient64537/study2/...\n      Male\n      59\n      Frontal\n      AP\n      NaN\n      NaN\n      NaN\n      -1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      -1.0\n      0.0\n      1.0\n      NaN\n      NaN\n      NaN\n    \n    \n      223410\n      CheXpert-v1.0-small/train/patient64537/study1/...\n      Male\n      59\n      Frontal\n      AP\n      NaN\n      NaN\n      NaN\n      -1.0\n      NaN\n      NaN\n      NaN\n      0.0\n      -1.0\n      NaN\n      -1.0\n      NaN\n      NaN\n      NaN\n    \n    \n      223411\n      CheXpert-v1.0-small/train/patient64538/study1/...\n      Female\n      0\n      Frontal\n      AP\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      -1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      223412\n      CheXpert-v1.0-small/train/patient64539/study1/...\n      Female\n      0\n      Frontal\n      AP\n      NaN\n      NaN\n      1.0\n      1.0\n      NaN\n      NaN\n      NaN\n      -1.0\n      1.0\n      0.0\n      NaN\n      NaN\n      NaN\n      0.0\n    \n    \n      223413\n      CheXpert-v1.0-small/train/patient64540/study1/...\n      Female\n      0\n      Frontal\n      AP\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.0\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n223414 rows × 19 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n#need to sort df_train_race by patient ID, otherwise just annoying to index\n\n#df_train_race = pd.read_excel('/content/drive/Shareddrives/CSCI451/chexpert_race.xlsx')\ndf_race_sorted = df_train_race.sort_values(by=['PATIENT'],ignore_index=True)\ndf_race_sorted = df_race_sorted.dropna()\n#df_race_sorted = df_race_sorted[df_race_sorted[\"PRIMARY_RACE\" == \"Patient Refused\"]]\n\ndf_race_sorted \n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PATIENT\n      GENDER\n      AGE_AT_CXR\n      PRIMARY_RACE\n      ETHNICITY\n    \n  \n  \n    \n      0\n      patient00001\n      Female\n      68\n      Other\n      Non-Hispanic/Non-Latino\n    \n    \n      1\n      patient00002\n      Female\n      87\n      White, non-Hispanic\n      Non-Hispanic/Non-Latino\n    \n    \n      2\n      patient00003\n      Male\n      41\n      White, non-Hispanic\n      Non-Hispanic/Non-Latino\n    \n    \n      3\n      patient00004\n      Female\n      20\n      Black or African American\n      Non-Hispanic/Non-Latino\n    \n    \n      4\n      patient00005\n      Male\n      33\n      White\n      Non-Hispanic/Non-Latino\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      65396\n      patient65731\n      Female\n      64\n      White\n      Non-Hispanic/Non-Latino\n    \n    \n      65397\n      patient65732\n      Female\n      0\n      Asian\n      Non-Hispanic/Non-Latino\n    \n    \n      65398\n      patient65735\n      Female\n      1\n      White\n      Non-Hispanic/Non-Latino\n    \n    \n      65399\n      patient65739\n      Female\n      44\n      Unknown\n      Non-Hispanic/Non-Latino\n    \n    \n      65400\n      patient65740\n      Female\n      75\n      Asian\n      Non-Hispanic/Non-Latino\n    \n  \n\n64855 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n#again, df_patients['PATIENT'] holds links for the file directory, here I split so instead I can have the patient ID\ndf_patients['PATIENT'] = [df_patients['Path'][i].split(\"/\")[2] for i in range(df_patients.shape[0])]\n\n#I then merge the patient race with df_race_sorted, which will give me the patient ID - race\ndf_patients_race = pd.merge(df_patients,df_race_sorted,on='PATIENT')\n\"\"\"\n#check\nfor index, row in df_patients_race[206949:].iterrows():\n  race_sorted_index = df_race_sorted.index[df_race_sorted[\"PATIENT\"] == row[\"PATIENT\"]].tolist()\n  if (len(race_sorted_index) != 1):\n    print(\"error!\")\n    break\n  i = race_sorted_index[0]\n  if not (row['PATIENT'] in row['Path']):\n    print(row,\"patient error!\")\n    break\n  if not (row['ETHNICITY'] == df_race_sorted[\"ETHNICITY\"][i]):\n    print(row,\"ethnicity error!\",row['ETHNICITY'],df_race_sorted[\"ETHNICITY\"][i])\n    break\n  if not (row['PRIMARY_RACE'] == df_race_sorted[\"PRIMARY_RACE\"][i]):\n    print(row,\"race error!\")\n    break\n  if not (row['Sex'] == df_race_sorted[\"GENDER\"][i]):\n    print(row,\"sex error!\")\n    break\n\"\"\"\n\n#there is a gender error for patient 51668, index 202352\n# gender error for patient 54170, index 206312\n# gender error for patient 54565, index 206948\n\ndf_patients_race"
  },
  {
    "objectID": "code/CS451.html#data-preparation-and-loading",
    "href": "code/CS451.html#data-preparation-and-loading",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "Data Preparation and Loading",
    "text": "Data Preparation and Loading\n\nimport sys\nimport os\nfrom pathlib import Path\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch import optim\n\nimport torchvision.transforms as transforms\nimport torchvision\n\nfrom fastprogress import master_bar, progress_bar\n\nfrom PIL import Image\n\n\n#I used a lot of code from https://www.kaggle.com/code/hmchuong/chexpert-pytorch-densenet121?scriptVersionId=18314696&cellId=15\n\nclass ChestXrayDataset(Dataset):\n  #creates a Torch Dataset that we can use to do machine learning on\n\n  def __init__(self,folder_dir,dataframe,image_labels,image_size,normalization):\n    #a lot of this function is \n    # folder_dir is the directory path to the data\n    # dataframe holds patient info and labels\n    # it takes in our image labels\n\n    self.image_paths=[]\n    #self.image_labels=[]\n    self.image_labels=[]\n\n    #This transforms our image, I think we would also need normalization\n    image_transformation = [\n        #transforms.Grayscale(num_output_channels=1), #ONLY USE FOR PYTORCH XRAY\n        transforms.Resize((image_size,image_size)),\n        transforms.ToTensor()\n    ]\n\n    #this normalizes using some constants from imagenet\n    if normalization:\n      #ONLY COMMENT OUT FOR PYTORCH XRAY\n      #image_transformation.append(transforms.Normalize(IMAGENET_MEAN,IMAGENET_STD))\n      image_transformation.append(transforms.Normalize([0.485],[0.229]))\n\n    self.image_transformation = transforms.Compose(image_transformation)\n\n    #this will index through all the patients from 000001, so forth, adding images from study1\n    for index, row in dataframe.iterrows():\n      \n      #image_path = os.path.join(folder_dir,Path(row['PATIENT']),Path('study1'),Path('view1_frontal.jpg'))\n\n      #here I use lateral and frontal images\n      row_path_list = row['Path'].split(\"/\")\n      image_path = os.path.join(folder_dir,row_path_list[2],row_path_list[3],row_path_list[4])\n      \n      #if (os.path.isfile(image_path)):\n      self.image_paths.append(image_path)\n      #else:\n      #  continue\n      #in this case I've hard-coded the image_labels to be if the patient is Black or not\n      #NOTICE that I'm append a list to a list here\n      \n      image_label = []\n      \n      if ( row[\"PRIMARY_RACE\"] ==  'White' or row[\"PRIMARY_RACE\"] == 'White, non-Hispanic' or row[\"PRIMARY_RACE\"] == 'White or Caucasian'):\n        image_label.append(1)\n      #else:\n      #  image_label.append(0)\n      elif ( row[\"PRIMARY_RACE\"] ==  'Black or African American' or row[\"PRIMARY_RACE\"] == 'Black, non-Hispanic'):\n        image_label.append(2)\n      #else:\n      #  image_label.append(0)\n\n      elif ( row[\"PRIMARY_RACE\"] ==  'Asian' or row[\"PRIMARY_RACE\"] == 'Asian, non-Hispanic' or row[\"PRIMARY_RACE\"] == 'Asian - Historical Conv'):\n        image_label.append(3)\n      else:\n        image_label.append(0)\n\n      #DELETE THIS IF NOT USING TORCH XRAY\n      #for i in range(17):\n      #  image_label.append(5)\n      \n      self.image_labels.append(image_label)\n\n      #self.image_labels.append([int(1*((row[\"PATIENT\"] == 'Black or African American') | (row[\"PATIENT\"] == 'Black, non-Hispanic')))])\n    \n    #print(self.image_paths)\n\n  def __len__(self):\n    #I think this is necessary for other things\n    return len(self.image_paths)\n\n  def __getitem__(self,index):\n    #This is also just necessary for other parts\n\n    # Read image\n    image_path = self.image_paths[index]\n    #ONLY COMMENT OUT FOR PYTORCH XRAY\n    image_data = Image.open(image_path).convert(\"RGB\")\n    #image_data = Image.open(image_path)\n    image_data = self.image_transformation(image_data)\n      \n    return image_data, torch.FloatTensor(self.image_labels[index])\n\n\n\nIMAGE_SIZE = 224                              # Image size (224x224)\nIMAGENET_MEAN = [0.485, 0.456, 0.406]         # Mean of ImageNet dataset (used for normalization)\nIMAGENET_STD = [0.229, 0.224, 0.225]          # Std of ImageNet dataset (used for normalization)\nBATCH_SIZE = 96                            \n#BATCH_SIZE = 1728 #ONLY FOR PYTORCH XRAY VISION\nLEARNING_RATE = 0.001\nLEARNING_RATE_SCHEDULE_FACTOR = 0.1           # Parameter used for reducing learning rate\nLEARNING_RATE_SCHEDULE_PATIENCE = 5           # Parameter used for reducing learning rate\nMAX_EPOCHS = 2                              # Maximum number of training epochs\n\n\nrace_is_black = ((df_train_race['PRIMARY_RACE'] == 'Black or African American')  | (df_train_race['PRIMARY_RACE'] =='Black, non-Hispanic'))\nrace_is_black.sum()\n\n\nrace_is_white = ((df_train_race['PRIMARY_RACE'] == 'White')  | (df_train_race['PRIMARY_RACE'] =='White, non-Hispanic') | (df_train_race['PRIMARY_RACE'] =='White or Caucasian'))\nrace_is_white.sum()\n\n\ndf_train_race_mod = df_train_race.sort_values(by=['PATIENT'])[:10000]\ndf_train_race_mod\n\n\nrace_is_black = ((df_train_race_mod['PRIMARY_RACE'] == 'Black or African American')  | (df_train_race_mod['PRIMARY_RACE'] =='Black, non-Hispanic'))\nrace_is_black.sum()\n\n\ndf_train_race_mod_test = df_train_race.sort_values(by=['PATIENT'])[2000:2400]\ntest_race_is_black = ((df_train_race_mod_test['PRIMARY_RACE'] == 'Black or African American')  | (df_train_race_mod_test['PRIMARY_RACE'] =='Black, non-Hispanic'))\ntest_race_is_black.sum()\n\n\nrace_is_black.tolist()[0]\n\n\nrace_is_white = ((df_train_race_mod['PRIMARY_RACE'] == 'White')  | (df_train_race_mod['PRIMARY_RACE'] =='White, non-Hispanic') | (df_train_race_mod['PRIMARY_RACE'] =='White or Caucasian'))\nrace_is_white.sum()\n\nDefine the dataset object here\n\n#train_dataset = ChestXrayDataset(\"/content/drive/Shareddrives/CSCI451/train\", df_train_race_mod, race_is_white, IMAGE_SIZE, True)\n\n#train the dataset with the first 2000 images, this includes frontal and lateral images\ntrain_dataset = ChestXrayDataset(\"/content/drive/Shareddrives/CSCI451/train\", df_patients_race[:1000], race_is_white, IMAGE_SIZE, True)\n\n#print(ChestXrayDataset.len(train_dataset))\n\n\nimport numpy as np\nprint(train_dataset.image_labels)\nprint(train_dataset.image_paths)\nprint(\"dataset length\", len(train_dataset))\n\nCreate the loader for the dataset here - basically it is nice to have this so we can load the data in batches when we need instead of loading the entire dataset onto RAM, which is costly and slow.\n\ntrain_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=2, shuffle=True, pin_memory=True)\n\n\n#I believe that this functions as a test - if this block returns an error, there is a problem with dataloading\nfor data, label in train_dataloader:\n    print(data.size())\n    print(label.size())\n    break\n\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n#device = torch.device('cpu')\n\n\n\"\"\"\ndef train(model,  data_loader, optimizer, k_epochs = 1, print_every = 2000):\n\n    begin = time.time()\n    # loss function is cross-entropy (multiclass logistic)\n    loss_fn = nn.CrossEntropyLoss() \n\n    # optimizer is Adam, which does fancier stuff with the gradients\n    \n    for epoch in range(k_epochs): \n\n        running_loss = 0.0\n\n        i = 0\n\n        #for i, data in enumerate(data_loader, 0):\n        for data,label in data_loader:  \n            #print(data.size())\n\n            # extract a batch of training data from the data loader\n            X = data\n            #only flatten for binary classification\n            y = torch.flatten(label)\n            #y = label\n            y = y.to(torch.long)\n            X = X.to(device)\n            y = y.to(device)\n\n            \n\n            # zero out gradients: we're going to recompute them in a moment\n            optimizer.zero_grad()\n\n            # compute the loss (forward pass)\n            y_hat = model(X)\n\n            #print(\"Here!\")\n            #print(y_hat)\n            #print(y)\n\n            loss = loss_fn(y_hat, y)\n\n            #print(\"All good here!\")\n\n            # compute the gradient (backward pass)\n            loss.backward()\n\n            # Adam uses the gradient to update the parameters\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n\n            # print the epoch, number of batches processed, and running loss \n            # in regular intervals\n            if i % print_every == print_every - 1: \n            #if True:   \n                print(f'[epoch: {epoch + 1}, batches: {i + 1:5d}], training loss: {running_loss / print_every:.3f}')\n                #print(loss.item())\n                running_loss = 0.0\n\n            #print(i)\n            \n            i += 1\n    end = time.time()\n    print(f'Finished training in {round(end - begin)}s')\n\"\"\"\n\n\n\"\"\"\nfrom scipy.sparse import coo_array\ndef test(model, data_loader):\n    correct = 0\n    total = 0\n    # torch.no_grad creates an environment in which we do NOT store the \n    # computational graph. We don't need to do this because we don't care about \n    # gradients unless we're training\n\n    i = 0\n\n    with torch.no_grad():\n        #for data in data_loader:\n        for data, label in data_loader:\n\n            #print(data.size())\n            #print(label.size())\n\n            X = data\n            # only flatten if binary setup\n            y = torch.flatten(label)\n            #y = label\n            #X, y = data\n            X = X.to(device)\n            y = y.to(device)\n            \n            # run all the images through the model\n            y_hat = model(X)\n\n            # the class with the largest model output is the prediction\n            _, predicted = torch.max(y_hat.data, 1)\n\n            # compute the accuracy\n            total += y.size(0)\n            correct += (predicted == y).sum().item()\n\n            i += 1\n\n    print(i)\n    print(correct)\n    print(total)\n    print(f'Test accuracy: {100 * correct // total} %')\n\"\"\"\n\n\nimport torch.optim as optim\n\ndef train(model, trainloader, optimizer, k_epochs = 1, print_every = 2000):\n\n    begin = time.time()\n\n    # loss function is cross-entropy (multiclass logistic)\n    loss_fn = nn.CrossEntropyLoss() \n\n    # optimizer is Adam, which does fancier stuff with the gradients\n    #optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(k_epochs): \n\n        running_loss = 0.0\n        for i, data in enumerate(trainloader, 0):\n\n            # extract a batch of training data from the data loader\n            X, y = data\n            X = X.to(device)\n\n            #print(y)\n            \n            #need to flatten y - this holds labels for each image in the batch, but as a list of a list\n            y = torch.flatten(y)\n            y = y.to(torch.long)\n            \n            #print(y)\n\n            y = y.to(device)\n\n            # zero out gradients: we're going to recompute them in a moment\n            optimizer.zero_grad()\n\n            # compute the loss (forward pass)\n            y_hat = model(X)\n\n            #print(y_hat)\n\n            loss = loss_fn(y_hat, y)\n\n            # compute the gradient (backward pass)\n            loss.backward()\n\n            # Adam uses the gradient to update the parameters\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n\n            # print the epoch, number of batches processed, and running loss \n            # in regular intervals\n            if i % print_every == print_every - 1:    \n                print(f'[epoch: {epoch + 1}, batches: {i + 1:5d}], loss: {running_loss / print_every:.3f}')\n                running_loss = 0.0\n\n    end = time.time()\n    print(f'Finished training in {round(end - begin)}s')\n\n\ndef test(model, testloader):\n    correct = 0\n    total = 0\n    # torch.no_grad creates an environment in which we do NOT store the \n    # computational graph. We don't need to do this because we don't care about \n    # gradients unless we're training\n    with torch.no_grad():\n        for data, label in testloader:\n            #X, y = data\n            X = data\n            y = label\n            X = X.to(device)\n\n            y = torch.flatten(label)\n            y = y.to(torch.long)\n            y = y.to(device)\n\n            print(y)\n            \n            # run all the images through the model\n            y_hat = model(X)\n\n            #print(y_hat)\n\n            # the class with the largest model output is the prediction\n            _, predicted = torch.max(y_hat.data, 1)\n\n            print(\"predicted: \",predicted)\n\n            # compute the accuracy\n            total += y.size(0)\n            correct += (predicted == y).sum().item()\n\n            #print(total,correct)\n\n    print(f'Test accuracy: {100 * correct // total} %')\n\n\n!pip install torchxrayvision\n\n\nfrom torchvision import models\nimport torchxrayvision as xrv\nimport skimage, torch, torchvision\n\n# instead train only the parameters of the final layer\n# can be around 50% faster\n\n#\"\"\"\n#pretrained resnet 18 model with weights from IMAGENET\nmodel = models.resnet18(weights='IMAGENET1K_V1')\nmodel = model.to(device)\n\n# no gradients for any of the model parameters, so no updates\n\nfor param in model.parameters():\n  param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model.fc.in_features\n#out_ftrs = 2 #for binary classification\nout_ftrs = 4\nmodel.fc = nn.Linear(num_ftrs, out_ftrs)\n\nmodel = model.to(device)\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\n#optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\noptimizer = optim.Adam(model.fc.parameters(), lr=0.0005)\n#\"\"\"\n\n\"\"\"\n#from pytorch xrayvision use a pretrained model\nmodel = xrv.models.DenseNet(weights=\"densenet121-res224-nih\") # NIH chest X-ray8\nmodel = model.to(device)\n\n# no gradients for any of the model parameters, so no updates\n\nfor param in model.parameters():\n  param.requires_grad = False\n\nprint(model.classifier.in_features,model.classifier.out_features)\n\n# Parameters of newly constructed modules have requires_grad=True by default\n#num_ftrs = model.classifier.in_features\n#out_ftrs = 2 #for binary classification\n#out_ftrs = 4\n#model.classifier = nn.Linear(num_ftrs, out_ftrs)\n\nprint(model.classifier.in_features,model.classifier.out_features)\n\nmodel = model.to(device)\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n\"\"\"\n\n\nfrom torchsummary import summary\n\nsummary(model, (3,224,224))\n\n\nimport time\n\n#previous value\ndf_val_race_mod = df_train_race.sort_values(by=['PATIENT'])[2000:2400]\n#df_val_race_mod = df_train_race.sort_values(by=['PATIENT'])[10000:12000]\n\nrace_is_black_val = ((df_val_race_mod['PRIMARY_RACE'] == 'Black or African American')  | (df_val_race_mod['PRIMARY_RACE'] =='Black, non-Hispanic'))\n#race_is_black.sum()\n\nrace_is_white_val = ((df_val_race_mod['PRIMARY_RACE'] == 'White')  | (df_val_race_mod['PRIMARY_RACE'] =='White, non-Hispanic') | (df_val_race_mod['PRIMARY_RACE'] =='White or Caucasian'))\nprint(race_is_white_val.sum())\n\n#val_dataset = ChestXrayDataset(\"/content/drive/MyDrive/chexpert_small/train\", df_val_race_mod, race_is_black_val, IMAGE_SIZE, True)\n\nval_dataset = ChestXrayDataset(\"/content/drive/Shareddrives/CSCI451/train\", df_patients_race[10000:11500], race_is_white_val, IMAGE_SIZE, True)\n\nval_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n\ntrain(model, train_dataloader, optimizer, k_epochs = 100, print_every = 20)\ntest(model,val_dataloader)\n\n\nodf_val_race_mod = df_train_race.sort_values(by=['PATIENT'])[2000:2321]\n\nrace_is_black_val = ((df_val_race_mod['PRIMARY_RACE'] == 'Black or African American')  | (df_val_race_mod['PRIMARY_RACE'] =='Black, non-Hispanic'))\n\nprint(race_is_black_val.sum())\nprint(df_val_race_mod.shape)\nprint(100*(1-race_is_black_val.sum()/df_val_race_mod.shape[0]))\n\nval_dataset = ChestXrayDataset(\"/content/drive/Shareddrives/CSCI451/train\", df_patients_race[2000:2321], race_is_white_val, IMAGE_SIZE, True)\n\nprint(BATCH_SIZE)\n\nval_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\ntest(model,val_dataloader)\n\nModel Optimization\n\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler, ConcatDataset\nfrom sklearn.model_selection import KFold\nimport time\n\ndef train_epoch(model, trainloader, optimizer, loss_fn, print_every = 2000):\n\n  training_loss = 0.0\n  for i, data in enumerate(trainloader, 0):\n\n      # extract a batch of training data from the data loader\n      X, y = data\n      X = X.to(device)\n\n      #need to flatten y - this holds labels for each image in the batch, but as a list of a list\n      y = torch.flatten(y)\n      y = y.to(torch.long)\n\n      y = y.to(device)\n\n      # zero out gradients: we're going to recompute them in a moment\n      optimizer.zero_grad()\n\n      # compute the loss (forward pass)\n      y_hat = model(X)\n\n      loss = loss_fn(y_hat, y)\n\n      # compute the gradient (backward pass)\n      loss.backward()\n\n      # Adam uses the gradient to update the parameters\n      optimizer.step()\n\n      # print statistics\n      training_loss += loss.item()\n\n  return training_loss\n\ndef test_epoch(model, testloader):\n  correct = 0\n  total = 0\n  with torch.no_grad():\n      for data, label in testloader:\n          #X, y = data\n          X = data\n          y = label\n          X = X.to(device)\n\n          y = torch.flatten(label)\n          y = y.to(torch.long)\n          y = y.to(device)\n\n          # run all the images through the model\n          y_hat = model(X)\n\n          # the class with the largest model output is the prediction\n          _, predicted = torch.max(y_hat.data, 1)\n\n          #print(\"predicted: \",predicted)\n\n          # compute the accuracy\n          total += y.size(0)\n          correct += (predicted == y).sum().item()\n  return correct/total\n    \ndef optimize(train_dataset,lr_params,max_epochs=100,batch_size=BATCH_SIZE):\n  \"\"\"\n  So far this only optimizes the learning rate, ideally it would optimize more\n  https://medium.com/dataseries/k-fold-cross-validation-with-pytorch-and-sklearn-d094aa00105f\n  a helpful example\n  \"\"\"\n\n  begin = time.time()\n\n  loss_fn = nn.CrossEntropyLoss()\n\n  k_folds = 5\n\n  splits=KFold(n_splits=k_folds,shuffle=True,random_state=42)\n\n  #best_param = lr_params[0]\n  #best_train_loss = 10.0\n  #best_valid_loss = 10.0\n  \n  best_train_losses = []\n  best_valid_losses = []\n\n  for lr_param in lr_params:\n\n    print(\"Param value: \",lr_param)\n\n    #holds best losses in each fold for particular param value\n    train_losses = []\n    valid_losses = []\n\n    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n\n      print(\"Fold: \",fold)\n\n      train_sampler_epoch = SubsetRandomSampler(train_idx)\n      val_sampler_epoch = SubsetRandomSampler(val_idx)\n      train_loader_epoch = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler_epoch)\n      val_loader_epoch = DataLoader(train_dataset, batch_size=batch_size, sampler=val_sampler_epoch)\n      \n      #pretrained resnet 18 model with weights from IMAGENET\n      model = models.resnet18(weights='IMAGENET1K_V1')\n      num_ftrs = model.fc.in_features\n      out_ftrs = 4\n      model.fc = nn.Linear(num_ftrs, out_ftrs)\n      model = model.to(device)\n\n      optimizer = optim.Adam(model.fc.parameters(), lr=lr_param)\n\n      best_epoch_train_loss = 100.0\n      best_epoch_val_loss = 0.0\n\n      for epoch in range(max_epochs):\n      \n        train_loss = train_epoch(model, train_loader_epoch, optimizer, loss_fn)\n        val_loss = test_epoch(model, val_loader_epoch)\n\n        #print(lr_param,\"Epoch: \",epoch,\" train loss: \",train_loss,\" test loss: \",val_loss)\n\n        if train_loss < best_epoch_train_loss:\n          best_epoch_train_loss = train_loss\n\n        #bit of a mislabel here, in this case, the val loss is actually an accuracy score, not a loss\n        if val_loss > best_epoch_val_loss:\n          best_epoch_val_loss = val_loss\n\n      train_losses.append(best_epoch_train_loss)\n      valid_losses.append(best_epoch_val_loss)\n\n      print(\"Best train loss: \",best_epoch_train_loss,\" best test loss: \",best_epoch_val_loss)\n\n    #best losses amongst fold\n    best_train_losses.append(sum(train_losses)/len(train_losses))\n    best_valid_losses.append(sum(valid_losses)/len(valid_losses))\n\n  print(\"Best param: \", lr_params)\n  print(\"Best train loss: \",best_train_losses)\n  print(\"Best test loss: \",best_valid_losses)\n\n  end = time.time()\n  print(f'Finished optimizing in {round(end - begin)}s')\n\n\nlr_params = [0.0001,0.0005,0.001,0.005,0.01]\noptimize(train_dataset,lr_params,max_epochs=25)\n\n\ndevice\n\n\ndf_val_race_mod = df_train_race.sort_values(by=['PATIENT'])[2000:2321]\ndf_val_race_mod\n\n\ntest(model,val_dataloader)\n\n\nprint(running_loss)\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F #this is for relu\n\nclass ConvNet(nn.Module): #inherits from nn.module\n\n  def __init__(self):\n    super().__init__() #run init method of parent class\n\n    \"\"\"\n    Let's just define the functions we'll use later for forward\n    \"\"\"\n\n    self.conv1 = nn.Conv2d(3, 100, 5) #3 input channels for rgb, 100 convolutional kernels, all size 5 by 5\n    self.conv2 = nn.Conv2d(100, 50, 3)\n    self.conv3 = nn.Conv2d(50, 20, 3)\n\n    self.pool = nn.MaxPool2d(2,2) #largest pixel value from each 2 by 2 window\n    self.fc1 = nn.Linear(13520,80)\n    self.fc2 = nn.Linear(80,40)\n    self.fc3 = nn.Linear(40,2) #only two outputs for the binary cat/dog label\n\n  def forward(self,x):\n    print('x_shape:',x.shape)\n    x = self.pool(F.relu(self.conv1(x))) #do kernel convolution to x, then do relu, then do max pooling\n    x = self.pool(F.relu(self.conv2(x)))\n    x = self.pool(F.relu(self.conv3(x)))\n\n    print('x_shape:',x.shape)\n\n    #DO NOT FORGET TO FLATTEN BEFORE LINEAR LAYERS\n\n    x = torch.flatten(x,1)\n\n    print('x_shape:',x.shape)\n\n    x = F.relu(self.fc1(x))\n\n    print('x_shape:',x.shape)\n\n    x = F.relu(self.fc2(x))\n\n    print('x_shape:',x.shape)\n\n    x = self.fc3(x) #just return the output, no nonlinear lyaers\n\n    print('x_shape:',x.shape)\n\n    return x\n\nmodel = ConvNet().to(device)\n\nimport torch\ntorch.cuda.empty_cache()\n\ntrain(model, train_dataloader, optimizer, k_epochs = 100, print_every = 50)\n\n\nmodel = DenseNet121(num_classes = 2 ).to(device)\nmodel.train()\ntrain(model, train_dataloader, optimizer, k_epochs = 100, print_every = 50)\n\n\n\nclass DenseNet121(nn.Module):\n    def __init__(self, num_classes, is_trained=True):\n        \"\"\"\n        Init model architecture\n        \n        Parameters\n        ----------\n        num_classes: int\n            number of classes\n        is_trained: bool\n            whether using pretrained model from ImageNet or not\n        \"\"\"\n        super().__init__()\n        \n        # Load the DenseNet121 from ImageNet\n        self.net = torchvision.models.densenet121(pretrained=is_trained)\n        \n        # Get the input dimension of last layer\n        kernel_count = self.net.classifier.in_features\n        \n        # Replace last layer with new layer that have num_classes nodes, after that apply Sigmoid to the output\n        self.net.classifier = nn.Sequential(nn.Linear(kernel_count, num_classes), nn.Sigmoid())\n        \n    def forward(self, inputs):\n        \"\"\"\n        Forward the netword with the inputs\n        \"\"\"\n        return self.net(inputs)\n\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n\nmodel = DenseNet121(num_classes=2).to(device) #binary classification\nmodel\n\n\nsum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# Loss function\nloss_criteria = nn.BCELoss()\n\n# Adam optimizer\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5)\n\n# Learning rate will be reduced automatically during training\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = LEARNING_RATE_SCHEDULE_FACTOR, patience = LEARNING_RATE_SCHEDULE_PATIENCE, mode = 'max', verbose=True)\n\n\ndef multi_label_auroc(y_gt, y_pred):\n    \"\"\" Calculate AUROC for each class\n\n    Parameters\n    ----------\n    y_gt: torch.Tensor\n        groundtruth\n    y_pred: torch.Tensor\n        prediction\n\n    Returns\n    -------\n    list\n        F1 of each class\n    \"\"\"\n    auroc = []\n    gt_np = y_gt.to(\"cpu\").numpy()\n    pred_np = y_pred.to(\"cpu\").numpy()\n    assert gt_np.shape == pred_np.shape, \"y_gt and y_pred should have the same size\"\n    for i in range(gt_np.shape[1]):\n        auroc.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n    return auroc\n\n\ndef epoch_training(epoch, model, train_dataloader, device, loss_criteria, optimizer, mb):\n    \"\"\"\n    Epoch training\n\n    Paramteters\n    -----------\n    epoch: int\n      epoch number\n    model: torch Module\n      model to train\n    train_dataloader: Dataset\n      data loader for training\n    device: str\n      \"cpu\" or \"cuda\"\n    loss_criteria: loss function\n      loss function used for training\n    optimizer: torch optimizer\n      optimizer used for training\n    mb: master bar of fastprogress\n      progress to log\n\n    Returns\n    -------\n    float\n      training loss\n    \"\"\"\n    # Switch model to training mode\n    model.train()\n    training_loss = 0 # Storing sum of training losses\n   \n    # For each batch\n    for batch, (images, labels) in enumerate(progress_bar(train_dataloader, parent=mb)):\n        \n        # Move X, Y  to device (GPU)\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Clear previous gradient\n        optimizer.zero_grad()\n\n        # Feed forward the model\n        pred = model(images)\n        loss = loss_criteria(pred, labels)\n\n        # Back propagation\n        loss.backward()\n\n        # Update parameters\n        optimizer.step()\n\n        # Update training loss after each batch\n        training_loss += loss.item()\n\n        mb.child.comment = f'Training loss {training_loss/(batch+1)}'\n\n    del images, labels, loss\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n    # return training loss\n    return training_loss/len(train_dataloader)\n\n\ndef evaluating(epoch, model, val_loader, device, loss_criteria, mb):\n    \"\"\"\n    Validate model on validation dataset\n    \n    Parameters\n    ----------\n    epoch: int\n        epoch number\n    model: torch Module\n        model used for validation\n    val_loader: Dataset\n        data loader of validation set\n    device: str\n        \"cuda\" or \"cpu\"\n    loss_criteria: loss function\n      loss function used for training\n    mb: master bar of fastprogress\n      progress to log\n  \n    Returns\n    -------\n    float\n        loss on validation set\n    float\n        metric score on validation set\n    \"\"\"\n\n    # Switch model to evaluation mode\n    model.eval()\n\n    val_loss = 0                                   # Total loss of model on validation set\n    out_pred = torch.FloatTensor().to(device)      # Tensor stores prediction values\n    out_gt = torch.FloatTensor().to(device)        # Tensor stores groundtruth values\n\n    with torch.no_grad(): # Turn off gradient\n        # For each batch\n        for step, (images, labels) in enumerate(progress_bar(val_loader, parent=mb)):\n            # Move images, labels to device (GPU)\n            images = images.to(device)\n            labels = labels.to(device)\n\n            # Update groundtruth values\n            out_gt = torch.cat((out_gt,  labels), 0)\n\n            # Feed forward the model\n            ps = model(images)\n            loss = loss_criteria(ps, labels)\n\n            # Update prediction values\n            out_pred = torch.cat((out_pred, ps), 0)\n\n            # Update validation loss after each batch\n            val_loss += loss\n            mb.child.comment = f'Validation loss {val_loss/(step+1)}'\n\n    # Clear memory\n    del images, labels, loss\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n    # return validation loss, and metric score\n    return val_loss/len(val_loader), np.array(multi_label_auroc(out_gt, out_pred)).mean()\n\n\nimport time\n\n\n!pip install git+https://github.com/fastai/fastai --upgrade\n\n!pip install git+https://github.com/fastai/fastprogress --upgrade\n\n\ndf_train_race_val = df_train_race.sort_values(by=['PATIENT'])[501:700]\nrace_is_black = ((df_train_race_val['PRIMARY_RACE'] == 'Black or African American')  | (df_train_race_val['PRIMARY_RACE'] =='Black, non-Hispanic'))\n\nval_dataset = ChestXrayDataset(\"/content/drive/MyDrive/chexpert_small/train\", df_train_race_val, race_is_black, IMAGE_SIZE, True)\nval_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\n\nfrom fastprogress.fastprogress import master_bar, progress_bar\nfrom time import sleep\n\n# Best AUROC value during training\nbest_score = 0\nmodel_path = \"densenet.pth\"\ntraining_losses = []\nvalidation_losses = []\nvalidation_score = []\n\n\n# Config progress bar\nmb = master_bar(range(MAX_EPOCHS))\nmb.names = ['Training loss', 'Validation loss', 'Validation AUROC']\nx = []\n\nnonimproved_epoch = 0\nstart_time = time.time()\n\n# Training each epoch\nfor epoch in mb:\n    #mb.first_bar.comment = f'Best AUROC score: {best_score}'\n    x.append(epoch)\n\n    # Training\n    train_loss = epoch_training(epoch, model, train_dataloader, device, loss_criteria, optimizer, mb)\n    mb.write('Finish training epoch {} with loss {:.4f}'.format(epoch, train_loss))\n    training_losses.append(train_loss)\n\n    # Evaluating\n    val_loss, new_score = evaluating(epoch, model, val_dataloader, device, loss_criteria, mb)\n    mb.write('Finish validation epoch {} with loss {:.4f} and score {:.4f}'.format(epoch, val_loss, new_score))\n    validation_losses.append(val_loss)\n    validation_score.append(new_score)\n\n    # Update learning rate\n    lr_scheduler.step(new_score)\n\n    # Update training chart\n    mb.update_graph([[x, training_losses], [x, validation_losses], [x, validation_score]], [0,epoch+1], [0,1])\n\n    # Save model\n    if best_score < new_score:\n        mb.write(f\"Improve AUROC from {best_score} to {new_score}\")\n        best_score = new_score\n        nonimproved_epoch = 0\n        torch.save({\"model\": model.state_dict(), \n                    \"optimizer\": optimizer.state_dict(), \n                    \"best_score\": best_score, \n                    \"epoch\": epoch, \n                    \"lr_scheduler\": lr_scheduler.state_dict()}, model_path)\n    else: \n        nonimproved_epoch += 1\n    if nonimproved_epoch > 10:\n        break\n        print(\"Early stopping\")\n    if time.time() - start_time > 3600*8:\n        break\n        print(\"Out of time\")\n\n\nimport json\n\nwith open(\"/var/log/colab-jupyter.log\", \"r\") as fo:\n  for line in fo:\n    print(json.loads(line)['msg'])\n\n\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimg = cv2.imread(train_dataset.image_paths[446],1)\n#cv2.imshow()\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n\n\n#references https://arxiv.org/pdf/1512.03385.pdf, original resnet paper\n# and https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/ for pointers on code\n# https://github.com/microsoft/nni/blob/master/examples/trials/cifar10_pytorch/models/resnet.py\n\nimport torch\nimport torch.nn as nn\n\nclass Resnet(nn.Module):\n  \"\"\"\n  for now I'm doing the 34 version\n  \"\"\"\n  def __init__(self, num_classes):\n\n    super(Resnet,self).__init__() #is (Resnet,self) necessary?\n\n    #want to take matrix 224*224 to 112*112, padding 3 ensures that each kernel \n    # centered on pixel in matrix, stride =2 makes it every other pixel, hence dimension is half\n    self.conv1 = nn.Sequential(\n      nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, stride = 2, padding=3), \n      nn.BatchNorm2d(num_features = 64),\n      nn.ReLU()\n    )\n    #now do 3 by 3 max pool, similarly need padding 1 to make sure kernel\n    # centered on each pixel, stride=2 make it other pixel, dimension 56*56\n    self.maxpool = nn.MaxPool2d(kernel_size = 3,stride=2,padding=1) #not sure why there is padding\n\n    #each time we reduce the size of the matrix by half, hence stride of 2 (for conv2_x, maxpool reduced dimension)\n    self.conv2_x = self.make_conv_layer(64,64,kernel_size = 3, stride = 1, num_layers = 3)\n    self.conv3_x = self.make_conv_layer(64,128,kernel_size = 3, stride = 2, num_layers = 4)\n    self.conv4_x = self.make_conv_layer(128,256,kernel_size = 3, stride = 2, num_layers = 6)\n    self.conv5_x = self.make_conv_layer(256,512,kernel_size = 3, stride = 2, num_layers = 3)\n\n    #size is now [512,7,7], want it to be [512,1,1] so kernel size 7\n    #self.avgpool = nn.AvgPool2d(kernel_size = 7,stride = 1) #no idea why this is 7\n    self.avgpool = nn.AdaptiveAvgPool2d((1,1)) #this is what preloaded resnet34 has\n\n    self.fc = nn.Linear(512,num_classes)\n\n  #this is specific to resnet 34\n  def make_conv_layer(self,in_channels, out_channels, kernel_size, stride, num_layers):\n    #conv 1 - 64 out, conv2 - 64 in 64 out, 64 in 64 out, 64 in 64 out, conv3 - 64 in 128 out, 128 in 128 out\n    \n    layer_list = []\n\n    in_ch = in_channels\n  \n    # first block is input stride, reset are stride of 1\n\n    layer_list.append(ResidualBlock(in_ch,out_channels,kernel_size,stride))\n    in_ch = out_channels\n\n    for i in range(1,num_layers):\n      layer_list.append(ResidualBlock(in_ch,out_channels,kernel_size,stride=1))\n\n    return nn.Sequential(*layer_list)\n\n  def forward(self,x):\n    x = self.conv1(x)\n    x = self.maxpool(x)\n    x = self.conv2_x(x)\n    x = self.conv3_x(x)\n    x = self.conv4_x(x)\n    x = self.conv5_x(x)\n    x = self.avgpool(x)\n    \n    #x = torch.flatten(x)\n    x = x.view(x.size(0), -1) #not sure why but flatten doesn't work\n    x = self.fc(x)\n    return x\n\n\nclass ResidualBlock(nn.Module):\n  def __init__(self,in_channels,out_channels,kernel_size,stride):\n    \"\"\"\n    Some math, input kernel to conv2_x is [64,56,56], can just do stride 1, size maintained\n    Input kernel to conv3_x is [64,56,56] want [128,28,28], after convolutions x is [128,28,28] (stride 2 at first then stride 1),\n    But x is still [64,56,56], so no padding, use kernel size 1 with 128 outchannels and stride 2\n    \"\"\"\n\n    super(ResidualBlock,self).__init__()\n    #again note padding here is 1 because kernel is 3 by 3\n    self.conv1 = nn.Sequential(\n        nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, stride = stride, padding = 1),\n        nn.BatchNorm2d(num_features = out_channels),\n        nn.ReLU()\n    )\n    self.conv2 = nn.Sequential(\n        nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, stride = 1, padding = 1),\n        nn.BatchNorm2d(num_features = out_channels) #don't do ReLU here since we might downsample\n    )\n\n    self.skip_connection = nn.Sequential() #identity\n\n    #idea here is that skip_connection will add the input x to itself\n    # but if stride != 1 or in_channels != out_channels, we need to change dimensions of x\n    # so that the dimensions of x after conv1 and conv2 applied\n    if stride != 1 or in_channels != out_channels:\n      self.skip_connection = nn.Sequential(\n          nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride),\n          nn.BatchNorm2d(num_features = out_channels)\n      )\n    self.ReLU = nn.ReLU()\n\n  def forward(self,x):\n    x_layer = self.conv1(x)\n    x_layer = self.conv2(x_layer)\n    x_layer += self.skip_connection(x)\n    x_layer = self.ReLU(x_layer)\n    return x_layer\n\n    \n\n\nfrom torchsummary import summary\nmodel = Resnet(4)\nmodel\nsummary(model,(3,224,224))\n\n\n[2] + [1]*(4-1)"
  }
]