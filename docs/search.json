[
  {
    "objectID": "posts/perceptron/perceptron_blog.html",
    "href": "posts/perceptron/perceptron_blog.html",
    "title": "Perceptron",
    "section": "",
    "text": "Link to code: https://github.com/jchung2020/jchung2020.github.io/tree/main/posts/perceptron"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#what-is-the-perceptron",
    "href": "posts/perceptron/perceptron_blog.html#what-is-the-perceptron",
    "title": "Perceptron",
    "section": "What is the Perceptron?",
    "text": "What is the Perceptron?\nThe Perceptron is a type of machine learning algorithm called a binary linear classifier. Given data with binary labels, the Perceptron can produce a hyperplane that separates the data according to each labels. Hence prediction only requires knowing the orientation of the point relative to the hyperplane. However, as we shall see, the Perceptron is limited by whether or not the data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#breakdown-of-the-perceptron-algorithm",
    "href": "posts/perceptron/perceptron_blog.html#breakdown-of-the-perceptron-algorithm",
    "title": "Perceptron",
    "section": "Breakdown of the Perceptron Algorithm",
    "text": "Breakdown of the Perceptron Algorithm\nThe Perceptron functions by using a weight vector \\(\\bf{w}\\) to characterize the (hopefully) separating hyperplane. After starting with a random initial guess for the weights, we continually update the weights by first choosing a random index i and hence its random point X_i.\nAs a note in the code, I convert the \\(i^{th}\\) label y[i] to y_sign_i with 2*y[i]-1. This step just ensures that instead of being mapped to 0 or 1 as y is, y_sign_i will be mapped to -1 and 1.\nThe main idea of the update is to add y_sign_i*X_i to the weights for points X_i with incorrect labels. This step changes the weight so the label on point X_i will be closer to the correct one. We check if the predicted label is incorrect by checking if the dot product of w and X_i multiplied by y_sign_i is negative or positive. Then, if the signs are the same, this quantity will be positive, and hence the multiplier to the shift in weights is 0 (so no change). Otherwise the weight vector is updated with this shift.\n\nfrom perceptron import Perceptron\n\n#function used to find separating hyperplane\nperceptron.fit(X,y,max_steps)\n\n\"\"\"\nPerceptron update code\n\"\"\"\n\n#take a random index i\ni = np.random.randint(n-1)\n\n#choose point X_i\nX_i = X_[i]\n\n#convert label of point i to -1 or 1\ny_sign_i = 2*y[i]-1\n\n#update weight\nself.w = self.w + int(np.dot(self.w,X_i)*y_sign_i < 0  )*y_sign_i*X_i"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#experiments",
    "href": "posts/perceptron/perceptron_blog.html#experiments",
    "title": "Perceptron",
    "section": "Experiments",
    "text": "Experiments\n\n2D Linearly Separable Data\nBelow I have plotted the given example for running the Perceptron algorithm on data with 2 features.\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100 \np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) \nxlab = plt.xlabel(\"Feature 1\") \nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nRunning the code, we can see that a perfect accuracy of 1.0 is reached. The weight vector corresponds to the example given.\n\nfrom perceptron import Perceptron\n%load_ext autoreload\n%autoreload 2\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\nprint(\"Weight vector: \",p.w)\nprint(\"Final accuracy: \", p.score(X,y))\nprint(\"Last 10 accuracy scores: \",p.history[-10:]) #just the last few values\n\nScore is good enough!\nWeight vector:  [2.10557404 3.1165449  0.25079936]\nFinal accuracy:  1.0\nLast 10 accuracy scores:  [0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n\n\nThis plot shows the evolution of the accuracy over iterations, which does not always increase.\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThe data can now be visualized with the separating line between the two clusters of points.\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n2D Non-linearly separable data\nBelow, I run the Perceptron algorithm on the same data, but shifted so that it is just barely not linearly separable.\n\nnp.random.seed(12345)\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.4, -1.4), (1.7, 1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y) \nxlab2 = plt.xlabel(\"Feature 1\") \nylab2 = plt.ylabel(\"Feature 2\")\n\n\n\n\nNow running the Perceptron algorithm:\n\np2 = Perceptron()\np2.fit(X2, y2, max_steps=1000)\nprint(\"Weight vector: \",p2.w)\nprint(\"Final accuracy: \", p2.score(X2,y2))\nprint(\"Last 10 accuracy scores: \",p2.history[-10:]) #just the last few values\n\nWeight vector:  [ 2.56926963  4.22077252 -0.74920064]\nFinal accuracy:  0.98\nLast 10 accuracy scores:  [0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n\n\nWe can see from plotting the accuracy that, while we never converge to a perfect classification after 1000 iterations, the score is still high.\n\nfig2 = plt.plot(p2.history)\nxlab2 = plt.xlabel(\"Iteration\")\nylab2 = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n\n6D Linearly Separable Data\nThese next points in 6D are not visualizable, but we can still run the Perceptron algorithm.\n\np_features = 7\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, 1.7, 1.7, -1.7, -1.7, 1.7), (1.7, 1.7, 1.7, 1.7, 1.7, 1.7)])\n\np3 = Perceptron()\np3.fit(X3, y3, max_steps=1000)\nprint(\"Weight vector: \",p3.w)\nprint(\"Final accuracy: \", p3.score(X3,y3))\nprint(\"Last 10 accuracy scores: \",p3.history[-10:]) #just the last few values\n\nScore is good enough!\nWeight vector:  [ 5.55526998  1.58997633 -1.00733643  1.04302711  3.54095849 -0.6594312\n  0.95889091]\nFinal accuracy:  1.0\nLast 10 accuracy scores:  [0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 1.0]\n\n\nIn this case, we do actually achieve a 100% classification! The data therefore is linearly separable.\n\nfig3 = plt.plot(p3.history)\nxlab3 = plt.xlabel(\"Iteration\")\nylab3 = plt.ylabel(\"Accuracy\")"
  },
  {
    "objectID": "posts/perceptron/perceptron_blog.html#runtime-complexity",
    "href": "posts/perceptron/perceptron_blog.html#runtime-complexity",
    "title": "Perceptron",
    "section": "Runtime Complexity",
    "text": "Runtime Complexity\nIn equation (1), we have a dot product between \\(\\bf{\\tilde{w}}^{(t)}\\) and \\(\\bf{x_i}\\). As both are dimension \\(p+1\\), this accounts for \\(O(p)\\) term by term multiplication and addition prodcedures. The addition of \\(\\bf{\\tilde{w}}^{(t)}\\) and \\(\\bf{x_i}\\) similarly consists of \\(O(p)\\) additions. So the total complexity is \\(O(p)\\). Since this is for a single index, the total number of points \\(n\\) is irrelevant."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Comparing the convergence of Gradient Descent Algorithms.\n\n\n\n\n\n\nMar 1, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn implementation of the Perceptron Algorithm.\n\n\n\n\n\n\nFeb 22, 2023\n\n\nJay-U Chung\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is the About page for this blog."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html",
    "href": "posts/gradient_descent/gradient_descent.html",
    "title": "Variations on Gradient Descent",
    "section": "",
    "text": "Link to code: https://github.com/jchung2020/jchung2020.github.io/tree/main/posts/gradient_descent\nHere I will implement three different variations on gradient descent using logistic regression loss: regular gradient descent, stochastic gradient descent, and stochastic gradient descent with momentum.\nWith gradient descent, I begin with a random guess for the minimizer of the loss function. At each point, I compute the gradient and take a step (whose size is modulated by the stepping size) in that direction. This updates the new guess for the minimum, and I continue until the gradient is close to the zero vector.\nFor stochastic gradient descent, I use the same general approach but instead divide the data up into random batches. Additionally, I implemented stochastic gradient descent with momentum, which uses the difference between our current and previous guesses for the momentum update to inform the next update on our guess. This ensures that if we have a good guess for the minimizer, we more quickly head in that direction (and more quickly converge).\nStarting with simple 2D data:\nIn this case, notice that stochastic gradient descent with momentum converges fastest, then stochastic gradient descent, then regular gradient descent.\nAs we can see, the separating lines for all algorithm are quite similar."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html#altering-the-stepping-size",
    "href": "posts/gradient_descent/gradient_descent.html#altering-the-stepping-size",
    "title": "Variations on Gradient Descent",
    "section": "Altering the Stepping Size",
    "text": "Altering the Stepping Size\nBelow is a case I show some larger stepping sizes. It is suprisingly robust, however, as even a large stepping size of 30 yields a reasonable loss. Perhaps the gradient is naturally small.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 30, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stepping size 30\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 50, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stepping size 50\")\n\nlegend = plt.legend() \nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.title(\"Large Stepping Size Comparison on Gradient Descent\")\n\nText(0.5, 1.0, 'Large Stepping Size Comparison on Gradient Descent')\n\n\n\n\n\nAs we can see, the loss never converges for a stepping size of 50, as this proves to be too large for proper gradient descent."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html#altering-the-batch-size",
    "href": "posts/gradient_descent/gradient_descent.html#altering-the-batch-size",
    "title": "Variations on Gradient Descent",
    "section": "Altering the Batch Size",
    "text": "Altering the Batch Size\nHere I demonstrate that the batch size can affect how quickly stochastic gradient search will converge.\n\nLR1 = LogisticRegression()\nLR1.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 25, \n                  alpha = 0.1) \n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.1) \n\nnum_steps = len(LR1.loss_history)\n\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"loss (batch size 25)\")\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"loss (batch size 10)\")\n#plt.plot(np.arange(num_steps) + 1, LR.history, label = \"accuracy\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.title(\"Batch Size Comparison for Stochastic Gradient Descent\")\n\nText(0.5, 1.0, 'Batch Size Comparison for Stochastic Gradient Descent')\n\n\n\n\n\nAs we can see, a smaller batch size allows for a faster convergence, perhaps because the gradient descent more frequently updates the point of the minimum loss."
  },
  {
    "objectID": "posts/gradient_descent/gradient_descent.html#gradient-descent-in-higher-dimensions",
    "href": "posts/gradient_descent/gradient_descent.html#gradient-descent-in-higher-dimensions",
    "title": "Variations on Gradient Descent",
    "section": "Gradient Descent in Higher Dimensions",
    "text": "Gradient Descent in Higher Dimensions\nBelow I have plotted comparisons of the gradient descent algorithms in 10 dimensions. Notice that the convergence is faster for stochastic gradient with momentum. In fact, all the gradient descent algorithms converge quite quickly.\nI initially used random vectors for the centers of the blobs but it seems that the data may be linearly separable (hence the fast convergence). Perhaps in higher dimensions, this method of generating data makes it easier to separate linearly. For this new data set, I randomized a vector to multiply to one data set so they are not as linearly separable.\n\np_features = 11\n\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(1.72810539, -0.76966727, -8.40792394,  0.00943856,  1.33926325,  1.80831111,\n -8.52324258,  1.39576299,  1.50974231, -6.81079023), (1.2532123,   -0.45271457, -4.67704148,  0.00651874,  1.5479877,   0.84146263,\n -3.45566331,  0.62523429,  0.684547,   -2.54964336)])\n\nfig, axs = plt.subplots(2, figsize=(10, 10))\nfig.suptitle('Vertically stacked subplots')\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = 0.1) \n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"stochastic gradient (momentum)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = 0.1)\n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 0.1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\naxs[0].plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\naxs[1].plot(np.arange(num_steps) + 1, LR.history, label = \"gradient\")\n\n#plt.loglog()\naxs[0].loglog()\naxs[1].loglog()\n\nlegend = axs[0].legend() \nlegend = axs[1].legend() \n\naxs[0].set_title(\"Loss Comparison for Gradient Descent Algorithms\")\naxs[0].set(xlabel = 'Iterations', ylabel= 'Loss')\naxs[1].set_title(\"Accuracy Comparison for Gradient Descent Algorithms\")\naxs[1].set(xlabel = 'Iterations', ylabel= 'Accuracy')\n\n[Text(0.5, 0, 'Iterations'), Text(0, 0.5, 'Accuracy')]\n\n\n\n\n\nStochastic gradient descent with momentum clearly converges the fastest. Stochastic gradient both with and without momentum appear to fluctuate in the loss (probably as a result of the random batches), but both converge faster than regular gradient descent and even reach 100% accuracy."
  }
]